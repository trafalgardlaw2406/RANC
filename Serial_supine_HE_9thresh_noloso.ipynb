{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Serial_supine_HE_9thresh_noloso.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7Mit1J+l56H7Oi3/1YRKV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trafalgardlaw2406/RANC/blob/main/Serial_supine_HE_9thresh_noloso.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNU6VVJg4B2w",
        "outputId": "97cdc83f-fc34-4874-8855-9216ac8d2889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import operator\n",
        "import functools\n",
        "import math\n",
        "import os\n",
        "\n",
        "from scipy import ndimage\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Activation, Input, Lambda, concatenate,Average\n",
        "from tensorflow.keras.datasets import mnist,fashion_mnist\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import constraints\n",
        "import sys\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO614lfU4cok",
        "outputId": "ef941fe0-28a4-41fb-9ab0-e8ffb08a0c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tea(Layer):\n",
        "    def __init__(self,\n",
        "                 units,\n",
        "                 **kwargs):\n",
        "        \"\"\"Initializes a new TeaLayer.\n",
        "\n",
        "        Arguments:\n",
        "            units -- The number of neurons to use for this layer.\"\"\"\n",
        "        self.units = units\n",
        "        # Needs to be set to `True` to use the `K.in_train_phase` function.\n",
        "        self.uses_learning_phase = True\n",
        "        super(Tea, self).__init__(**kwargs)\n",
        "\n",
        "def tea_weight_initializer(shape, dtype=np.float32):\n",
        "    \"\"\"Returns a tensor of alternating 1s and -1s, which is (kind of like)\n",
        "    how IBM initializes their weight matrix in their TeaLearning\n",
        "    literature.\n",
        "\n",
        "    Arguments:\n",
        "        shape -- The shape of the weights to intialize.\n",
        "\n",
        "    Keyword Arguments:\n",
        "        dtype -- The data type to use to initialize the weights.\n",
        "                 (default: {np.float32})\"\"\"\n",
        "    num_axons = shape[0]\n",
        "    num_neurons = shape[1]\n",
        "    ret_array = np.zeros((int(num_axons), int(num_neurons)), dtype=np.float32)\n",
        "    for axon_num, axon in enumerate(ret_array):\n",
        "        if axon_num % 2 == 0:\n",
        "            for i in range(len(axon)):\n",
        "                ret_array[axon_num][i] = 1\n",
        "        else:\n",
        "            for i in range(len(axon)):\n",
        "                ret_array[axon_num][i] = -1\n",
        "    return tf.convert_to_tensor(ret_array)\n",
        "\n",
        "def build(self, input_shape):\n",
        "    assert len(input_shape) >= 2\n",
        "    shape = (input_shape[-1], self.units)\n",
        "    self.static_weights = self.add_weight(\n",
        "        name='weights',\n",
        "        shape=shape,\n",
        "        initializer=tea_weight_initializer,\n",
        "        trainable=False)\n",
        "    # Intialize connections around 0.5 because they represent probabilities.\n",
        "    self.connections = self.add_weight(\n",
        "        name='connections',\n",
        "        initializer=initializers.TruncatedNormal(mean=0.5),\n",
        "        shape=shape)\n",
        "    self.biases = self.add_weight(\n",
        "        name='biases',\n",
        "        initializer='zeros',\n",
        "        shape=(self.units,))\n",
        "    super(Tea, self).build(input_shape)\n",
        "\n",
        "# Bind the method to our class\n",
        "Tea.build = build\n",
        "\n",
        "def call(self, x):\n",
        "    with tf.get_default_graph().gradient_override_map(\n",
        "        {\"Round\":\"CustomRound\"}):\n",
        "        # Constrain input\n",
        "        x = tf.round(x)\n",
        "        # Constrain connections\n",
        "        connections = self.connections\n",
        "        connections = tf.round(connections)\n",
        "        connections = K.clip(connections, 0, 1)\n",
        "        # Multiply connections with weights\n",
        "        weighted_connections = connections * self.static_weights\n",
        "        # Dot input with weighted connections\n",
        "        output = K.dot(x, weighted_connections)\n",
        "        # Constrain biases\n",
        "        biases = tf.round(self.biases)\n",
        "        output = K.bias_add(\n",
        "            output,\n",
        "            biases,\n",
        "            data_format='channels_last'\n",
        "        )\n",
        "        # Apply activation / spike\n",
        "        output = K.in_train_phase(\n",
        "            K.sigmoid(output),\n",
        "            tf.cast(tf.greater_equal(output, 0.0), tf.float32)\n",
        "        )\n",
        "    return output\n",
        "    \n",
        "# Bind the method to our class\n",
        "Tea.call = call\n",
        "\n",
        "def compute_output_shape(self, input_shape):\n",
        "    assert input_shape and len(input_shape) >= 2\n",
        "    assert input_shape[-1]\n",
        "    output_shape = list(input_shape)\n",
        "    output_shape[-1] = self.units\n",
        "    return tuple(output_shape)\n",
        "    \n",
        "# Bind the method to our class\n",
        "Tea.compute_output_shape = compute_output_shape\n",
        "\n",
        "\n",
        "class AdditivePooling(Layer):\n",
        "    \"\"\"A helper layer designed to format data for output during TeaLearning.\n",
        "    If the data input to the layer has multiple spikes per classification, the\n",
        "    spikes for each tick are summed up. Then, all neurons that correspond to a\n",
        "    certain class are summed up so that the output is the number of spikes for\n",
        "    each class. Neurons are assumed to be arranged such that each\n",
        "    `num_classes` neurons represent a guess for each of the classes. For\n",
        "    example, if the guesses correspond to number from 0 to 9, the nuerons are\n",
        "    arranged as such:\n",
        "\n",
        "        neuron_num: 0  1  2  3  4  5  6  7  8  9  10 11 12  ...\n",
        "        guess:      0  1  2  3  4  5  6  7  8  9  0  1  2   ...\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_classes,\n",
        "                 **kwargs):\n",
        "        \"\"\"Initializes a new `AdditivePooling` layer.\n",
        "\n",
        "        Arguments:\n",
        "            num_classes -- The number of classes to output.\n",
        "        \"\"\"\n",
        "        self.num_classes = num_classes\n",
        "        self.num_inputs = None\n",
        "        super(AdditivePooling, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 2\n",
        "        # The number of neurons must be collapsable into the number of classes\n",
        "        assert input_shape[-1] % self.num_classes == 0\n",
        "        self.num_inputs = input_shape[-1]\n",
        "\n",
        "    def call(self, x):\n",
        "        # Sum up ticks if there are ticks\n",
        "        if len(x.shape) >= 3:\n",
        "            output = K.sum(x, axis=1)\n",
        "        else:\n",
        "            output = x\n",
        "        # Reshape output\n",
        "        output = tf.reshape(\n",
        "            output,\n",
        "            [-1, int(self.num_inputs // self.num_classes), self.num_classes]\n",
        "        )\n",
        "        # Sum up neurons\n",
        "        output = tf.reduce_sum(output, 1)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_shape = list(input_shape)\n",
        "        # Last dimension will be number of classes\n",
        "        output_shape[-1] = self.num_classes\n",
        "        # Ticks were summed, so delete tick dimension if exists\n",
        "        if len(output_shape) >= 3:\n",
        "            del output_shape[1]\n",
        "        return tuple(output_shape)"
      ],
      "metadata": {
        "id": "iY5E9HBk4g5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Load\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# PyTorch (modeling)\n",
        "# import torch\n",
        "# from torch import nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data.sampler import SubsetRandomSampler\n",
        "# from torchvision import transforms\n",
        "# import torchvision.transforms.functional as TF\n",
        "# from torch.utils.data import Dataset\n",
        "# from torch.utils.data import random_split\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# # Visualization\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "from scipy import ndimage\n",
        "\n",
        "# These functions are introduced along the Part 1 notebook.\n",
        "\n",
        "# Position vectors. We load the data with respect to the file name, which is\n",
        "# a number corresponding to a specific in-bed position. We take advantage of this\n",
        "# and use the number to get the position with help of the following vectors.\n",
        "\n",
        "positions_i = [\"justAPlaceholder\", \"supine_1\", \"right_0\",\n",
        "               \"left_0\", \"right_30\", \"right_60\",\n",
        "               \"left_30\", \"left_60\", \"supine_2\",\n",
        "               \"supine_3\", \"supine_4\", \"supine_5\",\n",
        "               \"supine_6\", \"right_fetus\", \"left_fetus\",\n",
        "               \"supine_30\", \"supine_45\", \"supine_60\"]\n",
        "\n",
        "positions_i_short = [\"justAPlaceholder\", \"supine\", \"right\",\n",
        "               \"left\", \"right\", \"right\",\n",
        "               \"left\", \"left\", \"supine\",\n",
        "               \"supine\", \"supine\", \"supine\",\n",
        "               \"supine\", \"right\", \"left\",\n",
        "               \"supine\", \"supine\", \"supine\"]\n",
        "\n",
        "positions_ii = {\n",
        "    \"B\":\"supine\", \"1\":\"supine\", \"C\":\"right\",\n",
        "    \"D\":\"left\", \"E1\":\"right\", \"E2\":\"right\",\n",
        "    \"E3\":\"left\", \"E4\":\"left\", \"E5\":\"right\",\n",
        "    \"E6\":\"left\", \"F\":\"supine\", \"G1\":\"supine\",\n",
        "    \"G2\":\"right\", \"G3\":\"left\"\n",
        "}\n",
        "\n",
        "class_positions = ['supine', 'left', 'right', 'left_fetus', 'right_fetus']\n",
        "\n",
        "# We also want the classes to be encoded as numbers so we can work easier when\n",
        "# modeling. This function achieves so. Since left_fetus and right_fetus are not\n",
        "# considered as classes in the evaluation of the original paper and since they\n",
        "# are not considered in the \"Experiment I\", we encode them also as left and right\n",
        "# positions.\n",
        "\n",
        "def token_position_short(x):\n",
        "  return {\n",
        "      'supine': 0,\n",
        "      'left': 1,\n",
        "      'right': 2,\n",
        "      'left_fetus': 1,\n",
        "      'right_fetus': 2\n",
        "  }[x]\n",
        "\n",
        "def token_position(x):\n",
        "  return {\n",
        "      \"supine_1\":0, \n",
        "      \"right_0\":1,\n",
        "      \"left_0\":2, \n",
        "      \"right_30\":3, \n",
        "      \"right_60\":4,\n",
        "      \"left_30\":5, \n",
        "      \"left_60\":6, \n",
        "      \"supine_2\":7,\n",
        "      \"supine_3\":8, \n",
        "      \"supine_4\":9, \n",
        "      \"supine_5\":10,\n",
        "      \"supine_6\":11, \n",
        "      \"right_fetus\":12, \n",
        "      \"left_fetus\":13,\n",
        "      \"supine_30\":14, \n",
        "      \"supine_45\":15, \n",
        "      \"supine_60\":16\n",
        "  }[x]\n",
        "\n",
        "def token_position_new(x):\n",
        "  return {\n",
        "      \"supine_1\":0, \n",
        "      \"supine_2\":1,\n",
        "      \"supine_3\":2, \n",
        "      \"supine_4\":3, \n",
        "      \"supine_5\":4,\n",
        "      \"supine_6\":5, \n",
        "      \"supine_30\":6, \n",
        "      \"supine_45\":7, \n",
        "      \"supine_60\":8, \n",
        "      \"left_0\":9, \n",
        "      \"left_30\":10, \n",
        "      \"left_60\":11,\n",
        "      \"left_fetus\":12, \n",
        "      \"right_0\":13,\n",
        "      \"right_30\":14, \n",
        "      \"right_60\":15,\n",
        "      \"right_fetus\":16, \n",
        "  }[x]\n",
        "list_supine = [\"1.txt\",\"8.txt\",\"9.txt\",\"10.txt\",\"11.txt\",\"12.txt\",\"15.txt\",\"16.txt\",\"17.txt\"]\n",
        "\n",
        "list_supine_norm_1 = [\"1.txt\",\"8.txt\",\"9.txt\"]\n",
        "\n",
        "list_supine_norm_2 = [\"10.txt\",\"11.txt\",\"12.txt\"]\n",
        "\n",
        "list_supine_incl = [\"15.txt\",\"16.txt\",\"17.txt\"]\n",
        "\n",
        "list_left = [\"3.txt\",\"6.txt\",\"7.txt\",\"14.txt\"]\n",
        "\n",
        "list_right = [\"2.txt\",\"4.txt\",\"5.txt\",\"13.txt\"]\n",
        "\n",
        "def token_position_supine(x):\n",
        "  return {\n",
        "      \"supine_1\":0, \n",
        "      \"supine_2\":1,\n",
        "      \"supine_3\":2, \n",
        "      \"supine_4\":3, \n",
        "      \"supine_5\":4,\n",
        "      \"supine_6\":5, \n",
        "      \"supine_30\":6, \n",
        "      \"supine_45\":7, \n",
        "      \"supine_60\":8\n",
        "    }[x]\n",
        "\n",
        "def token_position_supine_norm_1(x):\n",
        "  return {\n",
        "      \"supine_1\":0, \n",
        "      \"supine_2\":1,\n",
        "      \"supine_3\":2, \n",
        "    }[x]\n",
        "\n",
        "def token_position_supine_norm_2(x):\n",
        "  return {\n",
        "      \"supine_4\":0, \n",
        "      \"supine_5\":1,\n",
        "      \"supine_6\":2, \n",
        "    }[x]\n",
        "\n",
        "def token_position_supine_incl(x):\n",
        "  return {\n",
        "      \"supine_30\":0, \n",
        "      \"supine_45\":1, \n",
        "      \"supine_60\":2\n",
        "    }[x]\n",
        "\n",
        "def token_position_left(x):\n",
        "  return {\n",
        "      \"left_0\":0, \n",
        "      \"left_30\":1, \n",
        "      \"left_60\":2,\n",
        "      \"left_fetus\":3,\n",
        "  }[x]\n",
        "\n",
        "def token_position_right(x):\n",
        "  return {\n",
        "      \"right_0\":0,\n",
        "      \"right_30\":1, \n",
        "      \"right_60\":2,\n",
        "      \"right_fetus\":3, \n",
        "  }[x]\n",
        "\n",
        "\n",
        "def load_exp_i(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      max_val = []\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(3, len(lines) - 3):\n",
        "                              \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                \n",
        "                # Spatio-temporal median filter 3x3x3\n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                past_image = ndimage.median_filter(past_image, 3)\n",
        "                future_image = ndimage.median_filter(future_image, 3)\n",
        "                raw_data = np.concatenate((raw_data[np.newaxis, :, :], past_image[np.newaxis, :, :], future_image[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "            \n",
        "            # with open(file_path, 'r') as f:\n",
        "            #   # Start from second recording, as the first two are corrupted\n",
        "            #   lines = f.read().splitlines()[2:]\n",
        "            #   for line in f.read().splitlines()[2:]:\n",
        "            #     # print(line)\n",
        "            #     raw_data = np.fromstring(line, dtype=float, sep='\\t')\n",
        "                # Change the range from [0-1000] to [0-255].\n",
        "                  # max_val.append(np.amax(raw_data))\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              # file_data = np.round(raw_data).astype(np.uint8)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "              # print(positions_i[int(file[:-4])])\n",
        "              file_label = token_position(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "      \n",
        "      # max_over_all = max(max_val)\n",
        "      # print(max_over_all)\n",
        "\n",
        "      # data = np.round(data * 255/1000).astype(np.uint8)\n",
        "      dataset[subject] = (data, labels)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_short(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      max_val = []\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(3, len(lines) - 3):\n",
        "                              \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                \n",
        "                # Spatio-temporal median filter 3x3x3\n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                past_image = ndimage.median_filter(past_image, 3)\n",
        "                future_image = ndimage.median_filter(future_image, 3)\n",
        "                raw_data = np.concatenate((raw_data[np.newaxis, :, :], past_image[np.newaxis, :, :], future_image[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              # file_data = np.round(raw_data).astype(np.uint8)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "              # print(positions_i[int(file[:-4])])\n",
        "              file_label = token_position_short(positions_i_short[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_supine(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_supine\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(5, len(lines) - 5):\n",
        "                            \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image_1 = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_1 = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                past_image_2 = np.fromstring(lines[i-2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_2 = np.fromstring(lines[i+2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "                # Spatio-temporal median filter 5x5x5\n",
        "              \n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                \n",
        "                past_image_1 = ndimage.median_filter(past_image_1, 3)\n",
        "                future_image_1 = ndimage.median_filter(future_image_1, 3)\n",
        "                past_image_2 = ndimage.median_filter(past_image_2, 3)\n",
        "                future_image_2 = ndimage.median_filter(future_image_2, 3)\n",
        "\n",
        "                raw_data = np.concatenate((past_image_2[np.newaxis, :, :],past_image_1[np.newaxis, :, :] ,raw_data[np.newaxis, :, :], \\\n",
        "                future_image_1[np.newaxis, :, :],future_image_2[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "              \n",
        "              # a=np.amax(raw_data)\n",
        "\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              \n",
        "              # file_data = np.round(raw_data).astype(np.uint8)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "\n",
        "              file_label = token_position_supine(positions_i[int(file[:-4])])\n",
        "              \n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_supine_norm_1(path):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_supine_norm_1\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            for line in f.read().splitlines()[2:]:\n",
        "              # print(line)\n",
        "              raw_data = np.fromstring(line, dtype=float, sep='\\t')\n",
        "              # Change the range from [0-1000] to [0-255].\n",
        "              max_val = np.amax(raw_data)\n",
        "              file_data = np.round(raw_data*255/max_val).astype(np.uint8)\n",
        "              \n",
        "              # file_data = np.round(raw_data).astype(float)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "\n",
        "              file_label = token_position_supine_norm_1(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_supine_norm_2(path):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_supine_norm_2\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            for line in f.read().splitlines()[2:]:\n",
        "              # print(line)\n",
        "              raw_data = np.fromstring(line, dtype=float, sep='\\t')\n",
        "              # Change the range from [0-1000] to [0-255].\n",
        "              max_val = np.amax(raw_data)\n",
        "              file_data = np.round(raw_data*255/max_val).astype(np.uint8)\n",
        "              \n",
        "              # file_data = np.round(raw_data).astype(float)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "\n",
        "              file_label = token_position_supine_norm_2(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_supine_incl(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_supine_incl\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            # with open(file_path, 'r') as f:\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(3, len(lines) - 3):\n",
        "\n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                \n",
        "                # Spatio-temporal median filter 3x3x3\n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                past_image = ndimage.median_filter(past_image, 3)\n",
        "                future_image = ndimage.median_filter(future_image, 3)\n",
        "                raw_data = np.concatenate((raw_data[np.newaxis, :, :], past_image[np.newaxis, :, :], future_image[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "\n",
        "              # Change the range from [0-1000] to [0-255].\n",
        "              # max_vol = np.amax(raw_data)\n",
        "              file_data = np.round(raw_data ).astype(np.uint8)\n",
        "\n",
        "              # file_data = np.round(raw_data).astype(np.uint8)\n",
        "              file_data = file_data.reshape(1, 64, 32)\n",
        "\n",
        "              file_label = token_position_supine_incl(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_left(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_left\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(5, len(lines) - 5):\n",
        "                            \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image_1 = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_1 = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                past_image_2 = np.fromstring(lines[i-2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_2 = np.fromstring(lines[i+2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "                # Spatio-temporal median filter 5x5x5\n",
        "              \n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                \n",
        "                past_image_1 = ndimage.median_filter(past_image_1, 3)\n",
        "                future_image_1 = ndimage.median_filter(future_image_1, 3)\n",
        "                past_image_2 = ndimage.median_filter(past_image_2, 3)\n",
        "                future_image_2 = ndimage.median_filter(future_image_2, 3)\n",
        "\n",
        "                raw_data = np.concatenate((past_image_2[np.newaxis, :, :],past_image_1[np.newaxis, :, :] ,raw_data[np.newaxis, :, :], \\\n",
        "                future_image_1[np.newaxis, :, :],future_image_2[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "          # with open(file_path, 'r') as f:\n",
        "          #   # Start from second recording, as the first two are corrupted\n",
        "          #   for line in f.read().splitlines()[2:]:\n",
        "          #     # print(line)\n",
        "          #     raw_data = np.fromstring(line, dtype=float, sep='\\t')\n",
        "          #     # Change the range from [0-1000] to [0-255].\n",
        "\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              \n",
        "\n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "\n",
        "              file_label = token_position_left(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_right(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_right\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(5, len(lines) - 5):\n",
        "                            \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image_1 = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_1 = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                past_image_2 = np.fromstring(lines[i-2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_2 = np.fromstring(lines[i+2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "                # Spatio-temporal median filter 5x5x5\n",
        "              \n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                \n",
        "                past_image_1 = ndimage.median_filter(past_image_1, 3)\n",
        "                future_image_1 = ndimage.median_filter(future_image_1, 3)\n",
        "                past_image_2 = ndimage.median_filter(past_image_2, 3)\n",
        "                future_image_2 = ndimage.median_filter(future_image_2, 3)\n",
        "\n",
        "                raw_data = np.concatenate((past_image_2[np.newaxis, :, :],past_image_1[np.newaxis, :, :] ,raw_data[np.newaxis, :, :], \\\n",
        "                future_image_1[np.newaxis, :, :],future_image_2[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "              # Change the range from [0-1000] to [0-255].\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "\n",
        "              file_label = token_position_right(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_new(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      max_val = []\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(3, len(lines) - 3):\n",
        "                              \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                \n",
        "                # Spatio-temporal median filter 3x3x3\n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                past_image = ndimage.median_filter(past_image, 3)\n",
        "                future_image = ndimage.median_filter(future_image, 3)\n",
        "                raw_data = np.concatenate((raw_data[np.newaxis, :, :], past_image[np.newaxis, :, :], future_image[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "            \n",
        "            # with open(file_path, 'r') as f:\n",
        "            #   # Start from second recording, as the first two are corrupted\n",
        "            #   lines = f.read().splitlines()[2:]\n",
        "            #   for line in f.read().splitlines()[2:]:\n",
        "            #     # print(line)\n",
        "            #     raw_data = np.fromstring(line, dtype=float, sep='\\t')\n",
        "                # Change the range from [0-1000] to [0-255].\n",
        "                  # max_val.append(np.amax(raw_data))\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              # file_data = np.round(raw_data).astype(np.uint8)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "              # print(positions_i[int(file[:-4])])\n",
        "              file_label = token_position_new(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "      \n",
        "      # max_over_all = max(max_val)\n",
        "      # print(max_over_all)\n",
        "\n",
        "      # data = np.round(data * 255/1000).astype(np.uint8)\n",
        "      dataset[subject] = (data, labels)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "def load_exp_ii(path):\n",
        "\n",
        "  exp_ii_data_air = {}\n",
        "  exp_ii_data_spo = {}\n",
        "\n",
        "  # each directory is a subject\n",
        "  for _, subject_dirs, _ in os.walk(path):\n",
        "    for subject in subject_dirs:\n",
        "      data = None\n",
        "      labels = None\n",
        "\n",
        "      # each directory is a matresss\n",
        "      for _, mat_dirs, _ in os.walk(os.path.join(path, subject)):\n",
        "        for mat in mat_dirs:\n",
        "          for _, _, files in os.walk(os.path.join(path, subject, mat)):\n",
        "            for file in files:\n",
        "              file_path = os.path.join(path, subject, mat, file)\n",
        "              raw_data = np.loadtxt(file_path)\n",
        "              # Change the range from [0-500] to [0-255].\n",
        "              file_data = np.round(raw_data*255/500).astype(np.uint8)\n",
        "              \n",
        "              file_data = resize_and_rotate(file_data)\n",
        "              \n",
        "              file_data = file_data.view(1, 64, 32)\n",
        "\n",
        "              if file[-6] == \"E\" or file[-6] == \"G\":\n",
        "                file_label = positions_ii[file[-6:-4]]\n",
        "              else:\n",
        "                file_label = positions_ii[file[-6]]\n",
        "\n",
        "              file_label = token_position(file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "          if mat == \"Air_Mat\":\n",
        "            exp_ii_data_air[subject] = (data, labels)\n",
        "          else:\n",
        "            exp_ii_data_spo[subject] = (data, labels)\n",
        "\n",
        "          data = None\n",
        "          labels = None\n",
        "\n",
        "    return exp_ii_data_air, exp_ii_data_spo\n",
        "\n",
        "import cv2 \n",
        "\n",
        "class Mat_Dataset():\n",
        "  def __init__(self,datasets, mats, Subject_IDs):\n",
        "\n",
        "    self.samples = []\n",
        "    self.labels = []\n",
        "\n",
        "    for mat in mats:\n",
        "      data = datasets[mat]\n",
        "      self.samples.append(np.vstack([data.get(key)[0] for key in Subject_IDs]))\n",
        "      self.labels.append(np.hstack([data.get(key)[1] for key in Subject_IDs]))\n",
        "\n",
        "    self.samples = np.vstack(self.samples)\n",
        "    self.labels = np.hstack(self.labels)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.samples.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.samples[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "xhu_c8xM4mv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.RegisterGradient(\"CustomRound\")\n",
        "def _const_round_grad(unused_op, grad):\n",
        "    return grad"
      ],
      "metadata": {
        "id": "B9aoyHfF4n1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "1avGGPiY4q7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exp_i_data = load_exp_i_supine(\"/content/drive/MyDrive/RANC/dataset/experiment-i\")\n",
        "\n",
        "datasets = {\"Base\":exp_i_data}\n",
        "subjects = [\"S1\",\"S2\",\"S3\",\"S4\",\"S5\",\"S6\",\"S7\",\"S8\",\"S9\",\"S10\",\"S11\",\"S12\",\"S13\"]\n",
        "\n",
        "data = Mat_Dataset(datasets,[\"Base\"],subjects)"
      ],
      "metadata": {
        "id": "XQcC1xIt5Dn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_data, test_data, y_train, y_test = train_test_split(data.samples,data.labels, test_size = 0.1, random_state= 25)"
      ],
      "metadata": {
        "id": "Y5QYYpGP5aZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = to_categorical(y_train,9)\n",
        "y_test = to_categorical(y_test,9)"
      ],
      "metadata": {
        "id": "hqweSvsb5s_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gARyR1L29uda",
        "outputId": "cf87183d-a2c8-4e4f-aa00-16d4f2689e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = []\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "    \n",
        "    mask = np.ones_like(train_data[i])\n",
        "\n",
        "    train_data[i] = cv2.equalizeHist(train_data[i])\n",
        "\n",
        "    e_1 = np.array(train_data[i]>=mask*25).astype(float)\n",
        "    e_2 = np.array(train_data[i]>=mask*50).astype(float)\n",
        "    e_3 = np.array(train_data[i]>=mask*75).astype(float)\n",
        "    e_4 = np.array(train_data[i]>=mask*100).astype(float)\n",
        "    e_5 = np.array(train_data[i]>=mask*125).astype(float)\n",
        "    e_6 = np.array(train_data[i]>=mask*150).astype(float)\n",
        "    e_7 = np.array(train_data[i]>=mask*175).astype(float)\n",
        "    e_8 = np.array(train_data[i]>=mask*200).astype(float)\n",
        "    e_9 = np.array(train_data[i]>=mask*225).astype(float)\n",
        "    # print(e_1[:,:,np.newaxis].shape)\n",
        "    x_train.append(np.concatenate((e_1[:,:,np.newaxis],e_2[:,:,np.newaxis],e_3[:,:,np.newaxis],\n",
        "                                   e_4[:,:,np.newaxis],e_5[:,:,np.newaxis],e_6[:,:,np.newaxis],\n",
        "                                   e_7[:,:,np.newaxis],e_8[:,:,np.newaxis],e_9[:,:,np.newaxis]),axis=2))\n",
        "\n",
        "x_train = np.array(x_train).astype(np.uint8)"
      ],
      "metadata": {
        "id": "x6dk90-e5_2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = []\n",
        "\n",
        "for i in range(len(test_data)):\n",
        "    \n",
        "    mask = np.ones_like(train_data[i])\n",
        "    \n",
        "    test_data[i] = cv2.equalizeHist(test_data[i])\n",
        "\n",
        "    e_1 = np.array(test_data[i]>=mask*25).astype(float)\n",
        "    e_2 = np.array(test_data[i]>=mask*50).astype(float)\n",
        "    e_3 = np.array(test_data[i]>=mask*75).astype(float)\n",
        "    e_4 = np.array(test_data[i]>=mask*100).astype(float)\n",
        "    e_5 = np.array(test_data[i]>=mask*125).astype(float)\n",
        "    e_6 = np.array(test_data[i]>=mask*150).astype(float)\n",
        "    e_7 = np.array(test_data[i]>=mask*175).astype(float)\n",
        "    e_8 = np.array(test_data[i]>=mask*200).astype(float)\n",
        "    e_9 = np.array(test_data[i]>=mask*225).astype(float)\n",
        "    # print(e_1[:,:,np.newaxis].shape)\n",
        "    x_test.append(np.concatenate((e_1[:,:,np.newaxis],e_2[:,:,np.newaxis],e_3[:,:,np.newaxis],\n",
        "                                   e_4[:,:,np.newaxis],e_5[:,:,np.newaxis],e_6[:,:,np.newaxis],\n",
        "                                   e_7[:,:,np.newaxis],e_8[:,:,np.newaxis],e_9[:,:,np.newaxis]),axis=2))\n",
        "\n",
        "x_test = np.array(x_test).astype(np.uint8)"
      ],
      "metadata": {
        "id": "-Nu4PbZQ7Fuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd-ErBta9-xT",
        "outputId": "04966d65-b9b8-426c-8796-ed8f0b1d3ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 32, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = Input(shape=(64, 32, 9))\n",
        "\n",
        "flattened = Flatten()(inputs)\n",
        "\n",
        "# Init 21 cores\n",
        "\n",
        "tea_0_1 = Tea(64)\n",
        "tea_0_2 = Tea(64)\n",
        "tea_0_3 = Tea(64)\n",
        "tea_0_4 = Tea(64)\n",
        "tea_0_5 = Tea(64)\n",
        "tea_0_6 = Tea(64)\n",
        "tea_0_7 = Tea(64)\n",
        "tea_0_8 = Tea(64)\n",
        "tea_0_9 = Tea(64)\n",
        "tea_0_10 = Tea(64)\n",
        "tea_0_11 = Tea(64)\n",
        "tea_0_12 = Tea(64)\n",
        "tea_0_13 = Tea(64)\n",
        "tea_0_14 = Tea(64)\n",
        "tea_0_15 = Tea(64)\n",
        "tea_0_16 = Tea(64)\n",
        "\n",
        "tea_1_1 = Tea(64)\n",
        "tea_1_2 = Tea(64)\n",
        "tea_1_3 = Tea(64)\n",
        "tea_1_4 = Tea(64)\n",
        "\n",
        "tea_2_1 = Tea(252)\n",
        "\n",
        "R_1 = Lambda(lambda x : x[:,     :1*2048 ])(flattened)\n",
        "G_1 = Lambda(lambda x : x[:, 1*2048:2*2048])(flattened)\n",
        "B_1 = Lambda(lambda x : x[:, 2*2048:3*2048])(flattened)\n",
        "\n",
        "R_2 = Lambda(lambda x : x[:, 3*2048:4*2048 ])(flattened)\n",
        "G_2 = Lambda(lambda x : x[:, 4*2048:5*2048 ])(flattened)\n",
        "B_2 = Lambda(lambda x : x[:, 5*2048:6*2048 ])(flattened)\n",
        "\n",
        "R_3 = Lambda(lambda x : x[:, 6*2048:7*2048])(flattened)\n",
        "G_3 = Lambda(lambda x : x[:, 7*2048:8*2048])(flattened)\n",
        "B_3 = Lambda(lambda x : x[:, 8*2048:9*2048])(flattened)\n",
        "\n",
        "x1_1  = Lambda(lambda x : x[:,     :256 ])(R_1)\n",
        "x2_1  = Lambda(lambda x : x[:, 119 : 375 ])(R_1)\n",
        "x3_1  = Lambda(lambda x : x[:, 238 :494 ])(R_1)\n",
        "x4_1  = Lambda(lambda x : x[:, 357 : 613])(R_1)\n",
        "x5_1  = Lambda(lambda x : x[:, 476:732])(R_1)\n",
        "x6_1  = Lambda(lambda x : x[:, 595:851])(R_1)\n",
        "x7_1  = Lambda(lambda x : x[:, 714:970])(R_1)\n",
        "x8_1  = Lambda(lambda x : x[:, 833:1089])(R_1)\n",
        "x9_1  = Lambda(lambda x : x[:, 952:1208])(R_1)\n",
        "x10_1  = Lambda(lambda x : x[:, 1071:1327])(R_1)\n",
        "x11_1  = Lambda(lambda x : x[:, 1190:1446])(R_1)\n",
        "x12_1  = Lambda(lambda x : x[:, 1309:1565])(R_1)\n",
        "x13_1  = Lambda(lambda x : x[:, 1428:1684])(R_1)\n",
        "x14_1  = Lambda(lambda x : x[:, 1547:1803])(R_1)\n",
        "x15_1  = Lambda(lambda x : x[:, 1666:1922])(R_1)\n",
        "x16_1  = Lambda(lambda x : x[:, 1785:2041])(R_1)\n",
        "\n",
        "x1_1  = tea_0_1(x1_1)\n",
        "x2_1  = tea_0_2(x2_1)\n",
        "x3_1  = tea_0_3(x3_1)\n",
        "x4_1  = tea_0_4(x4_1)\n",
        "x5_1  = tea_0_5(x5_1)\n",
        "x6_1  = tea_0_6(x6_1)\n",
        "x7_1  = tea_0_7(x7_1)\n",
        "x8_1  = tea_0_8(x8_1)\n",
        "x9_1  = tea_0_9(x9_1)\n",
        "x10_1  = tea_0_10(x10_1)\n",
        "x11_1  = tea_0_11(x11_1)\n",
        "x12_1  = tea_0_12(x12_1)\n",
        "x13_1  = tea_0_13(x13_1)\n",
        "x14_1  = tea_0_14(x14_1)\n",
        "x15_1  = tea_0_15(x15_1)\n",
        "x16_1  = tea_0_16(x16_1)\n",
        "\n",
        "x1_2  = Lambda(lambda x : x[:,     :256 ])(G_1)\n",
        "x2_2  = Lambda(lambda x : x[:, 119 : 375 ])(G_1)\n",
        "x3_2  = Lambda(lambda x : x[:, 238 :494 ])(G_1)\n",
        "x4_2  = Lambda(lambda x : x[:, 357 : 613])(G_1)\n",
        "x5_2  = Lambda(lambda x : x[:, 476:732])(G_1)\n",
        "x6_2  = Lambda(lambda x : x[:, 595:851])(G_1)\n",
        "x7_2  = Lambda(lambda x : x[:, 714:970])(G_1)\n",
        "x8_2  = Lambda(lambda x : x[:, 833:1089])(G_1)\n",
        "x9_2  = Lambda(lambda x : x[:, 952:1208])(G_1)\n",
        "x10_2  = Lambda(lambda x : x[:, 1071:1327])(G_1)\n",
        "x11_2  = Lambda(lambda x : x[:, 1190:1446])(G_1)\n",
        "x12_2  = Lambda(lambda x : x[:, 1309:1565])(G_1)\n",
        "x13_2  = Lambda(lambda x : x[:, 1428:1684])(G_1)\n",
        "x14_2  = Lambda(lambda x : x[:, 1547:1803])(G_1)\n",
        "x15_2  = Lambda(lambda x : x[:, 1666:1922])(G_1)\n",
        "x16_2  = Lambda(lambda x : x[:, 1785:2041])(G_1)\n",
        "\n",
        "x1_2  = tea_0_1(x1_2)\n",
        "x2_2  = tea_0_2(x2_2)\n",
        "x3_2  = tea_0_3(x3_2)\n",
        "x4_2  = tea_0_4(x4_2)\n",
        "x5_2  = tea_0_5(x5_2)\n",
        "x6_2  = tea_0_6(x6_2)\n",
        "x7_2  = tea_0_7(x7_2)\n",
        "x8_2  = tea_0_8(x8_2)\n",
        "x9_2  = tea_0_9(x9_2)\n",
        "x10_2  = tea_0_10(x10_2)\n",
        "x11_2  = tea_0_11(x11_2)\n",
        "x12_2  = tea_0_12(x12_2)\n",
        "x13_2  = tea_0_13(x13_2)\n",
        "x14_2  = tea_0_14(x14_2)\n",
        "x15_2  = tea_0_15(x15_2)\n",
        "x16_2  = tea_0_16(x16_2)\n",
        "\n",
        "x1_3  = Lambda(lambda x : x[:,     :256 ])(B_1)\n",
        "x2_3  = Lambda(lambda x : x[:, 119 : 375 ])(B_1)\n",
        "x3_3  = Lambda(lambda x : x[:, 238 :494 ])(B_1)\n",
        "x4_3  = Lambda(lambda x : x[:, 357 : 613])(B_1)\n",
        "x5_3  = Lambda(lambda x : x[:, 476:732])(B_1)\n",
        "x6_3  = Lambda(lambda x : x[:, 595:851])(B_1)\n",
        "x7_3  = Lambda(lambda x : x[:, 714:970])(B_1)\n",
        "x8_3  = Lambda(lambda x : x[:, 833:1089])(B_1)\n",
        "x9_3  = Lambda(lambda x : x[:, 952:1208])(B_1)\n",
        "x10_3  = Lambda(lambda x : x[:, 1071:1327])(B_1)\n",
        "x11_3  = Lambda(lambda x : x[:, 1190:1446])(B_1)\n",
        "x12_3  = Lambda(lambda x : x[:, 1309:1565])(B_1)\n",
        "x13_3  = Lambda(lambda x : x[:, 1428:1684])(B_1)\n",
        "x14_3  = Lambda(lambda x : x[:, 1547:1803])(B_1)\n",
        "x15_3  = Lambda(lambda x : x[:, 1666:1922])(B_1)\n",
        "x16_3  = Lambda(lambda x : x[:, 1785:2041])(B_1)\n",
        "\n",
        "x1_3  = tea_0_1(x1_3)\n",
        "x2_3  = tea_0_2(x2_3)\n",
        "x3_3  = tea_0_3(x3_3)\n",
        "x4_3  = tea_0_4(x4_3)\n",
        "x5_3  = tea_0_5(x5_3)\n",
        "x6_3  = tea_0_6(x6_3)\n",
        "x7_3  = tea_0_7(x7_3)\n",
        "x8_3  = tea_0_8(x8_3)\n",
        "x9_3  = tea_0_9(x9_3)\n",
        "x10_3  = tea_0_10(x10_3)\n",
        "x11_3  = tea_0_11(x11_3)\n",
        "x12_3  = tea_0_12(x12_3)\n",
        "x13_3  = tea_0_13(x13_3)\n",
        "x14_3  = tea_0_14(x14_3)\n",
        "x15_3  = tea_0_15(x15_3)\n",
        "x16_3  = tea_0_16(x16_3)\n",
        "\n",
        "### 2 ###\n",
        "\n",
        "x1_4  = Lambda(lambda x : x[:,     :256 ])(R_2)\n",
        "x2_4  = Lambda(lambda x : x[:, 119 : 375 ])(R_2)\n",
        "x3_4  = Lambda(lambda x : x[:, 238 :494 ])(R_2)\n",
        "x4_4  = Lambda(lambda x : x[:, 357 : 613])(R_2)\n",
        "x5_4  = Lambda(lambda x : x[:, 476:732])(R_2)\n",
        "x6_4  = Lambda(lambda x : x[:, 595:851])(R_2)\n",
        "x7_4  = Lambda(lambda x : x[:, 714:970])(R_2)\n",
        "x8_4  = Lambda(lambda x : x[:, 833:1089])(R_2)\n",
        "x9_4  = Lambda(lambda x : x[:, 952:1208])(R_2)\n",
        "x10_4  = Lambda(lambda x : x[:, 1071:1327])(R_2)\n",
        "x11_4  = Lambda(lambda x : x[:, 1190:1446])(R_2)\n",
        "x12_4  = Lambda(lambda x : x[:, 1309:1565])(R_2)\n",
        "x13_4  = Lambda(lambda x : x[:, 1428:1684])(R_2)\n",
        "x14_4  = Lambda(lambda x : x[:, 1547:1803])(R_2)\n",
        "x15_4  = Lambda(lambda x : x[:, 1666:1922])(R_2)\n",
        "x16_4  = Lambda(lambda x : x[:, 1785:2041])(R_2)\n",
        "\n",
        "x1_4  = tea_0_1(x1_4)\n",
        "x2_4  = tea_0_2(x2_4)\n",
        "x3_4  = tea_0_3(x3_4)\n",
        "x4_4  = tea_0_4(x4_4)\n",
        "x5_4  = tea_0_5(x5_4)\n",
        "x6_4  = tea_0_6(x6_4)\n",
        "x7_4  = tea_0_7(x7_4)\n",
        "x8_4  = tea_0_8(x8_4)\n",
        "x9_4  = tea_0_9(x9_4)\n",
        "x10_4  = tea_0_10(x10_4)\n",
        "x11_4  = tea_0_11(x11_4)\n",
        "x12_4  = tea_0_12(x12_4)\n",
        "x13_4  = tea_0_13(x13_4)\n",
        "x14_4  = tea_0_14(x14_4)\n",
        "x15_4  = tea_0_15(x15_4)\n",
        "x16_4  = tea_0_16(x16_4)\n",
        "\n",
        "x1_5  = Lambda(lambda x : x[:,     :256 ])(G_2)\n",
        "x2_5  = Lambda(lambda x : x[:, 119 : 375 ])(G_2)\n",
        "x3_5  = Lambda(lambda x : x[:, 238 :494 ])(G_2)\n",
        "x4_5  = Lambda(lambda x : x[:, 357 : 613])(G_2)\n",
        "x5_5  = Lambda(lambda x : x[:, 476:732])(G_2)\n",
        "x6_5  = Lambda(lambda x : x[:, 595:851])(G_2)\n",
        "x7_5  = Lambda(lambda x : x[:, 714:970])(G_2)\n",
        "x8_5  = Lambda(lambda x : x[:, 833:1089])(G_2)\n",
        "x9_5  = Lambda(lambda x : x[:, 952:1208])(G_2)\n",
        "x10_5  = Lambda(lambda x : x[:, 1071:1327])(G_2)\n",
        "x11_5  = Lambda(lambda x : x[:, 1190:1446])(G_2)\n",
        "x12_5  = Lambda(lambda x : x[:, 1309:1565])(G_2)\n",
        "x13_5  = Lambda(lambda x : x[:, 1428:1684])(G_2)\n",
        "x14_5  = Lambda(lambda x : x[:, 1547:1803])(G_2)\n",
        "x15_5  = Lambda(lambda x : x[:, 1666:1922])(G_2)\n",
        "x16_5  = Lambda(lambda x : x[:, 1785:2041])(G_2)\n",
        "\n",
        "x1_5  = tea_0_1(x1_5)\n",
        "x2_5  = tea_0_2(x2_5)\n",
        "x3_5  = tea_0_3(x3_5)\n",
        "x4_5  = tea_0_4(x4_5)\n",
        "x5_5  = tea_0_5(x5_5)\n",
        "x6_5  = tea_0_6(x6_5)\n",
        "x7_5  = tea_0_7(x7_5)\n",
        "x8_5  = tea_0_8(x8_5)\n",
        "x9_5  = tea_0_9(x9_5)\n",
        "x10_5  = tea_0_10(x10_5)\n",
        "x11_5  = tea_0_11(x11_5)\n",
        "x12_5  = tea_0_12(x12_5)\n",
        "x13_5  = tea_0_13(x13_5)\n",
        "x14_5  = tea_0_14(x14_5)\n",
        "x15_5  = tea_0_15(x15_5)\n",
        "x16_5  = tea_0_16(x16_5)\n",
        "\n",
        "x1_6  = Lambda(lambda x : x[:,     :256 ])(B_2)\n",
        "x2_6  = Lambda(lambda x : x[:, 119 : 375 ])(B_2)\n",
        "x3_6  = Lambda(lambda x : x[:, 238 :494 ])(B_2)\n",
        "x4_6  = Lambda(lambda x : x[:, 357 : 613])(B_2)\n",
        "x5_6  = Lambda(lambda x : x[:, 476:732])(B_2)\n",
        "x6_6  = Lambda(lambda x : x[:, 595:851])(B_2)\n",
        "x7_6  = Lambda(lambda x : x[:, 714:970])(B_2)\n",
        "x8_6  = Lambda(lambda x : x[:, 833:1089])(B_2)\n",
        "x9_6  = Lambda(lambda x : x[:, 952:1208])(B_2)\n",
        "x10_6  = Lambda(lambda x : x[:, 1071:1327])(B_2)\n",
        "x11_6  = Lambda(lambda x : x[:, 1190:1446])(B_2)\n",
        "x12_6  = Lambda(lambda x : x[:, 1309:1565])(B_2)\n",
        "x13_6  = Lambda(lambda x : x[:, 1428:1684])(B_2)\n",
        "x14_6  = Lambda(lambda x : x[:, 1547:1803])(B_2)\n",
        "x15_6  = Lambda(lambda x : x[:, 1666:1922])(B_2)\n",
        "x16_6  = Lambda(lambda x : x[:, 1785:2041])(B_2)\n",
        "\n",
        "x1_6  = tea_0_1(x1_6)\n",
        "x2_6  = tea_0_2(x2_6)\n",
        "x3_6  = tea_0_3(x3_6)\n",
        "x4_6  = tea_0_4(x4_6)\n",
        "x5_6  = tea_0_5(x5_6)\n",
        "x6_6  = tea_0_6(x6_6)\n",
        "x7_6  = tea_0_7(x7_6)\n",
        "x8_6  = tea_0_8(x8_6)\n",
        "x9_6  = tea_0_9(x9_6)\n",
        "x10_6  = tea_0_10(x10_6)\n",
        "x11_6  = tea_0_11(x11_6)\n",
        "x12_6  = tea_0_12(x12_6)\n",
        "x13_6  = tea_0_13(x13_6)\n",
        "x14_6  = tea_0_14(x14_6)\n",
        "x15_6  = tea_0_15(x15_6)\n",
        "x16_6  = tea_0_16(x16_6)\n",
        "\n",
        "### 3 ###\n",
        "\n",
        "x1_7  = Lambda(lambda x : x[:,     :256 ])(R_3)\n",
        "x2_7  = Lambda(lambda x : x[:, 119 : 375 ])(R_3)\n",
        "x3_7  = Lambda(lambda x : x[:, 238 :494 ])(R_3)\n",
        "x4_7  = Lambda(lambda x : x[:, 357 : 613])(R_3)\n",
        "x5_7  = Lambda(lambda x : x[:, 476:732])(R_3)\n",
        "x6_7  = Lambda(lambda x : x[:, 595:851])(R_3)\n",
        "x7_7  = Lambda(lambda x : x[:, 714:970])(R_3)\n",
        "x8_7  = Lambda(lambda x : x[:, 833:1089])(R_3)\n",
        "x9_7  = Lambda(lambda x : x[:, 952:1208])(R_3)\n",
        "x10_7  = Lambda(lambda x : x[:, 1071:1327])(R_3)\n",
        "x11_7  = Lambda(lambda x : x[:, 1190:1446])(R_3)\n",
        "x12_7  = Lambda(lambda x : x[:, 1309:1565])(R_3)\n",
        "x13_7  = Lambda(lambda x : x[:, 1428:1684])(R_3)\n",
        "x14_7  = Lambda(lambda x : x[:, 1547:1803])(R_3)\n",
        "x15_7  = Lambda(lambda x : x[:, 1666:1922])(R_3)\n",
        "x16_7  = Lambda(lambda x : x[:, 1785:2041])(R_3)\n",
        "\n",
        "x1_7  = tea_0_1(x1_7)\n",
        "x2_7  = tea_0_2(x2_7)\n",
        "x3_7  = tea_0_3(x3_7)\n",
        "x4_7  = tea_0_4(x4_7)\n",
        "x5_7  = tea_0_5(x5_7)\n",
        "x6_7  = tea_0_6(x6_7)\n",
        "x7_7  = tea_0_7(x7_7)\n",
        "x8_7  = tea_0_8(x8_7)\n",
        "x9_7  = tea_0_9(x9_7)\n",
        "x10_7  = tea_0_10(x10_7)\n",
        "x11_7  = tea_0_11(x11_7)\n",
        "x12_7  = tea_0_12(x12_7)\n",
        "x13_7  = tea_0_13(x13_7)\n",
        "x14_7  = tea_0_14(x14_7)\n",
        "x15_7  = tea_0_15(x15_7)\n",
        "x16_7  = tea_0_16(x16_7)\n",
        "\n",
        "x1_8  = Lambda(lambda x : x[:,     :256 ])(G_3)\n",
        "x2_8  = Lambda(lambda x : x[:, 119 : 375 ])(G_3)\n",
        "x3_8  = Lambda(lambda x : x[:, 238 :494 ])(G_3)\n",
        "x4_8  = Lambda(lambda x : x[:, 357 : 613])(G_3)\n",
        "x5_8  = Lambda(lambda x : x[:, 476:732])(G_3)\n",
        "x6_8  = Lambda(lambda x : x[:, 595:851])(G_3)\n",
        "x7_8  = Lambda(lambda x : x[:, 714:970])(G_3)\n",
        "x8_8  = Lambda(lambda x : x[:, 833:1089])(G_3)\n",
        "x9_8  = Lambda(lambda x : x[:, 952:1208])(G_3)\n",
        "x10_8  = Lambda(lambda x : x[:, 1071:1327])(G_3)\n",
        "x11_8  = Lambda(lambda x : x[:, 1190:1446])(G_3)\n",
        "x12_8  = Lambda(lambda x : x[:, 1309:1565])(G_3)\n",
        "x13_8  = Lambda(lambda x : x[:, 1428:1684])(G_3)\n",
        "x14_8  = Lambda(lambda x : x[:, 1547:1803])(G_3)\n",
        "x15_8  = Lambda(lambda x : x[:, 1666:1922])(G_3)\n",
        "x16_8  = Lambda(lambda x : x[:, 1785:2041])(G_3)\n",
        "\n",
        "x1_8  = tea_0_1(x1_8)\n",
        "x2_8  = tea_0_2(x2_8)\n",
        "x3_8  = tea_0_3(x3_8)\n",
        "x4_8  = tea_0_4(x4_8)\n",
        "x5_8  = tea_0_5(x5_8)\n",
        "x6_8  = tea_0_6(x6_8)\n",
        "x7_8  = tea_0_7(x7_8)\n",
        "x8_8  = tea_0_8(x8_8)\n",
        "x9_8  = tea_0_9(x9_8)\n",
        "x10_8  = tea_0_10(x10_8)\n",
        "x11_8  = tea_0_11(x11_8)\n",
        "x12_8  = tea_0_12(x12_8)\n",
        "x13_8  = tea_0_13(x13_8)\n",
        "x14_8  = tea_0_14(x14_8)\n",
        "x15_8  = tea_0_15(x15_8)\n",
        "x16_8  = tea_0_16(x16_8)\n",
        "\n",
        "x1_9  = Lambda(lambda x : x[:,     :256 ])(B_3)\n",
        "x2_9  = Lambda(lambda x : x[:, 119 : 375 ])(B_3)\n",
        "x3_9  = Lambda(lambda x : x[:, 238 :494 ])(B_3)\n",
        "x4_9  = Lambda(lambda x : x[:, 357 : 613])(B_3)\n",
        "x5_9  = Lambda(lambda x : x[:, 476:732])(B_3)\n",
        "x6_9  = Lambda(lambda x : x[:, 595:851])(B_3)\n",
        "x7_9  = Lambda(lambda x : x[:, 714:970])(B_3)\n",
        "x8_9  = Lambda(lambda x : x[:, 833:1089])(B_3)\n",
        "x9_9  = Lambda(lambda x : x[:, 952:1208])(B_3)\n",
        "x10_9  = Lambda(lambda x : x[:, 1071:1327])(B_3)\n",
        "x11_9  = Lambda(lambda x : x[:, 1190:1446])(B_3)\n",
        "x12_9  = Lambda(lambda x : x[:, 1309:1565])(B_3)\n",
        "x13_9  = Lambda(lambda x : x[:, 1428:1684])(B_3)\n",
        "x14_9  = Lambda(lambda x : x[:, 1547:1803])(B_3)\n",
        "x15_9  = Lambda(lambda x : x[:, 1666:1922])(B_3)\n",
        "x16_9  = Lambda(lambda x : x[:, 1785:2041])(B_3)\n",
        "\n",
        "x1_9  = tea_0_1(x1_9)\n",
        "x2_9  = tea_0_2(x2_9)\n",
        "x3_9  = tea_0_3(x3_9)\n",
        "x4_9  = tea_0_4(x4_9)\n",
        "x5_9  = tea_0_5(x5_9)\n",
        "x6_9  = tea_0_6(x6_9)\n",
        "x7_9  = tea_0_7(x7_9)\n",
        "x8_9  = tea_0_8(x8_9)\n",
        "x9_9  = tea_0_9(x9_9)\n",
        "x10_9  = tea_0_10(x10_9)\n",
        "x11_9  = tea_0_11(x11_9)\n",
        "x12_9  = tea_0_12(x12_9)\n",
        "x13_9  = tea_0_13(x13_9)\n",
        "x14_9  = tea_0_14(x14_9)\n",
        "x15_9  = tea_0_15(x15_9)\n",
        "x16_9  = tea_0_16(x16_9)\n",
        "\n",
        "#Average 1->5\n",
        "x1_1_1 = Average()([x1_1,x1_2,x1_3,x1_4,x1_5])\n",
        "x2_1_1 = Average()([x2_1,x2_2,x2_3,x2_4,x2_5])\n",
        "x3_1_1 = Average()([x3_1,x3_2,x3_3,x3_4,x3_5])\n",
        "x4_1_1 = Average()([x4_1,x4_2,x4_3,x4_4,x4_5])\n",
        "x5_1_1 = Average()([x5_1,x5_2,x5_3,x5_4,x5_5])\n",
        "x6_1_1 = Average()([x6_1,x6_2,x6_3,x6_4,x6_5])\n",
        "x7_1_1 = Average()([x7_1,x7_2,x7_3,x7_4,x7_5])\n",
        "x8_1_1 = Average()([x8_1,x8_2,x8_3,x8_4,x8_5])\n",
        "x9_1_1 = Average()([x9_1,x9_2,x9_3,x9_4,x9_5])\n",
        "x10_1_1 = Average()([x10_1,x10_2,x10_3,x10_4,x10_5])\n",
        "x11_1_1 = Average()([x11_1,x11_2,x11_3,x11_4,x11_5])\n",
        "x12_1_1 = Average()([x12_1,x12_2,x12_3,x12_4,x12_5])\n",
        "x13_1_1 = Average()([x13_1,x13_2,x13_3,x13_4,x13_5])\n",
        "x14_1_1 = Average()([x14_1,x14_2,x14_3,x14_4,x14_5])\n",
        "x15_1_1 = Average()([x15_1,x15_2,x15_3,x15_4,x15_5])\n",
        "x16_1_1 = Average()([x16_1,x16_2,x16_3,x16_4,x16_5])\n",
        "\n",
        "#Average image 2->6\n",
        "\n",
        "x1_1_2 = Average()([x1_2,x1_3,x1_4,x1_5,x1_6])\n",
        "x2_1_2 = Average()([x2_2,x2_3,x2_4,x2_5,x2_6])\n",
        "x3_1_2 = Average()([x3_2,x3_3,x3_4,x3_5,x3_6])\n",
        "x4_1_2 = Average()([x4_2,x4_3,x4_4,x4_5,x4_6])\n",
        "x5_1_2 = Average()([x5_2,x5_3,x5_4,x5_5,x5_6])\n",
        "x6_1_2 = Average()([x6_2,x6_3,x6_4,x6_5,x6_6])\n",
        "x7_1_2 = Average()([x7_2,x7_3,x7_4,x7_5,x7_6])\n",
        "x8_1_2 = Average()([x8_2,x8_3,x8_4,x8_5,x8_6])\n",
        "x9_1_2 = Average()([x9_2,x9_3,x9_4,x9_5,x9_6])\n",
        "x10_1_2 = Average()([x10_2,x10_3,x10_4,x10_5,x10_6])\n",
        "x11_1_2 = Average()([x11_2,x11_3,x11_4,x11_5,x11_6])\n",
        "x12_1_2 = Average()([x12_2,x12_3,x12_4,x12_5,x12_6])\n",
        "x13_1_2 = Average()([x13_2,x13_3,x13_4,x13_5,x13_6])\n",
        "x14_1_2 = Average()([x14_2,x14_3,x14_4,x14_5,x14_6])\n",
        "x15_1_2 = Average()([x15_2,x15_3,x15_4,x15_5,x15_6])\n",
        "x16_1_2 = Average()([x16_2,x16_3,x16_4,x16_5,x16_6])\n",
        "\n",
        "#Average image 3->7\n",
        "\n",
        "x1_1_3 = Average()([x1_3,x1_4,x1_5,x1_6,x1_7])\n",
        "x2_1_3 = Average()([x2_3,x2_4,x2_5,x2_6,x2_7])\n",
        "x3_1_3 = Average()([x3_3,x3_4,x3_5,x3_6,x3_7])\n",
        "x4_1_3 = Average()([x4_3,x4_4,x4_5,x4_6,x4_7])\n",
        "x5_1_3 = Average()([x5_3,x5_4,x5_5,x5_6,x5_7])\n",
        "x6_1_3 = Average()([x6_3,x6_4,x6_5,x6_6,x6_7])\n",
        "x7_1_3 = Average()([x7_3,x7_4,x7_5,x7_6,x7_7])\n",
        "x8_1_3 = Average()([x8_3,x8_4,x8_5,x8_6,x8_7])\n",
        "x9_1_3 = Average()([x9_3,x9_4,x9_5,x9_6,x9_7])\n",
        "x10_1_3 = Average()([x10_3,x10_4,x10_5,x10_6,x10_7])\n",
        "x11_1_3 = Average()([x11_3,x11_4,x11_5,x11_6,x11_7])\n",
        "x12_1_3 = Average()([x12_3,x12_4,x12_5,x12_6,x12_7])\n",
        "x13_1_3 = Average()([x13_3,x13_4,x13_5,x13_6,x13_7])\n",
        "x14_1_3 = Average()([x14_3,x14_4,x14_5,x14_6,x14_7])\n",
        "x15_1_3 = Average()([x15_3,x15_4,x15_5,x15_6,x15_7])\n",
        "x16_1_3 = Average()([x16_3,x16_4,x16_5,x16_6,x16_7])\n",
        "\n",
        "#Average image 4->8\n",
        "\n",
        "x1_1_4 = Average()([x1_4,x1_5,x1_6,x1_7,x1_8])\n",
        "x2_1_4 = Average()([x2_4,x2_5,x2_6,x2_7,x2_8])\n",
        "x3_1_4 = Average()([x3_4,x3_5,x3_6,x3_7,x3_8])\n",
        "x4_1_4 = Average()([x4_4,x4_5,x4_6,x4_7,x4_8])\n",
        "x5_1_4 = Average()([x5_4,x5_5,x5_6,x5_7,x5_8])\n",
        "x6_1_4 = Average()([x6_4,x6_5,x6_6,x6_7,x6_8])\n",
        "x7_1_4 = Average()([x7_4,x7_5,x7_6,x7_7,x7_8])\n",
        "x8_1_4 = Average()([x8_4,x8_5,x8_6,x8_7,x8_8])\n",
        "x9_1_4 = Average()([x9_4,x9_5,x9_6,x9_7,x9_8])\n",
        "x10_1_4 = Average()([x10_4,x10_5,x10_6,x10_7,x10_8])\n",
        "x11_1_4 = Average()([x11_4,x11_5,x11_6,x11_7,x11_8])\n",
        "x12_1_4 = Average()([x12_4,x12_5,x12_6,x12_7,x12_8])\n",
        "x13_1_4 = Average()([x13_4,x13_5,x13_6,x13_7,x13_8])\n",
        "x14_1_4 = Average()([x14_4,x14_5,x14_6,x14_7,x14_8])\n",
        "x15_1_4 = Average()([x15_4,x15_5,x15_6,x15_7,x15_8])\n",
        "x16_1_4 = Average()([x16_4,x16_5,x16_6,x16_7,x16_8])\n",
        "\n",
        "#Average image 5->9\n",
        "\n",
        "x1_1_5 = Average()([x1_5,x1_6,x1_7,x1_8,x1_9])\n",
        "x2_1_5 = Average()([x2_5,x2_6,x2_7,x2_8,x2_9])\n",
        "x3_1_5 = Average()([x3_5,x3_6,x3_7,x3_8,x3_9])\n",
        "x4_1_5 = Average()([x4_5,x4_6,x4_7,x4_8,x4_9])\n",
        "x5_1_5 = Average()([x5_5,x5_6,x5_7,x5_8,x5_9])\n",
        "x6_1_5 = Average()([x6_5,x6_6,x6_7,x6_8,x6_9])\n",
        "x7_1_5 = Average()([x7_5,x7_6,x7_7,x7_8,x7_9])\n",
        "x8_1_5 = Average()([x8_5,x8_6,x8_7,x8_8,x8_9])\n",
        "x9_1_5 = Average()([x9_5,x9_6,x9_7,x9_6,x9_9])\n",
        "x10_1_5 = Average()([x10_5,x10_6,x10_7,x10_8,x10_9])\n",
        "x11_1_5 = Average()([x11_5,x11_6,x11_7,x11_8,x11_9])\n",
        "x12_1_5 = Average()([x12_5,x12_6,x12_7,x12_8,x12_9])\n",
        "x13_1_5 = Average()([x13_5,x13_6,x13_7,x13_8,x13_9])\n",
        "x14_1_5 = Average()([x14_5,x14_6,x14_7,x14_8,x14_9])\n",
        "x15_1_5 = Average()([x15_5,x15_6,x15_7,x15_8,x15_9])\n",
        "x16_1_5 = Average()([x16_5,x16_6,x16_7,x16_8,x16_9])\n",
        "x1_1 = concatenate(([x1_1_1,x2_1_1,x3_1_1,x4_1_1]),axis=1)\n",
        "x2_1 = concatenate(([x5_1_1,x6_1_1,x7_1_1,x8_1_1]),axis=1)\n",
        "x3_1 = concatenate(([x9_1_1,x10_1_1,x11_1_1,x12_1_1]),axis=1)\n",
        "x4_1 = concatenate(([x13_1_1,x14_1_1,x15_1_1,x16_1_1]),axis=1)\n",
        "\n",
        "x1_2 = concatenate(([x1_1_2,x2_1_2,x3_1_2,x4_1_2]),axis=1)\n",
        "x2_2 = concatenate(([x5_1_2,x6_1_2,x7_1_2,x8_1_2]),axis=1)\n",
        "x3_2 = concatenate(([x9_1_2,x10_1_2,x11_1_2,x12_1_2]),axis=1)\n",
        "x4_2 = concatenate(([x13_1_2,x14_1_2,x15_1_2,x16_1_2]),axis=1)\n",
        "\n",
        "x1_3 = concatenate(([x1_1_3,x2_1_3,x3_1_3,x4_1_3]),axis=1)\n",
        "x2_3 = concatenate(([x5_1_3,x6_1_3,x7_1_3,x8_1_3]),axis=1)\n",
        "x3_3 = concatenate(([x9_1_3,x10_1_3,x11_1_3,x12_1_3]),axis=1)\n",
        "x4_3 = concatenate(([x13_1_3,x14_1_3,x15_1_3,x16_1_3]),axis=1)\n",
        "\n",
        "x1_4 = concatenate(([x1_1_4,x2_1_4,x3_1_4,x4_1_4]),axis=1)\n",
        "x2_4 = concatenate(([x5_1_4,x6_1_4,x7_1_4,x8_1_4]),axis=1)\n",
        "x3_4 = concatenate(([x9_1_4,x10_1_4,x11_1_4,x12_1_4]),axis=1)\n",
        "x4_4 = concatenate(([x13_1_4,x14_1_4,x15_1_4,x16_1_4]),axis=1)\n",
        "\n",
        "x1_5 = concatenate(([x1_1_5,x2_1_5,x3_1_5,x4_1_5]),axis=1)\n",
        "x2_5 = concatenate(([x5_1_5,x6_1_5,x7_1_5,x8_1_5]),axis=1)\n",
        "x3_5 = concatenate(([x9_1_5,x10_1_5,x11_1_5,x12_1_5]),axis=1)\n",
        "x4_5 = concatenate(([x13_1_5,x14_1_5,x15_1_5,x16_1_5]),axis=1)\n",
        "\n",
        "x1_1 = tea_1_1(x1_1)\n",
        "x2_1 = tea_1_2(x2_1)\n",
        "x3_1 = tea_1_3(x3_1)\n",
        "x4_1 = tea_1_4(x4_1)\n",
        "\n",
        "x1_2 = tea_1_1(x1_2)\n",
        "x2_2 = tea_1_2(x2_2)\n",
        "x3_2 = tea_1_3(x3_2)\n",
        "x4_2 = tea_1_4(x4_2)\n",
        "\n",
        "x1_3 = tea_1_1(x1_3)\n",
        "x2_3 = tea_1_2(x2_3)\n",
        "x3_3 = tea_1_3(x3_3)\n",
        "x4_3 = tea_1_4(x4_3)\n",
        "\n",
        "x1_4 = tea_1_1(x1_4)\n",
        "x2_4 = tea_1_2(x2_4)\n",
        "x3_4 = tea_1_3(x3_4)\n",
        "x4_4 = tea_1_4(x4_4)\n",
        "\n",
        "x1_5 = tea_1_1(x1_5)\n",
        "x2_5 = tea_1_2(x2_5)\n",
        "x3_5 = tea_1_3(x3_5)\n",
        "x4_5 = tea_1_4(x4_5)\n",
        "#Average after layer 2\n",
        "x_out_1 = Average()([x1_1,x1_2,x1_3,x1_4,x1_5])\n",
        "x_out_2 = Average()([x2_1,x2_2,x2_3,x2_4,x2_5])\n",
        "x_out_3 = Average()([x3_1,x3_2,x3_3,x3_4,x3_5])\n",
        "x_out_4 = Average()([x4_1,x4_2,x4_3,x4_4,x4_5])\n",
        "\n",
        "# Layer 3\n",
        "\n",
        "x_out = concatenate(([x_out_1,x_out_2,x_out_3,x_out_4]),axis=1)\n",
        "\n",
        "x_out = tea_2_1(x_out) # 252 divided by 9\n",
        "\n",
        "x_out = AdditivePooling(9)(x_out)\n",
        "\n",
        "predictions = Activation('softmax')(x_out)\n",
        "#Model\n",
        "\n",
        "model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=0.0001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=30,\n",
        "          verbose=1,\n",
        "          validation_split=0.1)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vb1IyLe-jCp",
        "outputId": "c6f7c3be-d89c-43d8-c4c9-b97a01f44157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 7459 samples, validate on 829 samples\n",
            "Epoch 1/30\n",
            "7459/7459 [==============================] - ETA: 0s - loss: 1.9501 - acc: 0.3303"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7459/7459 [==============================] - 14s 2ms/sample - loss: 1.9501 - acc: 0.3303 - val_loss: 1.9345 - val_acc: 0.3462\n",
            "Epoch 2/30\n",
            "7459/7459 [==============================] - 6s 821us/sample - loss: 0.9140 - acc: 0.6807 - val_loss: 1.2530 - val_acc: 0.5718\n",
            "Epoch 3/30\n",
            "7459/7459 [==============================] - 6s 817us/sample - loss: 0.4960 - acc: 0.8484 - val_loss: 0.9490 - val_acc: 0.6550\n",
            "Epoch 4/30\n",
            "7459/7459 [==============================] - 6s 822us/sample - loss: 0.3034 - acc: 0.9158 - val_loss: 0.6877 - val_acc: 0.7467\n",
            "Epoch 5/30\n",
            "7459/7459 [==============================] - 6s 820us/sample - loss: 0.2167 - acc: 0.9394 - val_loss: 0.5160 - val_acc: 0.8106\n",
            "Epoch 6/30\n",
            "7459/7459 [==============================] - 6s 810us/sample - loss: 0.1569 - acc: 0.9617 - val_loss: 0.3622 - val_acc: 0.8818\n",
            "Epoch 7/30\n",
            "7459/7459 [==============================] - 6s 813us/sample - loss: 0.1124 - acc: 0.9736 - val_loss: 0.3099 - val_acc: 0.8938\n",
            "Epoch 8/30\n",
            "7459/7459 [==============================] - 6s 812us/sample - loss: 0.0803 - acc: 0.9846 - val_loss: 0.1916 - val_acc: 0.9397\n",
            "Epoch 9/30\n",
            "7459/7459 [==============================] - 6s 805us/sample - loss: 0.0670 - acc: 0.9846 - val_loss: 0.2161 - val_acc: 0.9192\n",
            "Epoch 10/30\n",
            "7459/7459 [==============================] - 6s 808us/sample - loss: 0.0528 - acc: 0.9897 - val_loss: 0.2634 - val_acc: 0.9228\n",
            "Epoch 11/30\n",
            "7459/7459 [==============================] - 6s 803us/sample - loss: 0.0408 - acc: 0.9930 - val_loss: 0.1495 - val_acc: 0.9433\n",
            "Epoch 12/30\n",
            "7459/7459 [==============================] - 6s 804us/sample - loss: 0.0399 - acc: 0.9914 - val_loss: 0.1461 - val_acc: 0.9481\n",
            "Epoch 13/30\n",
            "7459/7459 [==============================] - 6s 815us/sample - loss: 0.0332 - acc: 0.9946 - val_loss: 0.1460 - val_acc: 0.9385\n",
            "Epoch 14/30\n",
            "7459/7459 [==============================] - 6s 801us/sample - loss: 0.0347 - acc: 0.9938 - val_loss: 0.1205 - val_acc: 0.9650\n",
            "Epoch 15/30\n",
            "7459/7459 [==============================] - 6s 812us/sample - loss: 0.0216 - acc: 0.9965 - val_loss: 0.1535 - val_acc: 0.9469\n",
            "Epoch 16/30\n",
            "7459/7459 [==============================] - 6s 805us/sample - loss: 0.0333 - acc: 0.9924 - val_loss: 0.1489 - val_acc: 0.9542\n",
            "Epoch 17/30\n",
            "7459/7459 [==============================] - 6s 804us/sample - loss: 0.0180 - acc: 0.9975 - val_loss: 0.2109 - val_acc: 0.9312\n",
            "Epoch 18/30\n",
            "7459/7459 [==============================] - 6s 814us/sample - loss: 0.0139 - acc: 0.9984 - val_loss: 0.1498 - val_acc: 0.9481\n",
            "Epoch 19/30\n",
            "7459/7459 [==============================] - 6s 813us/sample - loss: 0.0166 - acc: 0.9968 - val_loss: 0.1000 - val_acc: 0.9638\n",
            "Epoch 20/30\n",
            "7459/7459 [==============================] - 6s 812us/sample - loss: 0.0180 - acc: 0.9965 - val_loss: 0.1296 - val_acc: 0.9481\n",
            "Epoch 21/30\n",
            "7459/7459 [==============================] - 6s 822us/sample - loss: 0.0117 - acc: 0.9984 - val_loss: 0.0599 - val_acc: 0.9783\n",
            "Epoch 22/30\n",
            "7459/7459 [==============================] - 6s 819us/sample - loss: 0.0117 - acc: 0.9980 - val_loss: 0.0698 - val_acc: 0.9747\n",
            "Epoch 23/30\n",
            "7459/7459 [==============================] - 6s 820us/sample - loss: 0.0112 - acc: 0.9987 - val_loss: 0.1107 - val_acc: 0.9686\n",
            "Epoch 24/30\n",
            "7459/7459 [==============================] - 6s 812us/sample - loss: 0.0097 - acc: 0.9985 - val_loss: 0.0825 - val_acc: 0.9783\n",
            "Epoch 25/30\n",
            "7459/7459 [==============================] - 6s 817us/sample - loss: 0.0123 - acc: 0.9979 - val_loss: 0.0683 - val_acc: 0.9771\n",
            "Epoch 26/30\n",
            "7459/7459 [==============================] - 6s 820us/sample - loss: 0.0065 - acc: 0.9989 - val_loss: 0.0748 - val_acc: 0.9759\n",
            "Epoch 27/30\n",
            "7459/7459 [==============================] - 6s 822us/sample - loss: 0.0069 - acc: 0.9989 - val_loss: 0.0776 - val_acc: 0.9747\n",
            "Epoch 28/30\n",
            "7459/7459 [==============================] - 6s 837us/sample - loss: 0.0091 - acc: 0.9983 - val_loss: 0.0680 - val_acc: 0.9771\n",
            "Epoch 29/30\n",
            "7459/7459 [==============================] - 6s 826us/sample - loss: 0.0116 - acc: 0.9977 - val_loss: 0.0845 - val_acc: 0.9710\n",
            "Epoch 30/30\n",
            "7459/7459 [==============================] - 6s 844us/sample - loss: 0.0163 - acc: 0.9960 - val_loss: 0.0585 - val_acc: 0.9783\n",
            "Test loss: 0.053698891395628126\n",
            "Test accuracy: 0.98479915\n"
          ]
        }
      ]
    }
  ]
}