{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trafalgardlaw2406/RANC/blob/main/Serial_10folds_HE_HM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR7d89xDOjj8",
        "outputId": "c7348272-cb3b-49c1-8cb7-b9f63d8db502"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc09mcem9tAI",
        "outputId": "d1453a71-b07b-4491-f9fe-fce8a4b3a432"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import operator\n",
        "import functools\n",
        "import math\n",
        "import os\n",
        "\n",
        "from scipy import ndimage\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Activation, Input, Lambda, concatenate,Average\n",
        "from tensorflow.keras.datasets import mnist,fashion_mnist\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import constraints\n",
        "import sys\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbJ77QTN4KCP"
      },
      "outputs": [],
      "source": [
        "class Tea(Layer):\n",
        "    def __init__(self,\n",
        "                 units,\n",
        "                 **kwargs):\n",
        "        \"\"\"Initializes a new TeaLayer.\n",
        "\n",
        "        Arguments:\n",
        "            units -- The number of neurons to use for this layer.\"\"\"\n",
        "        self.units = units\n",
        "        # Needs to be set to `True` to use the `K.in_train_phase` function.\n",
        "        self.uses_learning_phase = True\n",
        "        super(Tea, self).__init__(**kwargs)\n",
        "\n",
        "def tea_weight_initializer(shape, dtype=np.float32):\n",
        "    \"\"\"Returns a tensor of alternating 1s and -1s, which is (kind of like)\n",
        "    how IBM initializes their weight matrix in their TeaLearning\n",
        "    literature.\n",
        "\n",
        "    Arguments:\n",
        "        shape -- The shape of the weights to intialize.\n",
        "\n",
        "    Keyword Arguments:\n",
        "        dtype -- The data type to use to initialize the weights.\n",
        "                 (default: {np.float32})\"\"\"\n",
        "    num_axons = shape[0]\n",
        "    num_neurons = shape[1]\n",
        "    ret_array = np.zeros((int(num_axons), int(num_neurons)), dtype=np.float32)\n",
        "    for axon_num, axon in enumerate(ret_array):\n",
        "        if axon_num % 2 == 0:\n",
        "            for i in range(len(axon)):\n",
        "                ret_array[axon_num][i] = 1\n",
        "        else:\n",
        "            for i in range(len(axon)):\n",
        "                ret_array[axon_num][i] = -1\n",
        "    return tf.convert_to_tensor(ret_array)\n",
        "\n",
        "def build(self, input_shape):\n",
        "    assert len(input_shape) >= 2\n",
        "    shape = (input_shape[-1], self.units)\n",
        "    self.static_weights = self.add_weight(\n",
        "        name='weights',\n",
        "        shape=shape,\n",
        "        initializer=tea_weight_initializer,\n",
        "        trainable=False)\n",
        "    # Intialize connections around 0.5 because they represent probabilities.\n",
        "    self.connections = self.add_weight(\n",
        "        name='connections',\n",
        "        initializer=initializers.TruncatedNormal(mean=0.5),\n",
        "        shape=shape)\n",
        "    self.biases = self.add_weight(\n",
        "        name='biases',\n",
        "        initializer='zeros',\n",
        "        shape=(self.units,))\n",
        "    super(Tea, self).build(input_shape)\n",
        "\n",
        "# Bind the method to our class\n",
        "Tea.build = build\n",
        "\n",
        "def call(self, x):\n",
        "    with tf.get_default_graph().gradient_override_map(\n",
        "        {\"Round\":\"CustomRound\"}):\n",
        "        # Constrain input\n",
        "        x = tf.round(x)\n",
        "        # Constrain connections\n",
        "        connections = self.connections\n",
        "        connections = tf.round(connections)\n",
        "        connections = K.clip(connections, 0, 1)\n",
        "        # Multiply connections with weights\n",
        "        weighted_connections = connections * self.static_weights\n",
        "        # Dot input with weighted connections\n",
        "        output = K.dot(x, weighted_connections)\n",
        "        # Constrain biases\n",
        "        biases = tf.round(self.biases)\n",
        "        output = K.bias_add(\n",
        "            output,\n",
        "            biases,\n",
        "            data_format='channels_last'\n",
        "        )\n",
        "        # Apply activation / spike\n",
        "        output = K.in_train_phase(\n",
        "            K.sigmoid(output),\n",
        "            tf.cast(tf.greater_equal(output, 0.0), tf.float32)\n",
        "        )\n",
        "    return output\n",
        "    \n",
        "# Bind the method to our class\n",
        "Tea.call = call\n",
        "\n",
        "def compute_output_shape(self, input_shape):\n",
        "    assert input_shape and len(input_shape) >= 2\n",
        "    assert input_shape[-1]\n",
        "    output_shape = list(input_shape)\n",
        "    output_shape[-1] = self.units\n",
        "    return tuple(output_shape)\n",
        "    \n",
        "# Bind the method to our class\n",
        "Tea.compute_output_shape = compute_output_shape\n",
        "\n",
        "\n",
        "class AdditivePooling(Layer):\n",
        "    \"\"\"A helper layer designed to format data for output during TeaLearning.\n",
        "    If the data input to the layer has multiple spikes per classification, the\n",
        "    spikes for each tick are summed up. Then, all neurons that correspond to a\n",
        "    certain class are summed up so that the output is the number of spikes for\n",
        "    each class. Neurons are assumed to be arranged such that each\n",
        "    `num_classes` neurons represent a guess for each of the classes. For\n",
        "    example, if the guesses correspond to number from 0 to 9, the nuerons are\n",
        "    arranged as such:\n",
        "\n",
        "        neuron_num: 0  1  2  3  4  5  6  7  8  9  10 11 12  ...\n",
        "        guess:      0  1  2  3  4  5  6  7  8  9  0  1  2   ...\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_classes,\n",
        "                 **kwargs):\n",
        "        \"\"\"Initializes a new `AdditivePooling` layer.\n",
        "\n",
        "        Arguments:\n",
        "            num_classes -- The number of classes to output.\n",
        "        \"\"\"\n",
        "        self.num_classes = num_classes\n",
        "        self.num_inputs = None\n",
        "        super(AdditivePooling, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 2\n",
        "        # The number of neurons must be collapsable into the number of classes\n",
        "        assert input_shape[-1] % self.num_classes == 0\n",
        "        self.num_inputs = input_shape[-1]\n",
        "\n",
        "    def call(self, x):\n",
        "        # Sum up ticks if there are ticks\n",
        "        if len(x.shape) >= 3:\n",
        "            output = K.sum(x, axis=1)\n",
        "        else:\n",
        "            output = x\n",
        "        # Reshape output\n",
        "        output = tf.reshape(\n",
        "            output,\n",
        "            [-1, int(self.num_inputs // self.num_classes), self.num_classes]\n",
        "        )\n",
        "        # Sum up neurons\n",
        "        output = tf.reduce_sum(output, 1)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_shape = list(input_shape)\n",
        "        # Last dimension will be number of classes\n",
        "        output_shape[-1] = self.num_classes\n",
        "        # Ticks were summed, so delete tick dimension if exists\n",
        "        if len(output_shape) >= 3:\n",
        "            del output_shape[1]\n",
        "        return tuple(output_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoNCM_RwFCrE"
      },
      "outputs": [],
      "source": [
        "# Data Load\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# PyTorch (modeling)\n",
        "# import torch\n",
        "# from torch import nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data.sampler import SubsetRandomSampler\n",
        "# from torchvision import transforms\n",
        "# import torchvision.transforms.functional as TF\n",
        "# from torch.utils.data import Dataset\n",
        "# from torch.utils.data import random_split\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# # Visualization\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "from scipy import ndimage\n",
        "\n",
        "# These functions are introduced along the Part 1 notebook.\n",
        "\n",
        "# Position vectors. We load the data with respect to the file name, which is\n",
        "# a number corresponding to a specific in-bed position. We take advantage of this\n",
        "# and use the number to get the position with help of the following vectors.\n",
        "\n",
        "positions_i = [\"justAPlaceholder\", \"supine_1\", \"right_0\",\n",
        "               \"left_0\", \"right_30\", \"right_60\",\n",
        "               \"left_30\", \"left_60\", \"supine_2\",\n",
        "               \"supine_3\", \"supine_4\", \"supine_5\",\n",
        "               \"supine_6\", \"right_fetus\", \"left_fetus\",\n",
        "               \"supine_30\", \"supine_45\", \"supine_60\"]\n",
        "\n",
        "positions_i_short = [\"justAPlaceholder\", \"supine\", \"right\",\n",
        "               \"left\", \"right\", \"right\",\n",
        "               \"left\", \"left\", \"supine\",\n",
        "               \"supine\", \"supine\", \"supine\",\n",
        "               \"supine\", \"right\", \"left\",\n",
        "               \"supine\", \"supine\", \"supine\"]\n",
        "\n",
        "positions_ii = {\n",
        "    \"B\":\"supine\", \"1\":\"supine\", \"C\":\"right\",\n",
        "    \"D\":\"left\", \"E1\":\"right\", \"E2\":\"right\",\n",
        "    \"E3\":\"left\", \"E4\":\"left\", \"E5\":\"right\",\n",
        "    \"E6\":\"left\", \"F\":\"supine\", \"G1\":\"supine\",\n",
        "    \"G2\":\"right\", \"G3\":\"left\"\n",
        "}\n",
        "\n",
        "class_positions = ['supine', 'left', 'right', 'left_fetus', 'right_fetus']\n",
        "\n",
        "# We also want the classes to be encoded as numbers so we can work easier when\n",
        "# modeling. This function achieves so. Since left_fetus and right_fetus are not\n",
        "# considered as classes in the evaluation of the original paper and since they\n",
        "# are not considered in the \"Experiment I\", we encode them also as left and right\n",
        "# positions.\n",
        "\n",
        "def token_position_short(x):\n",
        "  return {\n",
        "      'supine': 0,\n",
        "      'left': 1,\n",
        "      'right': 2,\n",
        "      'left_fetus': 1,\n",
        "      'right_fetus': 2\n",
        "  }[x]\n",
        "\n",
        "def token_position(x):\n",
        "  return {\n",
        "      \"supine_1\":0, \n",
        "      \"right_0\":1,\n",
        "      \"left_0\":2, \n",
        "      \"right_30\":3, \n",
        "      \"right_60\":4,\n",
        "      \"left_30\":5, \n",
        "      \"left_60\":6, \n",
        "      \"supine_2\":7,\n",
        "      \"supine_3\":8, \n",
        "      \"supine_4\":9, \n",
        "      \"supine_5\":10,\n",
        "      \"supine_6\":11, \n",
        "      \"right_fetus\":12, \n",
        "      \"left_fetus\":13,\n",
        "      \"supine_30\":14, \n",
        "      \"supine_45\":15, \n",
        "      \"supine_60\":16\n",
        "  }[x]\n",
        "\n",
        "def token_position_new(x):\n",
        "  return {\n",
        "      \"supine_1\":0, \n",
        "      \"supine_2\":1,\n",
        "      \"supine_3\":2, \n",
        "      \"supine_4\":3, \n",
        "      \"supine_5\":4,\n",
        "      \"supine_6\":5, \n",
        "      \"supine_30\":6, \n",
        "      \"supine_45\":7, \n",
        "      \"supine_60\":8, \n",
        "      \"left_0\":9, \n",
        "      \"left_30\":10, \n",
        "      \"left_60\":11,\n",
        "      \"left_fetus\":12, \n",
        "      \"right_0\":13,\n",
        "      \"right_30\":14, \n",
        "      \"right_60\":15,\n",
        "      \"right_fetus\":16, \n",
        "  }[x]\n",
        "list_supine = [\"1.txt\",\"8.txt\",\"9.txt\",\"10.txt\",\"11.txt\",\"12.txt\",\"15.txt\",\"16.txt\",\"17.txt\"]\n",
        "\n",
        "list_supine_norm_1 = [\"1.txt\",\"8.txt\",\"9.txt\"]\n",
        "\n",
        "list_supine_norm_2 = [\"10.txt\",\"11.txt\",\"12.txt\"]\n",
        "\n",
        "list_supine_incl = [\"15.txt\",\"16.txt\",\"17.txt\"]\n",
        "\n",
        "list_left = [\"3.txt\",\"6.txt\",\"7.txt\",\"14.txt\"]\n",
        "\n",
        "list_right = [\"2.txt\",\"4.txt\",\"5.txt\",\"13.txt\"]\n",
        "\n",
        "def token_position_supine(x):\n",
        "  return {\n",
        "      \"supine_1\":0, \n",
        "      \"supine_2\":1,\n",
        "      \"supine_3\":2, \n",
        "      \"supine_4\":3, \n",
        "      \"supine_5\":4,\n",
        "      \"supine_6\":5, \n",
        "      \"supine_30\":6, \n",
        "      \"supine_45\":7, \n",
        "      \"supine_60\":8\n",
        "    }[x]\n",
        "\n",
        "def token_position_supine_norm_1(x):\n",
        "  return {\n",
        "      \"supine_1\":0, \n",
        "      \"supine_2\":1,\n",
        "      \"supine_3\":2, \n",
        "    }[x]\n",
        "\n",
        "def token_position_supine_norm_2(x):\n",
        "  return {\n",
        "      \"supine_4\":0, \n",
        "      \"supine_5\":1,\n",
        "      \"supine_6\":2, \n",
        "    }[x]\n",
        "\n",
        "def token_position_supine_incl(x):\n",
        "  return {\n",
        "      \"supine_30\":0, \n",
        "      \"supine_45\":1, \n",
        "      \"supine_60\":2\n",
        "    }[x]\n",
        "\n",
        "def token_position_left(x):\n",
        "  return {\n",
        "      \"left_0\":0, \n",
        "      \"left_30\":1, \n",
        "      \"left_60\":2,\n",
        "      \"left_fetus\":3,\n",
        "  }[x]\n",
        "\n",
        "def token_position_right(x):\n",
        "  return {\n",
        "      \"right_0\":0,\n",
        "      \"right_30\":1, \n",
        "      \"right_60\":2,\n",
        "      \"right_fetus\":3, \n",
        "  }[x]\n",
        "\n",
        "\n",
        "def load_exp_i(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      max_val = []\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(3, len(lines) - 3):\n",
        "                              \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                \n",
        "                # Spatio-temporal median filter 3x3x3\n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                past_image = ndimage.median_filter(past_image, 3)\n",
        "                future_image = ndimage.median_filter(future_image, 3)\n",
        "                raw_data = np.concatenate((raw_data[np.newaxis, :, :], past_image[np.newaxis, :, :], future_image[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "            \n",
        "            # with open(file_path, 'r') as f:\n",
        "            #   # Start from second recording, as the first two are corrupted\n",
        "            #   lines = f.read().splitlines()[2:]\n",
        "            #   for line in f.read().splitlines()[2:]:\n",
        "            #     # print(line)\n",
        "            #     raw_data = np.fromstring(line, dtype=float, sep='\\t')\n",
        "                # Change the range from [0-1000] to [0-255].\n",
        "                  # max_val.append(np.amax(raw_data))\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              # file_data = np.round(raw_data).astype(np.uint8)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "              # print(positions_i[int(file[:-4])])\n",
        "              file_label = token_position(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "      \n",
        "      # max_over_all = max(max_val)\n",
        "      # print(max_over_all)\n",
        "\n",
        "      # data = np.round(data * 255/1000).astype(np.uint8)\n",
        "      dataset[subject] = (data, labels)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_short(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      max_val = []\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(3, len(lines) - 3):\n",
        "                              \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                \n",
        "                # Spatio-temporal median filter 3x3x3\n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                past_image = ndimage.median_filter(past_image, 3)\n",
        "                future_image = ndimage.median_filter(future_image, 3)\n",
        "                raw_data = np.concatenate((raw_data[np.newaxis, :, :], past_image[np.newaxis, :, :], future_image[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              # file_data = np.round(raw_data).astype(np.uint8)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "              # print(positions_i[int(file[:-4])])\n",
        "              file_label = token_position_short(positions_i_short[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_supine(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_supine\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(5, len(lines) - 5):\n",
        "                            \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image_1 = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_1 = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                past_image_2 = np.fromstring(lines[i-2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_2 = np.fromstring(lines[i+2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "                # Spatio-temporal median filter 5x5x5\n",
        "              \n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                \n",
        "                past_image_1 = ndimage.median_filter(past_image_1, 3)\n",
        "                future_image_1 = ndimage.median_filter(future_image_1, 3)\n",
        "                past_image_2 = ndimage.median_filter(past_image_2, 3)\n",
        "                future_image_2 = ndimage.median_filter(future_image_2, 3)\n",
        "\n",
        "                raw_data = np.concatenate((past_image_2[np.newaxis, :, :],past_image_1[np.newaxis, :, :] ,raw_data[np.newaxis, :, :], \\\n",
        "                future_image_1[np.newaxis, :, :],future_image_2[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "              \n",
        "              # a=np.amax(raw_data)\n",
        "\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              \n",
        "              # file_data = np.round(raw_data).astype(np.uint8)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "\n",
        "              file_label = token_position_supine(positions_i[int(file[:-4])])\n",
        "              \n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_supine_norm_1(path):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_supine_norm_1\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            for line in f.read().splitlines()[2:]:\n",
        "              # print(line)\n",
        "              raw_data = np.fromstring(line, dtype=float, sep='\\t')\n",
        "              # Change the range from [0-1000] to [0-255].\n",
        "              max_val = np.amax(raw_data)\n",
        "              file_data = np.round(raw_data*255/max_val).astype(np.uint8)\n",
        "              \n",
        "              # file_data = np.round(raw_data).astype(float)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "\n",
        "              file_label = token_position_supine_norm_1(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_supine_norm_2(path):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_supine_norm_2\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            for line in f.read().splitlines()[2:]:\n",
        "              # print(line)\n",
        "              raw_data = np.fromstring(line, dtype=float, sep='\\t')\n",
        "              # Change the range from [0-1000] to [0-255].\n",
        "              max_val = np.amax(raw_data)\n",
        "              file_data = np.round(raw_data*255/max_val).astype(np.uint8)\n",
        "              \n",
        "              # file_data = np.round(raw_data).astype(float)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "\n",
        "              file_label = token_position_supine_norm_2(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_supine_incl(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_supine_incl\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            # with open(file_path, 'r') as f:\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(3, len(lines) - 3):\n",
        "\n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                \n",
        "                # Spatio-temporal median filter 3x3x3\n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                past_image = ndimage.median_filter(past_image, 3)\n",
        "                future_image = ndimage.median_filter(future_image, 3)\n",
        "                raw_data = np.concatenate((raw_data[np.newaxis, :, :], past_image[np.newaxis, :, :], future_image[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "\n",
        "              # Change the range from [0-1000] to [0-255].\n",
        "              # max_vol = np.amax(raw_data)\n",
        "              file_data = np.round(raw_data ).astype(np.uint8)\n",
        "\n",
        "              # file_data = np.round(raw_data).astype(np.uint8)\n",
        "              file_data = file_data.reshape(1, 64, 32)\n",
        "\n",
        "              file_label = token_position_supine_incl(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_left(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_left\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(5, len(lines) - 5):\n",
        "                            \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image_1 = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_1 = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                past_image_2 = np.fromstring(lines[i-2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_2 = np.fromstring(lines[i+2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "                # Spatio-temporal median filter 5x5x5\n",
        "              \n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                \n",
        "                past_image_1 = ndimage.median_filter(past_image_1, 3)\n",
        "                future_image_1 = ndimage.median_filter(future_image_1, 3)\n",
        "                past_image_2 = ndimage.median_filter(past_image_2, 3)\n",
        "                future_image_2 = ndimage.median_filter(future_image_2, 3)\n",
        "\n",
        "                raw_data = np.concatenate((past_image_2[np.newaxis, :, :],past_image_1[np.newaxis, :, :] ,raw_data[np.newaxis, :, :], \\\n",
        "                future_image_1[np.newaxis, :, :],future_image_2[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "          # with open(file_path, 'r') as f:\n",
        "          #   # Start from second recording, as the first two are corrupted\n",
        "          #   for line in f.read().splitlines()[2:]:\n",
        "          #     # print(line)\n",
        "          #     raw_data = np.fromstring(line, dtype=float, sep='\\t')\n",
        "          #     # Change the range from [0-1000] to [0-255].\n",
        "\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              \n",
        "\n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "\n",
        "              file_label = token_position_left(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_right(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_right\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(5, len(lines) - 5):\n",
        "                            \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image_1 = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_1 = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                past_image_2 = np.fromstring(lines[i-2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_2 = np.fromstring(lines[i+2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "                # Spatio-temporal median filter 5x5x5\n",
        "              \n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                \n",
        "                past_image_1 = ndimage.median_filter(past_image_1, 3)\n",
        "                future_image_1 = ndimage.median_filter(future_image_1, 3)\n",
        "                past_image_2 = ndimage.median_filter(past_image_2, 3)\n",
        "                future_image_2 = ndimage.median_filter(future_image_2, 3)\n",
        "\n",
        "                raw_data = np.concatenate((past_image_2[np.newaxis, :, :],past_image_1[np.newaxis, :, :] ,raw_data[np.newaxis, :, :], \\\n",
        "                future_image_1[np.newaxis, :, :],future_image_2[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "              # Change the range from [0-1000] to [0-255].\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "\n",
        "              file_label = token_position_right(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_new(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      max_val = []\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(3, len(lines) - 3):\n",
        "                              \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                \n",
        "                # Spatio-temporal median filter 3x3x3\n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                past_image = ndimage.median_filter(past_image, 3)\n",
        "                future_image = ndimage.median_filter(future_image, 3)\n",
        "                raw_data = np.concatenate((raw_data[np.newaxis, :, :], past_image[np.newaxis, :, :], future_image[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "            \n",
        "            # with open(file_path, 'r') as f:\n",
        "            #   # Start from second recording, as the first two are corrupted\n",
        "            #   lines = f.read().splitlines()[2:]\n",
        "            #   for line in f.read().splitlines()[2:]:\n",
        "            #     # print(line)\n",
        "            #     raw_data = np.fromstring(line, dtype=float, sep='\\t')\n",
        "                # Change the range from [0-1000] to [0-255].\n",
        "                  # max_val.append(np.amax(raw_data))\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              # file_data = np.round(raw_data).astype(np.uint8)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "              # print(positions_i[int(file[:-4])])\n",
        "              file_label = token_position_new(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "      \n",
        "      # max_over_all = max(max_val)\n",
        "      # print(max_over_all)\n",
        "\n",
        "      # data = np.round(data * 255/1000).astype(np.uint8)\n",
        "      dataset[subject] = (data, labels)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "def load_exp_ii(path):\n",
        "\n",
        "  exp_ii_data_air = {}\n",
        "  exp_ii_data_spo = {}\n",
        "\n",
        "  # each directory is a subject\n",
        "  for _, subject_dirs, _ in os.walk(path):\n",
        "    for subject in subject_dirs:\n",
        "      data = None\n",
        "      labels = None\n",
        "\n",
        "      # each directory is a matresss\n",
        "      for _, mat_dirs, _ in os.walk(os.path.join(path, subject)):\n",
        "        for mat in mat_dirs:\n",
        "          for _, _, files in os.walk(os.path.join(path, subject, mat)):\n",
        "            for file in files:\n",
        "              file_path = os.path.join(path, subject, mat, file)\n",
        "              raw_data = np.loadtxt(file_path)\n",
        "              # Change the range from [0-500] to [0-255].\n",
        "              file_data = np.round(raw_data*255/500).astype(np.uint8)\n",
        "              \n",
        "              file_data = resize_and_rotate(file_data)\n",
        "              \n",
        "              file_data = file_data.view(1, 64, 32)\n",
        "\n",
        "              if file[-6] == \"E\" or file[-6] == \"G\":\n",
        "                file_label = positions_ii[file[-6:-4]]\n",
        "              else:\n",
        "                file_label = positions_ii[file[-6]]\n",
        "\n",
        "              file_label = token_position(file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "          if mat == \"Air_Mat\":\n",
        "            exp_ii_data_air[subject] = (data, labels)\n",
        "          else:\n",
        "            exp_ii_data_spo[subject] = (data, labels)\n",
        "\n",
        "          data = None\n",
        "          labels = None\n",
        "\n",
        "    return exp_ii_data_air, exp_ii_data_spo\n",
        "\n",
        "import cv2 \n",
        "\n",
        "class Mat_Dataset():\n",
        "  def __init__(self,datasets, mats, Subject_IDs):\n",
        "\n",
        "    self.samples = []\n",
        "    self.labels = []\n",
        "\n",
        "    for mat in mats:\n",
        "      data = datasets[mat]\n",
        "      self.samples.append(np.vstack([data.get(key)[0] for key in Subject_IDs]))\n",
        "      self.labels.append(np.hstack([data.get(key)[1] for key in Subject_IDs]))\n",
        "\n",
        "    self.samples = np.vstack(self.samples)\n",
        "    self.labels = np.hstack(self.labels)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.samples.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.samples[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIfuZeUpDzcO"
      },
      "outputs": [],
      "source": [
        "@tf.RegisterGradient(\"CustomRound\")\n",
        "def _const_round_grad(unused_op, grad):\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enw2Y-4s6PmX"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XkaSlBG4PJv"
      },
      "outputs": [],
      "source": [
        "exp_i_data = load_exp_i_supine(\"/content/drive/MyDrive/RANC/dataset/experiment-i\")\n",
        "\n",
        "datasets = {\"Base\":exp_i_data}\n",
        "subjects = [\"S1\",\"S2\",\"S3\",\"S4\",\"S5\",\"S6\",\"S7\",\"S8\",\"S9\",\"S10\",\"S11\",\"S12\",\"S13\"]\n",
        "\n",
        "train_data = Mat_Dataset(datasets,[\"Base\"],subjects)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0VklBMSLN1I",
        "outputId": "b86f01ec-b12c-4fc3-cdcd-371e2d00e447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "y_train = to_categorical(train_data.labels, 9)\n",
        "print(y_train[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRzhqcRdzKIM"
      },
      "outputs": [],
      "source": [
        " x_train = []\n",
        "\n",
        "for i in range(len(train_data.samples)):\n",
        "    \n",
        "    train_data.samples[i] = cv2.equalizeHist(train_data.samples[i])\n",
        "\n",
        "    heat = cv2.applyColorMap(train_data.samples[i], cv2.COLORMAP_JET)\n",
        "    mask = np.ones_like(heat)\n",
        "    bin1 = np.array(heat>=mask*63).astype(np.uint8)\n",
        "    bin2 = np.array(heat>=mask*127).astype(np.uint8)\n",
        "    bin3 = np.array(heat>=mask*190).astype(np.uint8)\n",
        "    bin_out = np.concatenate((bin1,bin2,bin3),axis=2)\n",
        "\n",
        "    x_train.append(bin_out)\n",
        "\n",
        "x_train = np.array(x_train).astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hp7mi2791Caz"
      },
      "outputs": [],
      "source": [
        "random.seed(1)\n",
        "(x_train,y_train) = shuffle(x_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brihHAWCwxE1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwDjZceGLY_J",
        "outputId": "b438fa84-7a2d-4fab-cba2-6c8a18ee0f62"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 7367 samples, validate on 1842 samples\n",
            "Epoch 1/30\n",
            "7367/7367 [==============================] - ETA: 0s - loss: 1.9077 - acc: 0.1990"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7367/7367 [==============================] - 18s 2ms/sample - loss: 1.9077 - acc: 0.1990 - val_loss: 1.6947 - val_acc: 0.2655\n",
            "Epoch 2/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0714 - acc: 0.5352 - val_loss: 0.8462 - val_acc: 0.6412\n",
            "Epoch 3/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.2845 - acc: 0.9065 - val_loss: 0.2567 - val_acc: 0.9191\n",
            "Epoch 4/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0705 - acc: 0.9794 - val_loss: 0.0717 - val_acc: 0.9777\n",
            "Epoch 5/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0353 - acc: 0.9909 - val_loss: 0.1150 - val_acc: 0.9501\n",
            "Epoch 6/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0184 - acc: 0.9958 - val_loss: 0.1100 - val_acc: 0.9582\n",
            "Epoch 7/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0191 - acc: 0.9948 - val_loss: 0.1725 - val_acc: 0.9381\n",
            "Epoch 8/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0056 - acc: 0.9989 - val_loss: 0.0670 - val_acc: 0.9777\n",
            "Epoch 9/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.1474 - acc: 0.9526 - val_loss: 0.1704 - val_acc: 0.9311\n",
            "Epoch 10/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0063 - acc: 0.9984 - val_loss: 0.0512 - val_acc: 0.9832\n",
            "Epoch 11/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0039 - acc: 0.9992 - val_loss: 0.0217 - val_acc: 0.9886\n",
            "Epoch 12/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0014 - acc: 0.9999 - val_loss: 0.0234 - val_acc: 0.9902\n",
            "Epoch 13/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 6.8825e-04 - acc: 1.0000 - val_loss: 0.0335 - val_acc: 0.9864\n",
            "Epoch 14/30\n",
            "7367/7367 [==============================] - 8s 1ms/sample - loss: 8.5702e-04 - acc: 0.9999 - val_loss: 0.0177 - val_acc: 0.9919\n",
            "Epoch 15/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 4.3495e-04 - acc: 1.0000 - val_loss: 0.0276 - val_acc: 0.9902\n",
            "Epoch 16/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 4.5558e-04 - acc: 1.0000 - val_loss: 0.0127 - val_acc: 0.9957\n",
            "Epoch 17/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 3.1141e-04 - acc: 1.0000 - val_loss: 0.0229 - val_acc: 0.9924\n",
            "Epoch 18/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0065 - acc: 0.9978 - val_loss: 0.7315 - val_acc: 0.8274\n",
            "Epoch 19/30\n",
            "7367/7367 [==============================] - 8s 1ms/sample - loss: 0.0572 - acc: 0.9830 - val_loss: 0.0234 - val_acc: 0.9891\n",
            "Epoch 20/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0058 - acc: 0.9986 - val_loss: 0.0251 - val_acc: 0.9913\n",
            "Epoch 21/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0016 - acc: 0.9999 - val_loss: 0.0186 - val_acc: 0.9913\n",
            "Epoch 22/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 3.0663e-04 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 0.9984\n",
            "Epoch 23/30\n",
            "7367/7367 [==============================] - 8s 1ms/sample - loss: 2.1790e-04 - acc: 1.0000 - val_loss: 0.0098 - val_acc: 0.9957\n",
            "Epoch 24/30\n",
            "7367/7367 [==============================] - 8s 1ms/sample - loss: 1.6889e-04 - acc: 1.0000 - val_loss: 0.0078 - val_acc: 0.9973\n",
            "Epoch 25/30\n",
            "7367/7367 [==============================] - 8s 1ms/sample - loss: 2.0637e-04 - acc: 1.0000 - val_loss: 0.0068 - val_acc: 0.9978\n",
            "Epoch 26/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0017 - acc: 0.9993 - val_loss: 0.0337 - val_acc: 0.9859\n",
            "Epoch 27/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0889 - acc: 0.9723 - val_loss: 0.0453 - val_acc: 0.9815\n",
            "Epoch 28/30\n",
            "7367/7367 [==============================] - 8s 1ms/sample - loss: 0.0053 - acc: 0.9986 - val_loss: 0.0403 - val_acc: 0.9826\n",
            "Epoch 29/30\n",
            "7367/7367 [==============================] - 8s 1ms/sample - loss: 0.0015 - acc: 0.9999 - val_loss: 0.0095 - val_acc: 0.9957\n",
            "Epoch 30/30\n",
            "7367/7367 [==============================] - 8s 1ms/sample - loss: 2.8224e-04 - acc: 1.0000 - val_loss: 0.0034 - val_acc: 0.9984\n",
            "Test loss: 0.0051498144671200076\n",
            "Test accuracy: 99.78284239768982 %\n",
            "Train on 7367 samples, validate on 1842 samples\n",
            "Epoch 1/30\n",
            "7367/7367 [==============================] - 15s 2ms/sample - loss: 0.9628 - acc: 0.6592 - val_loss: 0.3442 - val_acc: 0.8654\n",
            "Epoch 2/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.1634 - acc: 0.9488 - val_loss: 0.1652 - val_acc: 0.9506\n",
            "Epoch 3/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0838 - acc: 0.9757 - val_loss: 0.1643 - val_acc: 0.9468\n",
            "Epoch 4/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0427 - acc: 0.9919 - val_loss: 0.0605 - val_acc: 0.9821\n",
            "Epoch 5/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0297 - acc: 0.9929 - val_loss: 0.1210 - val_acc: 0.9631\n",
            "Epoch 6/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0291 - acc: 0.9929 - val_loss: 0.0811 - val_acc: 0.9761\n",
            "Epoch 7/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0267 - acc: 0.9933 - val_loss: 0.0242 - val_acc: 0.9951\n",
            "Epoch 8/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0055 - acc: 0.9996 - val_loss: 0.0322 - val_acc: 0.9902\n",
            "Epoch 9/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0059 - acc: 0.9993 - val_loss: 0.2374 - val_acc: 0.9191\n",
            "Epoch 10/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0814 - acc: 0.9758 - val_loss: 0.0191 - val_acc: 0.9946\n",
            "Epoch 11/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0120 - acc: 0.9976 - val_loss: 0.0325 - val_acc: 0.9897\n",
            "Epoch 12/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0064 - acc: 0.9990 - val_loss: 0.0089 - val_acc: 0.9984\n",
            "Epoch 13/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0050 - val_acc: 0.9995\n",
            "Epoch 14/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0086 - val_acc: 0.9984\n",
            "Epoch 15/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 8.1827e-04 - acc: 1.0000 - val_loss: 0.0063 - val_acc: 0.9989\n",
            "Epoch 16/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 8.2342e-04 - acc: 1.0000 - val_loss: 0.0204 - val_acc: 0.9935\n",
            "Epoch 17/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 6.2014e-04 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 0.9984\n",
            "Epoch 18/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 6.7077e-04 - acc: 1.0000 - val_loss: 0.0056 - val_acc: 0.9984\n",
            "Epoch 19/30\n",
            "7367/7367 [==============================] - 8s 1ms/sample - loss: 0.0109 - acc: 0.9974 - val_loss: 0.2285 - val_acc: 0.9435\n",
            "Epoch 20/30\n",
            "7367/7367 [==============================] - 8s 1ms/sample - loss: 0.0459 - acc: 0.9855 - val_loss: 0.0177 - val_acc: 0.9951\n",
            "Epoch 21/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0044 - acc: 0.9992 - val_loss: 0.0154 - val_acc: 0.9962\n",
            "Epoch 22/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0065 - acc: 0.9984 - val_loss: 0.0073 - val_acc: 0.9989\n",
            "Epoch 23/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 7.3358e-04 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 0.9989\n",
            "Epoch 24/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 4.7528e-04 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 0.9995\n",
            "Epoch 25/30\n",
            "7367/7367 [==============================] - 8s 1ms/sample - loss: 4.1036e-04 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 0.9995\n",
            "Epoch 26/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 3.7152e-04 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 0.9995\n",
            "Epoch 27/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 3.6368e-04 - acc: 1.0000 - val_loss: 0.0034 - val_acc: 0.9995\n",
            "Epoch 28/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 3.4314e-04 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 0.9995\n",
            "Epoch 29/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 3.0244e-04 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 0.9995\n",
            "Epoch 30/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 2.4567e-04 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 0.9995\n",
            "Test loss: 0.006373972505431908\n",
            "Test accuracy: 99.67426657676697 %\n",
            "Train on 7367 samples, validate on 1842 samples\n",
            "Epoch 1/30\n",
            "7367/7367 [==============================] - 16s 2ms/sample - loss: 1.4552 - acc: 0.4736 - val_loss: 0.9341 - val_acc: 0.6906\n",
            "Epoch 2/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.6695 - acc: 0.7604 - val_loss: 0.6592 - val_acc: 0.7503\n",
            "Epoch 3/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.5966 - acc: 0.7667 - val_loss: 0.5218 - val_acc: 0.7725\n",
            "Epoch 4/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.4995 - acc: 0.7815 - val_loss: 0.6074 - val_acc: 0.7530\n",
            "Epoch 5/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.5944 - acc: 0.7626 - val_loss: 0.5174 - val_acc: 0.7752\n",
            "Epoch 6/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.5031 - acc: 0.7827 - val_loss: 0.7476 - val_acc: 0.7058\n",
            "Epoch 7/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.5312 - acc: 0.7749 - val_loss: 0.5169 - val_acc: 0.7758\n",
            "Epoch 8/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.4813 - acc: 0.7853 - val_loss: 0.4807 - val_acc: 0.7823\n",
            "Epoch 9/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.4697 - acc: 0.7872 - val_loss: 0.4818 - val_acc: 0.7828\n",
            "Epoch 10/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.4697 - acc: 0.7870 - val_loss: 0.4798 - val_acc: 0.7828\n",
            "Epoch 11/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.4691 - acc: 0.7872 - val_loss: 0.4790 - val_acc: 0.7823\n",
            "Epoch 12/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.4703 - acc: 0.7870 - val_loss: 0.8775 - val_acc: 0.6998\n",
            "Epoch 13/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.5518 - acc: 0.7734 - val_loss: 0.4899 - val_acc: 0.7790\n",
            "Epoch 14/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.4741 - acc: 0.7872 - val_loss: 0.4816 - val_acc: 0.7828\n",
            "Epoch 15/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.4519 - acc: 0.8230 - val_loss: 0.4081 - val_acc: 0.8985\n",
            "Epoch 16/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.4003 - acc: 0.8932 - val_loss: 0.4008 - val_acc: 0.8985\n",
            "Epoch 17/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3999 - acc: 0.8932 - val_loss: 0.4004 - val_acc: 0.8990\n",
            "Epoch 18/30\n",
            "7367/7367 [==============================] - 8s 1ms/sample - loss: 0.3997 - acc: 0.8932 - val_loss: 0.4023 - val_acc: 0.8974\n",
            "Epoch 19/30\n",
            "7367/7367 [==============================] - 8s 1ms/sample - loss: 0.3996 - acc: 0.8932 - val_loss: 0.4001 - val_acc: 0.8990\n",
            "Epoch 20/30\n",
            "7367/7367 [==============================] - 8s 1ms/sample - loss: 0.3996 - acc: 0.8932 - val_loss: 0.4000 - val_acc: 0.8990\n",
            "Epoch 21/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3996 - acc: 0.8932 - val_loss: 0.4000 - val_acc: 0.8990\n",
            "Epoch 22/30\n",
            "7367/7367 [==============================] - 8s 1ms/sample - loss: 0.3996 - acc: 0.8932 - val_loss: 0.3999 - val_acc: 0.8990\n",
            "Epoch 23/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3995 - acc: 0.8932 - val_loss: 0.4006 - val_acc: 0.8990\n",
            "Epoch 24/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3996 - acc: 0.8932 - val_loss: 0.4020 - val_acc: 0.8974\n",
            "Epoch 25/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3995 - acc: 0.8932 - val_loss: 0.4000 - val_acc: 0.8990\n",
            "Epoch 26/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3995 - acc: 0.8932 - val_loss: 0.4019 - val_acc: 0.8990\n",
            "Epoch 27/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3995 - acc: 0.8932 - val_loss: 0.4019 - val_acc: 0.8985\n",
            "Epoch 28/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3995 - acc: 0.8932 - val_loss: 0.4018 - val_acc: 0.8990\n",
            "Epoch 29/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3994 - acc: 0.8932 - val_loss: 0.4008 - val_acc: 0.8985\n",
            "Epoch 30/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3994 - acc: 0.8932 - val_loss: 0.4007 - val_acc: 0.8990\n",
            "Test loss: 0.40355029074428656\n",
            "Test accuracy: 88.92508149147034 %\n",
            "Train on 7367 samples, validate on 1842 samples\n",
            "Epoch 1/30\n",
            "7367/7367 [==============================] - 17s 2ms/sample - loss: 2.0231 - acc: 0.2384 - val_loss: 1.7596 - val_acc: 0.3018\n",
            "Epoch 2/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0563 - acc: 0.6497 - val_loss: 0.7665 - val_acc: 0.7872\n",
            "Epoch 3/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.5066 - acc: 0.8617 - val_loss: 0.4529 - val_acc: 0.8561\n",
            "Epoch 4/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3972 - acc: 0.8762 - val_loss: 0.4731 - val_acc: 0.8556\n",
            "Epoch 5/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3611 - acc: 0.8864 - val_loss: 0.3972 - val_acc: 0.8659\n",
            "Epoch 6/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3466 - acc: 0.8872 - val_loss: 0.3718 - val_acc: 0.8730\n",
            "Epoch 7/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3356 - acc: 0.8905 - val_loss: 0.3483 - val_acc: 0.8817\n",
            "Epoch 8/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3359 - acc: 0.8905 - val_loss: 0.3730 - val_acc: 0.8697\n",
            "Epoch 9/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3438 - acc: 0.8867 - val_loss: 0.3672 - val_acc: 0.8730\n",
            "Epoch 10/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3745 - acc: 0.8767 - val_loss: 0.3526 - val_acc: 0.8795\n",
            "Epoch 11/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3231 - acc: 0.8936 - val_loss: 0.3467 - val_acc: 0.8817\n",
            "Epoch 12/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3245 - acc: 0.8926 - val_loss: 0.3544 - val_acc: 0.8784\n",
            "Epoch 13/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3363 - acc: 0.8881 - val_loss: 0.3451 - val_acc: 0.8827\n",
            "Epoch 14/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3219 - acc: 0.8937 - val_loss: 0.3504 - val_acc: 0.8795\n",
            "Epoch 15/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3208 - acc: 0.8937 - val_loss: 0.3455 - val_acc: 0.8817\n",
            "Epoch 16/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3174 - acc: 0.8937 - val_loss: 0.3452 - val_acc: 0.8806\n",
            "Epoch 17/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3157 - acc: 0.8937 - val_loss: 0.3411 - val_acc: 0.8806\n",
            "Epoch 18/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3156 - acc: 0.8937 - val_loss: 0.3412 - val_acc: 0.8806\n",
            "Epoch 19/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3155 - acc: 0.8937 - val_loss: 0.3382 - val_acc: 0.8822\n",
            "Epoch 20/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3151 - acc: 0.8937 - val_loss: 0.3398 - val_acc: 0.8817\n",
            "Epoch 21/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3697 - acc: 0.8791 - val_loss: 0.3529 - val_acc: 0.8779\n",
            "Epoch 22/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3260 - acc: 0.8911 - val_loss: 0.3521 - val_acc: 0.8773\n",
            "Epoch 23/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3174 - acc: 0.8932 - val_loss: 0.3334 - val_acc: 0.8827\n",
            "Epoch 24/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3151 - acc: 0.8937 - val_loss: 0.3306 - val_acc: 0.8838\n",
            "Epoch 25/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3180 - acc: 0.8928 - val_loss: 0.3394 - val_acc: 0.8817\n",
            "Epoch 26/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3159 - acc: 0.8936 - val_loss: 0.3361 - val_acc: 0.8811\n",
            "Epoch 27/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3152 - acc: 0.8937 - val_loss: 0.3303 - val_acc: 0.8838\n",
            "Epoch 28/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3148 - acc: 0.8937 - val_loss: 0.3318 - val_acc: 0.8827\n",
            "Epoch 29/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3147 - acc: 0.8937 - val_loss: 0.3311 - val_acc: 0.8833\n",
            "Epoch 30/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3147 - acc: 0.8937 - val_loss: 0.3301 - val_acc: 0.8838\n",
            "Test loss: 0.3152379495830929\n",
            "Test accuracy: 90.01085758209229 %\n",
            "Train on 7367 samples, validate on 1842 samples\n",
            "Epoch 1/30\n",
            "7367/7367 [==============================] - 18s 2ms/sample - loss: 1.7960 - acc: 0.3057 - val_loss: 1.3494 - val_acc: 0.4642\n",
            "Epoch 2/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.1837 - acc: 0.5280 - val_loss: 1.3467 - val_acc: 0.4674\n",
            "Epoch 3/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.1017 - acc: 0.5364 - val_loss: 1.1126 - val_acc: 0.5358\n",
            "Epoch 4/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0478 - acc: 0.5502 - val_loss: 1.0851 - val_acc: 0.5375\n",
            "Epoch 5/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.1704 - acc: 0.5094 - val_loss: 1.2945 - val_acc: 0.4359\n",
            "Epoch 6/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.1614 - acc: 0.5158 - val_loss: 1.0672 - val_acc: 0.5407\n",
            "Epoch 7/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0295 - acc: 0.5523 - val_loss: 1.0527 - val_acc: 0.5423\n",
            "Epoch 8/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0234 - acc: 0.5527 - val_loss: 1.0532 - val_acc: 0.5423\n",
            "Epoch 9/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0228 - acc: 0.5527 - val_loss: 1.0523 - val_acc: 0.5429\n",
            "Epoch 10/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0228 - acc: 0.5527 - val_loss: 1.0585 - val_acc: 0.5402\n",
            "Epoch 11/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0226 - acc: 0.5527 - val_loss: 1.0531 - val_acc: 0.5418\n",
            "Epoch 12/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0224 - acc: 0.5527 - val_loss: 1.0518 - val_acc: 0.5423\n",
            "Epoch 13/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0223 - acc: 0.5527 - val_loss: 1.0549 - val_acc: 0.5413\n",
            "Epoch 14/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0516 - acc: 0.5466 - val_loss: 1.1008 - val_acc: 0.5326\n",
            "Epoch 15/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0375 - acc: 0.5493 - val_loss: 1.0808 - val_acc: 0.5337\n",
            "Epoch 16/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0228 - acc: 0.5527 - val_loss: 1.0536 - val_acc: 0.5429\n",
            "Epoch 17/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0421 - acc: 0.5478 - val_loss: 1.0550 - val_acc: 0.5418\n",
            "Epoch 18/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0304 - acc: 0.5512 - val_loss: 1.0503 - val_acc: 0.5429\n",
            "Epoch 19/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0213 - acc: 0.5529 - val_loss: 1.0484 - val_acc: 0.5429\n",
            "Epoch 20/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0209 - acc: 0.5527 - val_loss: 1.0489 - val_acc: 0.5429\n",
            "Epoch 21/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0629 - acc: 0.5413 - val_loss: 1.5140 - val_acc: 0.4305\n",
            "Epoch 22/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0452 - acc: 0.5481 - val_loss: 1.0503 - val_acc: 0.5423\n",
            "Epoch 23/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0212 - acc: 0.5527 - val_loss: 1.0500 - val_acc: 0.5423\n",
            "Epoch 24/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0210 - acc: 0.5527 - val_loss: 1.0487 - val_acc: 0.5429\n",
            "Epoch 25/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0208 - acc: 0.5529 - val_loss: 1.0490 - val_acc: 0.5429\n",
            "Epoch 26/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0207 - acc: 0.5530 - val_loss: 1.0486 - val_acc: 0.5423\n",
            "Epoch 27/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0205 - acc: 0.5529 - val_loss: 1.0484 - val_acc: 0.5429\n",
            "Epoch 28/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0217 - acc: 0.5526 - val_loss: 1.0531 - val_acc: 0.5413\n",
            "Epoch 29/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0208 - acc: 0.5529 - val_loss: 1.0484 - val_acc: 0.5429\n",
            "Epoch 30/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0205 - acc: 0.5529 - val_loss: 1.0488 - val_acc: 0.5429\n",
            "Test loss: 1.0184954059240483\n",
            "Test accuracy: 55.591750144958496 %\n",
            "Train on 7367 samples, validate on 1842 samples\n",
            "Epoch 1/30\n",
            "7367/7367 [==============================] - 18s 2ms/sample - loss: 1.2850 - acc: 0.5811 - val_loss: 0.5756 - val_acc: 0.8398\n",
            "Epoch 2/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.3408 - acc: 0.9317 - val_loss: 0.4430 - val_acc: 0.8773\n",
            "Epoch 3/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.1632 - acc: 0.9852 - val_loss: 0.1640 - val_acc: 0.9772\n",
            "Epoch 4/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.1475 - acc: 0.9889 - val_loss: 0.1461 - val_acc: 0.9837\n",
            "Epoch 5/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.1579 - acc: 0.9832 - val_loss: 0.1490 - val_acc: 0.9821\n",
            "Epoch 6/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.1102 - acc: 0.9985 - val_loss: 0.1088 - val_acc: 0.9962\n",
            "Epoch 7/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.1059 - acc: 0.9990 - val_loss: 0.1089 - val_acc: 0.9967\n",
            "Epoch 8/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.1030 - acc: 0.9995 - val_loss: 0.2247 - val_acc: 0.9609\n",
            "Epoch 9/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.1395 - acc: 0.9876 - val_loss: 0.2227 - val_acc: 0.9636\n",
            "Epoch 10/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.1219 - acc: 0.9927 - val_loss: 0.1097 - val_acc: 0.9978\n",
            "Epoch 11/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.1170 - acc: 0.9942 - val_loss: 0.1142 - val_acc: 0.9957\n",
            "Epoch 12/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.1353 - acc: 0.9885 - val_loss: 0.1079 - val_acc: 0.9973\n",
            "Epoch 13/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.1046 - acc: 0.9988 - val_loss: 0.1016 - val_acc: 0.9995\n",
            "Epoch 14/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.1022 - acc: 0.9992 - val_loss: 0.1048 - val_acc: 0.9984\n",
            "Epoch 15/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.1063 - acc: 0.9977 - val_loss: 0.1175 - val_acc: 0.9940\n",
            "Epoch 16/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.1182 - acc: 0.9935 - val_loss: 0.0921 - val_acc: 0.9995\n",
            "Epoch 17/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0905 - acc: 0.9999 - val_loss: 0.0932 - val_acc: 0.9984\n",
            "Epoch 18/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0896 - acc: 0.9997 - val_loss: 0.0901 - val_acc: 1.0000\n",
            "Epoch 19/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0895 - acc: 0.9997 - val_loss: 0.0898 - val_acc: 1.0000\n",
            "Epoch 20/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0892 - acc: 0.9999 - val_loss: 0.0932 - val_acc: 0.9995\n",
            "Epoch 21/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0887 - acc: 1.0000 - val_loss: 0.0913 - val_acc: 0.9995\n",
            "Epoch 22/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0886 - acc: 1.0000 - val_loss: 0.0896 - val_acc: 1.0000\n",
            "Epoch 23/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0885 - acc: 1.0000 - val_loss: 0.0898 - val_acc: 1.0000\n",
            "Epoch 24/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0885 - acc: 1.0000 - val_loss: 0.0894 - val_acc: 1.0000\n",
            "Epoch 25/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0885 - acc: 1.0000 - val_loss: 0.0894 - val_acc: 1.0000\n",
            "Epoch 26/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0885 - acc: 1.0000 - val_loss: 0.0894 - val_acc: 1.0000\n",
            "Epoch 27/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0884 - acc: 1.0000 - val_loss: 0.0893 - val_acc: 1.0000\n",
            "Epoch 28/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0884 - acc: 1.0000 - val_loss: 0.0893 - val_acc: 1.0000\n",
            "Epoch 29/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0883 - acc: 1.0000 - val_loss: 0.0893 - val_acc: 1.0000\n",
            "Epoch 30/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0883 - acc: 1.0000 - val_loss: 0.0894 - val_acc: 1.0000\n",
            "Test loss: 0.10776251044031333\n",
            "Test accuracy: 100.0 %\n",
            "Train on 7367 samples, validate on 1842 samples\n",
            "Epoch 1/30\n",
            "7367/7367 [==============================] - 19s 3ms/sample - loss: 1.0878 - acc: 0.6129 - val_loss: 0.4498 - val_acc: 0.8426\n",
            "Epoch 2/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.2106 - acc: 0.9351 - val_loss: 0.1009 - val_acc: 0.9772\n",
            "Epoch 3/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0645 - acc: 0.9825 - val_loss: 0.0850 - val_acc: 0.9767\n",
            "Epoch 4/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0238 - acc: 0.9944 - val_loss: 0.0945 - val_acc: 0.9739\n",
            "Epoch 5/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0379 - acc: 0.9894 - val_loss: 0.0255 - val_acc: 0.9919\n",
            "Epoch 6/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0256 - acc: 0.9925 - val_loss: 0.0458 - val_acc: 0.9826\n",
            "Epoch 7/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0190 - acc: 0.9946 - val_loss: 0.0450 - val_acc: 0.9864\n",
            "Epoch 8/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0652 - acc: 0.9806 - val_loss: 0.1301 - val_acc: 0.9484\n",
            "Epoch 9/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0158 - acc: 0.9966 - val_loss: 0.0224 - val_acc: 0.9951\n",
            "Epoch 10/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0032 - acc: 0.9995 - val_loss: 0.0025 - val_acc: 0.9995\n",
            "Epoch 11/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 8.9479e-04 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 0.9984\n",
            "Epoch 12/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 5.1616e-04 - acc: 1.0000 - val_loss: 0.0035 - val_acc: 0.9984\n",
            "Epoch 13/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 6.9249e-04 - acc: 1.0000 - val_loss: 6.2188e-04 - val_acc: 1.0000\n",
            "Epoch 14/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 4.3624e-04 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 0.9989\n",
            "Epoch 15/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 6.1479e-04 - acc: 0.9999 - val_loss: 9.4008e-04 - val_acc: 0.9995\n",
            "Epoch 16/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 2.4466e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9995\n",
            "Epoch 17/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 2.8134e-04 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 0.9973\n",
            "Epoch 18/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0825 - acc: 0.9726 - val_loss: 0.0240 - val_acc: 0.9940\n",
            "Epoch 19/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0121 - acc: 0.9969 - val_loss: 0.0027 - val_acc: 1.0000\n",
            "Epoch 20/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
            "Epoch 21/30\n",
            "7367/7367 [==============================] - 10s 1ms/sample - loss: 5.2701e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 0.9995\n",
            "Epoch 22/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 4.2516e-04 - acc: 1.0000 - val_loss: 8.7986e-04 - val_acc: 1.0000\n",
            "Epoch 23/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 3.4698e-04 - acc: 1.0000 - val_loss: 0.0011 - val_acc: 1.0000\n",
            "Epoch 24/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 4.2111e-04 - acc: 1.0000 - val_loss: 0.0130 - val_acc: 0.9962\n",
            "Epoch 25/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0087 - acc: 0.9978 - val_loss: 0.4662 - val_acc: 0.8865\n",
            "Epoch 26/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0882 - acc: 0.9749 - val_loss: 0.0054 - val_acc: 0.9989\n",
            "Epoch 27/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9995\n",
            "Epoch 28/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 7.8658e-04 - acc: 1.0000 - val_loss: 9.7180e-04 - val_acc: 1.0000\n",
            "Epoch 29/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 4.7741e-04 - acc: 1.0000 - val_loss: 0.0013 - val_acc: 0.9995\n",
            "Epoch 30/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 3.6930e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9995\n",
            "Test loss: 0.0008423418669881002\n",
            "Test accuracy: 100.0 %\n",
            "Train on 7367 samples, validate on 1842 samples\n",
            "Epoch 1/30\n",
            "7367/7367 [==============================] - 21s 3ms/sample - loss: 0.8037 - acc: 0.7090 - val_loss: 0.6477 - val_acc: 0.7698\n",
            "Epoch 2/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.1092 - acc: 0.9678 - val_loss: 0.1711 - val_acc: 0.9463\n",
            "Epoch 3/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0583 - acc: 0.9836 - val_loss: 0.0850 - val_acc: 0.9729\n",
            "Epoch 4/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0530 - acc: 0.9833 - val_loss: 0.4902 - val_acc: 0.8328\n",
            "Epoch 5/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0423 - acc: 0.9870 - val_loss: 0.2326 - val_acc: 0.9397\n",
            "Epoch 6/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0300 - acc: 0.9919 - val_loss: 0.0313 - val_acc: 0.9891\n",
            "Epoch 7/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0017 - acc: 0.9999 - val_loss: 0.0322 - val_acc: 0.9913\n",
            "Epoch 8/30\n",
            "7367/7367 [==============================] - 10s 1ms/sample - loss: 9.5225e-04 - acc: 1.0000 - val_loss: 0.0247 - val_acc: 0.9924\n",
            "Epoch 9/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 4.0563e-04 - acc: 1.0000 - val_loss: 0.0244 - val_acc: 0.9935\n",
            "Epoch 10/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 3.7842e-04 - acc: 1.0000 - val_loss: 0.0303 - val_acc: 0.9919\n",
            "Epoch 11/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 2.5898e-04 - acc: 1.0000 - val_loss: 0.0125 - val_acc: 0.9957\n",
            "Epoch 12/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.8847e-04 - acc: 1.0000 - val_loss: 0.0233 - val_acc: 0.9940\n",
            "Epoch 13/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 2.1764e-04 - acc: 1.0000 - val_loss: 0.0175 - val_acc: 0.9957\n",
            "Epoch 14/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.5062e-04 - acc: 1.0000 - val_loss: 0.0154 - val_acc: 0.9951\n",
            "Epoch 15/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 2.6941e-04 - acc: 1.0000 - val_loss: 0.0136 - val_acc: 0.9951\n",
            "Epoch 16/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 2.7643e-04 - acc: 1.0000 - val_loss: 0.0206 - val_acc: 0.9951\n",
            "Epoch 17/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 2.8051e-04 - acc: 1.0000 - val_loss: 0.0081 - val_acc: 0.9973\n",
            "Epoch 18/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.2712 - acc: 0.9183 - val_loss: 0.0871 - val_acc: 0.9777\n",
            "Epoch 19/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0175 - acc: 0.9966 - val_loss: 0.2376 - val_acc: 0.9251\n",
            "Epoch 20/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0060 - acc: 0.9988 - val_loss: 0.0735 - val_acc: 0.9777\n",
            "Epoch 21/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0312 - acc: 0.9908 - val_loss: 0.0090 - val_acc: 0.9984\n",
            "Epoch 22/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0022 - acc: 0.9996 - val_loss: 0.0039 - val_acc: 0.9995\n",
            "Epoch 23/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 4.8204e-04 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 1.0000\n",
            "Epoch 24/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 3.5872e-04 - acc: 1.0000 - val_loss: 0.0035 - val_acc: 0.9989\n",
            "Epoch 25/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 2.5895e-04 - acc: 1.0000 - val_loss: 0.0050 - val_acc: 0.9989\n",
            "Epoch 26/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 3.2701e-04 - acc: 1.0000 - val_loss: 0.0037 - val_acc: 0.9984\n",
            "Epoch 27/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.5948e-04 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 0.9995\n",
            "Epoch 28/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.3833e-04 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 0.9995\n",
            "Epoch 29/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.2583e-04 - acc: 1.0000 - val_loss: 0.0030 - val_acc: 0.9989\n",
            "Epoch 30/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4102e-04 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 0.9995\n",
            "Test loss: 0.0005852213284337234\n",
            "Test accuracy: 100.0 %\n",
            "Train on 7367 samples, validate on 1842 samples\n",
            "Epoch 1/30\n",
            "7367/7367 [==============================] - 21s 3ms/sample - loss: 1.2875 - acc: 0.5196 - val_loss: 0.6625 - val_acc: 0.7524\n",
            "Epoch 2/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.2454 - acc: 0.9139 - val_loss: 0.1031 - val_acc: 0.9582\n",
            "Epoch 3/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0719 - acc: 0.9787 - val_loss: 0.1870 - val_acc: 0.9077\n",
            "Epoch 4/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0499 - acc: 0.9855 - val_loss: 0.0826 - val_acc: 0.9663\n",
            "Epoch 5/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0069 - acc: 0.9988 - val_loss: 0.0183 - val_acc: 0.9924\n",
            "Epoch 6/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0081 - val_acc: 0.9967\n",
            "Epoch 7/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0128 - acc: 0.9965 - val_loss: 0.0237 - val_acc: 0.9924\n",
            "Epoch 8/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0042 - acc: 0.9990 - val_loss: 0.0129 - val_acc: 0.9951\n",
            "Epoch 9/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.1127 - acc: 0.9651 - val_loss: 0.0235 - val_acc: 0.9919\n",
            "Epoch 10/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0036 - acc: 0.9993 - val_loss: 0.1165 - val_acc: 0.9582\n",
            "Epoch 11/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0095 - acc: 0.9974 - val_loss: 0.0044 - val_acc: 0.9995\n",
            "Epoch 12/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0016 - acc: 0.9997 - val_loss: 0.0019 - val_acc: 1.0000\n",
            "Epoch 13/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 4.2671e-04 - acc: 1.0000 - val_loss: 0.0061 - val_acc: 0.9989\n",
            "Epoch 14/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0019 - acc: 0.9996 - val_loss: 0.0081 - val_acc: 0.9978\n",
            "Epoch 15/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 3.5204e-04 - acc: 1.0000 - val_loss: 0.0075 - val_acc: 0.9984\n",
            "Epoch 16/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0516 - acc: 0.9856 - val_loss: 0.1514 - val_acc: 0.9566\n",
            "Epoch 17/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0291 - acc: 0.9924 - val_loss: 0.0251 - val_acc: 0.9902\n",
            "Epoch 18/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0024 - acc: 0.9995 - val_loss: 0.0131 - val_acc: 0.9940\n",
            "Epoch 19/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0021 - val_acc: 0.9989\n",
            "Epoch 20/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 2.5451e-04 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 0.9989\n",
            "Epoch 21/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.5430e-04 - acc: 1.0000 - val_loss: 0.0040 - val_acc: 0.9973\n",
            "Epoch 22/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.2092e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9995\n",
            "Epoch 23/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.0731e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 1.0000\n",
            "Epoch 24/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.5126e-04 - acc: 1.0000 - val_loss: 0.0033 - val_acc: 0.9989\n",
            "Epoch 25/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.1745e-04 - acc: 1.0000 - val_loss: 0.0061 - val_acc: 0.9984\n",
            "Epoch 26/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 8.5484e-05 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 0.9995\n",
            "Epoch 27/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 6.8420e-05 - acc: 1.0000 - val_loss: 0.0028 - val_acc: 0.9995\n",
            "Epoch 28/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 6.5690e-05 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9995\n",
            "Epoch 29/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 6.7161e-05 - acc: 1.0000 - val_loss: 0.0037 - val_acc: 0.9989\n",
            "Epoch 30/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 5.5791e-05 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 0.9989\n",
            "Test loss: 0.002571024069011825\n",
            "Test accuracy: 99.89142417907715 %\n",
            "Train on 7367 samples, validate on 1842 samples\n",
            "Epoch 1/30\n",
            "7367/7367 [==============================] - 21s 3ms/sample - loss: 2.2176 - acc: 0.1666 - val_loss: 2.1972 - val_acc: 0.1194\n",
            "Epoch 2/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 2.1972 - acc: 0.1727 - val_loss: 2.1972 - val_acc: 0.1194\n",
            "Epoch 3/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 2.1972 - acc: 0.1788 - val_loss: 2.1972 - val_acc: 0.1194\n",
            "Epoch 4/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 2.1214 - acc: 0.1864 - val_loss: 1.8916 - val_acc: 0.3436\n",
            "Epoch 5/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.5975 - acc: 0.3308 - val_loss: 1.5858 - val_acc: 0.4300\n",
            "Epoch 6/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.5012 - acc: 0.3426 - val_loss: 1.5552 - val_acc: 0.4224\n",
            "Epoch 7/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4668 - acc: 0.3691 - val_loss: 1.4858 - val_acc: 0.4479\n",
            "Epoch 8/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4780 - acc: 0.3566 - val_loss: 1.4892 - val_acc: 0.4468\n",
            "Epoch 9/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4620 - acc: 0.3666 - val_loss: 1.4844 - val_acc: 0.4501\n",
            "Epoch 10/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4604 - acc: 0.3669 - val_loss: 1.4864 - val_acc: 0.4495\n",
            "Epoch 11/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4604 - acc: 0.3691 - val_loss: 1.4865 - val_acc: 0.4495\n",
            "Epoch 12/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4604 - acc: 0.3742 - val_loss: 1.4861 - val_acc: 0.4495\n",
            "Epoch 13/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4604 - acc: 0.3765 - val_loss: 1.4853 - val_acc: 0.4495\n",
            "Epoch 14/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4604 - acc: 0.3787 - val_loss: 1.4880 - val_acc: 0.4484\n",
            "Epoch 15/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4604 - acc: 0.3911 - val_loss: 1.4844 - val_acc: 0.4501\n",
            "Epoch 16/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4604 - acc: 0.3884 - val_loss: 1.4851 - val_acc: 0.4495\n",
            "Epoch 17/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4604 - acc: 0.3905 - val_loss: 1.4858 - val_acc: 0.4484\n",
            "Epoch 18/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4604 - acc: 0.3903 - val_loss: 1.4850 - val_acc: 0.4490\n",
            "Epoch 19/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4604 - acc: 0.3926 - val_loss: 1.4854 - val_acc: 0.4495\n",
            "Epoch 20/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4604 - acc: 0.3932 - val_loss: 1.4856 - val_acc: 0.4495\n",
            "Epoch 21/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4604 - acc: 0.3934 - val_loss: 1.4865 - val_acc: 0.4495\n",
            "Epoch 22/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4604 - acc: 0.3954 - val_loss: 1.4858 - val_acc: 0.4495\n",
            "Epoch 23/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4604 - acc: 0.3972 - val_loss: 1.4858 - val_acc: 0.4495\n",
            "Epoch 24/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4604 - acc: 0.3938 - val_loss: 1.4855 - val_acc: 0.4501\n",
            "Epoch 25/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4604 - acc: 0.3945 - val_loss: 1.4843 - val_acc: 0.4501\n",
            "Epoch 26/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4604 - acc: 0.3968 - val_loss: 1.4846 - val_acc: 0.4501\n",
            "Epoch 27/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4604 - acc: 0.3950 - val_loss: 1.4852 - val_acc: 0.4495\n",
            "Epoch 28/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4604 - acc: 0.3987 - val_loss: 1.4845 - val_acc: 0.4501\n",
            "Epoch 29/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4604 - acc: 0.3976 - val_loss: 1.4844 - val_acc: 0.4501\n",
            "Epoch 30/30\n",
            "7367/7367 [==============================] - 9s 1ms/sample - loss: 1.4604 - acc: 0.3984 - val_loss: 1.4853 - val_acc: 0.4490\n",
            "Test loss: 1.447936220791029\n",
            "Test accuracy: 48.8043487071991 %\n"
          ]
        }
      ],
      "source": [
        "# Define per-fold score containers\n",
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "\n",
        "kfold = KFold(n_splits = 10, shuffle = True)\n",
        "fold_no = 1\n",
        "for train,test in kfold.split(x_train,y_train):\n",
        "  inputs = Input(shape=(64, 32, 9))\n",
        "\n",
        "  flattened = Flatten()(inputs)\n",
        "\n",
        "  flattened_inputs_1 = Lambda(lambda x : x[:,      :2048*3 ])(flattened)\n",
        "  flattened_inputs_2 = Lambda(lambda x : x[:,2048*3:2048*6])(flattened)\n",
        "  flattened_inputs_3 = Lambda(lambda x : x[:,2048*6: ])(flattened)\n",
        "\n",
        "  R_1 = Lambda(lambda x : x[:,     :2048 ])(flattened_inputs_1)\n",
        "  G_1 = Lambda(lambda x : x[:, 2048:4096 ])(flattened_inputs_1)\n",
        "  B_1 = Lambda(lambda x : x[:, 4096:     ])(flattened_inputs_1)\n",
        "\n",
        "  R_2 = Lambda(lambda x : x[:,     :2048 ])(flattened_inputs_2)\n",
        "  G_2 = Lambda(lambda x : x[:, 2048:4096 ])(flattened_inputs_2)\n",
        "  B_2 = Lambda(lambda x : x[:, 4096:     ])(flattened_inputs_2)\n",
        "\n",
        "  R_3 = Lambda(lambda x : x[:,     :2048 ])(flattened_inputs_3)\n",
        "  G_3 = Lambda(lambda x : x[:, 2048:4096 ])(flattened_inputs_3)\n",
        "  B_3 = Lambda(lambda x : x[:, 4096:     ])(flattened_inputs_3)\n",
        "\n",
        "  # Init 21 cores\n",
        "\n",
        "  tea_0_1 = Tea(64)\n",
        "  tea_0_2 = Tea(64)\n",
        "  tea_0_3 = Tea(64)\n",
        "  tea_0_4 = Tea(64)\n",
        "  tea_0_5 = Tea(64)\n",
        "  tea_0_6 = Tea(64)\n",
        "  tea_0_7 = Tea(64)\n",
        "  tea_0_8 = Tea(64)\n",
        "  tea_0_9 = Tea(64)\n",
        "  tea_0_10 = Tea(64)\n",
        "  tea_0_11 = Tea(64)\n",
        "  tea_0_12 = Tea(64)\n",
        "  tea_0_13 = Tea(64)\n",
        "  tea_0_14 = Tea(64)\n",
        "  tea_0_15 = Tea(64)\n",
        "  tea_0_16 = Tea(64)\n",
        "\n",
        "  tea_1_1 = Tea(64)\n",
        "  tea_1_2 = Tea(64)\n",
        "  tea_1_3 = Tea(64)\n",
        "  tea_1_4 = Tea(64)\n",
        "\n",
        "  tea_2_1 = Tea(252)\n",
        "\n",
        "\n",
        "  x1_1  = Lambda(lambda x : x[:,     :256 ])(R_1)\n",
        "  x2_1  = Lambda(lambda x : x[:, 119 : 375 ])(R_1)\n",
        "  x3_1  = Lambda(lambda x : x[:, 238 :494 ])(R_1)\n",
        "  x4_1  = Lambda(lambda x : x[:, 357 : 613])(R_1)\n",
        "  x5_1  = Lambda(lambda x : x[:, 476:732])(R_1)\n",
        "  x6_1  = Lambda(lambda x : x[:, 595:851])(R_1)\n",
        "  x7_1  = Lambda(lambda x : x[:, 714:970])(R_1)\n",
        "  x8_1  = Lambda(lambda x : x[:, 833:1089])(R_1)\n",
        "  x9_1  = Lambda(lambda x : x[:, 952:1208])(R_1)\n",
        "  x10_1  = Lambda(lambda x : x[:, 1071:1327])(R_1)\n",
        "  x11_1  = Lambda(lambda x : x[:, 1190:1446])(R_1)\n",
        "  x12_1  = Lambda(lambda x : x[:, 1309:1565])(R_1)\n",
        "  x13_1  = Lambda(lambda x : x[:, 1428:1684])(R_1)\n",
        "  x14_1  = Lambda(lambda x : x[:, 1547:1803])(R_1)\n",
        "  x15_1  = Lambda(lambda x : x[:, 1666:1922])(R_1)\n",
        "  x16_1  = Lambda(lambda x : x[:, 1785:2041])(R_1)\n",
        "\n",
        "  x1_1  = tea_0_1(x1_1)\n",
        "  x2_1  = tea_0_2(x2_1)\n",
        "  x3_1  = tea_0_3(x3_1)\n",
        "  x4_1  = tea_0_4(x4_1)\n",
        "  x5_1  = tea_0_5(x5_1)\n",
        "  x6_1  = tea_0_6(x6_1)\n",
        "  x7_1  = tea_0_7(x7_1)\n",
        "  x8_1  = tea_0_8(x8_1)\n",
        "  x9_1  = tea_0_9(x9_1)\n",
        "  x10_1  = tea_0_10(x10_1)\n",
        "  x11_1  = tea_0_11(x11_1)\n",
        "  x12_1  = tea_0_12(x12_1)\n",
        "  x13_1  = tea_0_13(x13_1)\n",
        "  x14_1  = tea_0_14(x14_1)\n",
        "  x15_1  = tea_0_15(x15_1)\n",
        "  x16_1  = tea_0_16(x16_1)\n",
        "\n",
        "  x1_2  = Lambda(lambda x : x[:,     :256 ])(G_1)\n",
        "  x2_2  = Lambda(lambda x : x[:, 119 : 375 ])(G_1)\n",
        "  x3_2  = Lambda(lambda x : x[:, 238 :494 ])(G_1)\n",
        "  x4_2  = Lambda(lambda x : x[:, 357 : 613])(G_1)\n",
        "  x5_2  = Lambda(lambda x : x[:, 476:732])(G_1)\n",
        "  x6_2  = Lambda(lambda x : x[:, 595:851])(G_1)\n",
        "  x7_2  = Lambda(lambda x : x[:, 714:970])(G_1)\n",
        "  x8_2  = Lambda(lambda x : x[:, 833:1089])(G_1)\n",
        "  x9_2  = Lambda(lambda x : x[:, 952:1208])(G_1)\n",
        "  x10_2  = Lambda(lambda x : x[:, 1071:1327])(G_1)\n",
        "  x11_2  = Lambda(lambda x : x[:, 1190:1446])(G_1)\n",
        "  x12_2  = Lambda(lambda x : x[:, 1309:1565])(G_1)\n",
        "  x13_2  = Lambda(lambda x : x[:, 1428:1684])(G_1)\n",
        "  x14_2  = Lambda(lambda x : x[:, 1547:1803])(G_1)\n",
        "  x15_2  = Lambda(lambda x : x[:, 1666:1922])(G_1)\n",
        "  x16_2  = Lambda(lambda x : x[:, 1785:2041])(G_1)\n",
        "\n",
        "  x1_2  = tea_0_1(x1_2)\n",
        "  x2_2  = tea_0_2(x2_2)\n",
        "  x3_2  = tea_0_3(x3_2)\n",
        "  x4_2  = tea_0_4(x4_2)\n",
        "  x5_2  = tea_0_5(x5_2)\n",
        "  x6_2  = tea_0_6(x6_2)\n",
        "  x7_2  = tea_0_7(x7_2)\n",
        "  x8_2  = tea_0_8(x8_2)\n",
        "  x9_2  = tea_0_9(x9_2)\n",
        "  x10_2  = tea_0_10(x10_2)\n",
        "  x11_2  = tea_0_11(x11_2)\n",
        "  x12_2  = tea_0_12(x12_2)\n",
        "  x13_2  = tea_0_13(x13_2)\n",
        "  x14_2  = tea_0_14(x14_2)\n",
        "  x15_2  = tea_0_15(x15_2)\n",
        "  x16_2  = tea_0_16(x16_2)\n",
        "\n",
        "  x1_3  = Lambda(lambda x : x[:,     :256 ])(B_1)\n",
        "  x2_3  = Lambda(lambda x : x[:, 119 : 375 ])(B_1)\n",
        "  x3_3  = Lambda(lambda x : x[:, 238 :494 ])(B_1)\n",
        "  x4_3  = Lambda(lambda x : x[:, 357 : 613])(B_1)\n",
        "  x5_3  = Lambda(lambda x : x[:, 476:732])(B_1)\n",
        "  x6_3  = Lambda(lambda x : x[:, 595:851])(B_1)\n",
        "  x7_3  = Lambda(lambda x : x[:, 714:970])(B_1)\n",
        "  x8_3  = Lambda(lambda x : x[:, 833:1089])(B_1)\n",
        "  x9_3  = Lambda(lambda x : x[:, 952:1208])(B_1)\n",
        "  x10_3  = Lambda(lambda x : x[:, 1071:1327])(B_1)\n",
        "  x11_3  = Lambda(lambda x : x[:, 1190:1446])(B_1)\n",
        "  x12_3  = Lambda(lambda x : x[:, 1309:1565])(B_1)\n",
        "  x13_3  = Lambda(lambda x : x[:, 1428:1684])(B_1)\n",
        "  x14_3  = Lambda(lambda x : x[:, 1547:1803])(B_1)\n",
        "  x15_3  = Lambda(lambda x : x[:, 1666:1922])(B_1)\n",
        "  x16_3  = Lambda(lambda x : x[:, 1785:2041])(B_1)\n",
        "\n",
        "  x1_3  = tea_0_1(x1_3)\n",
        "  x2_3  = tea_0_2(x2_3)\n",
        "  x3_3  = tea_0_3(x3_3)\n",
        "  x4_3  = tea_0_4(x4_3)\n",
        "  x5_3  = tea_0_5(x5_3)\n",
        "  x6_3  = tea_0_6(x6_3)\n",
        "  x7_3  = tea_0_7(x7_3)\n",
        "  x8_3  = tea_0_8(x8_3)\n",
        "  x9_3  = tea_0_9(x9_3)\n",
        "  x10_3  = tea_0_10(x10_3)\n",
        "  x11_3  = tea_0_11(x11_3)\n",
        "  x12_3  = tea_0_12(x12_3)\n",
        "  x13_3  = tea_0_13(x13_3)\n",
        "  x14_3  = tea_0_14(x14_3)\n",
        "  x15_3  = tea_0_15(x15_3)\n",
        "  x16_3  = tea_0_16(x16_3)\n",
        "\n",
        "  ### 2 ###\n",
        "\n",
        "  x1_4  = Lambda(lambda x : x[:,     :256 ])(R_2)\n",
        "  x2_4  = Lambda(lambda x : x[:, 119 : 375 ])(R_2)\n",
        "  x3_4  = Lambda(lambda x : x[:, 238 :494 ])(R_2)\n",
        "  x4_4  = Lambda(lambda x : x[:, 357 : 613])(R_2)\n",
        "  x5_4  = Lambda(lambda x : x[:, 476:732])(R_2)\n",
        "  x6_4  = Lambda(lambda x : x[:, 595:851])(R_2)\n",
        "  x7_4  = Lambda(lambda x : x[:, 714:970])(R_2)\n",
        "  x8_4  = Lambda(lambda x : x[:, 833:1089])(R_2)\n",
        "  x9_4  = Lambda(lambda x : x[:, 952:1208])(R_2)\n",
        "  x10_4  = Lambda(lambda x : x[:, 1071:1327])(R_2)\n",
        "  x11_4  = Lambda(lambda x : x[:, 1190:1446])(R_2)\n",
        "  x12_4  = Lambda(lambda x : x[:, 1309:1565])(R_2)\n",
        "  x13_4  = Lambda(lambda x : x[:, 1428:1684])(R_2)\n",
        "  x14_4  = Lambda(lambda x : x[:, 1547:1803])(R_2)\n",
        "  x15_4  = Lambda(lambda x : x[:, 1666:1922])(R_2)\n",
        "  x16_4  = Lambda(lambda x : x[:, 1785:2041])(R_2)\n",
        "\n",
        "  x1_4  = tea_0_1(x1_4)\n",
        "  x2_4  = tea_0_2(x2_4)\n",
        "  x3_4  = tea_0_3(x3_4)\n",
        "  x4_4  = tea_0_4(x4_4)\n",
        "  x5_4  = tea_0_5(x5_4)\n",
        "  x6_4  = tea_0_6(x6_4)\n",
        "  x7_4  = tea_0_7(x7_4)\n",
        "  x8_4  = tea_0_8(x8_4)\n",
        "  x9_4  = tea_0_9(x9_4)\n",
        "  x10_4  = tea_0_10(x10_4)\n",
        "  x11_4  = tea_0_11(x11_4)\n",
        "  x12_4  = tea_0_12(x12_4)\n",
        "  x13_4  = tea_0_13(x13_4)\n",
        "  x14_4  = tea_0_14(x14_4)\n",
        "  x15_4  = tea_0_15(x15_4)\n",
        "  x16_4  = tea_0_16(x16_4)\n",
        "\n",
        "  x1_5  = Lambda(lambda x : x[:,     :256 ])(G_2)\n",
        "  x2_5  = Lambda(lambda x : x[:, 119 : 375 ])(G_2)\n",
        "  x3_5  = Lambda(lambda x : x[:, 238 :494 ])(G_2)\n",
        "  x4_5  = Lambda(lambda x : x[:, 357 : 613])(G_2)\n",
        "  x5_5  = Lambda(lambda x : x[:, 476:732])(G_2)\n",
        "  x6_5  = Lambda(lambda x : x[:, 595:851])(G_2)\n",
        "  x7_5  = Lambda(lambda x : x[:, 714:970])(G_2)\n",
        "  x8_5  = Lambda(lambda x : x[:, 833:1089])(G_2)\n",
        "  x9_5  = Lambda(lambda x : x[:, 952:1208])(G_2)\n",
        "  x10_5  = Lambda(lambda x : x[:, 1071:1327])(G_2)\n",
        "  x11_5  = Lambda(lambda x : x[:, 1190:1446])(G_2)\n",
        "  x12_5  = Lambda(lambda x : x[:, 1309:1565])(G_2)\n",
        "  x13_5  = Lambda(lambda x : x[:, 1428:1684])(G_2)\n",
        "  x14_5  = Lambda(lambda x : x[:, 1547:1803])(G_2)\n",
        "  x15_5  = Lambda(lambda x : x[:, 1666:1922])(G_2)\n",
        "  x16_5  = Lambda(lambda x : x[:, 1785:2041])(G_2)\n",
        "\n",
        "  x1_5  = tea_0_1(x1_5)\n",
        "  x2_5  = tea_0_2(x2_5)\n",
        "  x3_5  = tea_0_3(x3_5)\n",
        "  x4_5  = tea_0_4(x4_5)\n",
        "  x5_5  = tea_0_5(x5_5)\n",
        "  x6_5  = tea_0_6(x6_5)\n",
        "  x7_5  = tea_0_7(x7_5)\n",
        "  x8_5  = tea_0_8(x8_5)\n",
        "  x9_5  = tea_0_9(x9_5)\n",
        "  x10_5  = tea_0_10(x10_5)\n",
        "  x11_5  = tea_0_11(x11_5)\n",
        "  x12_5  = tea_0_12(x12_5)\n",
        "  x13_5  = tea_0_13(x13_5)\n",
        "  x14_5  = tea_0_14(x14_5)\n",
        "  x15_5  = tea_0_15(x15_5)\n",
        "  x16_5  = tea_0_16(x16_5)\n",
        "\n",
        "  x1_6  = Lambda(lambda x : x[:,     :256 ])(B_2)\n",
        "  x2_6  = Lambda(lambda x : x[:, 119 : 375 ])(B_2)\n",
        "  x3_6  = Lambda(lambda x : x[:, 238 :494 ])(B_2)\n",
        "  x4_6  = Lambda(lambda x : x[:, 357 : 613])(B_2)\n",
        "  x5_6  = Lambda(lambda x : x[:, 476:732])(B_2)\n",
        "  x6_6  = Lambda(lambda x : x[:, 595:851])(B_2)\n",
        "  x7_6  = Lambda(lambda x : x[:, 714:970])(B_2)\n",
        "  x8_6  = Lambda(lambda x : x[:, 833:1089])(B_2)\n",
        "  x9_6  = Lambda(lambda x : x[:, 952:1208])(B_2)\n",
        "  x10_6  = Lambda(lambda x : x[:, 1071:1327])(B_2)\n",
        "  x11_6  = Lambda(lambda x : x[:, 1190:1446])(B_2)\n",
        "  x12_6  = Lambda(lambda x : x[:, 1309:1565])(B_2)\n",
        "  x13_6  = Lambda(lambda x : x[:, 1428:1684])(B_2)\n",
        "  x14_6  = Lambda(lambda x : x[:, 1547:1803])(B_2)\n",
        "  x15_6  = Lambda(lambda x : x[:, 1666:1922])(B_2)\n",
        "  x16_6  = Lambda(lambda x : x[:, 1785:2041])(B_2)\n",
        "\n",
        "  x1_6  = tea_0_1(x1_6)\n",
        "  x2_6  = tea_0_2(x2_6)\n",
        "  x3_6  = tea_0_3(x3_6)\n",
        "  x4_6  = tea_0_4(x4_6)\n",
        "  x5_6  = tea_0_5(x5_6)\n",
        "  x6_6  = tea_0_6(x6_6)\n",
        "  x7_6  = tea_0_7(x7_6)\n",
        "  x8_6  = tea_0_8(x8_6)\n",
        "  x9_6  = tea_0_9(x9_6)\n",
        "  x10_6  = tea_0_10(x10_6)\n",
        "  x11_6  = tea_0_11(x11_6)\n",
        "  x12_6  = tea_0_12(x12_6)\n",
        "  x13_6  = tea_0_13(x13_6)\n",
        "  x14_6  = tea_0_14(x14_6)\n",
        "  x15_6  = tea_0_15(x15_6)\n",
        "  x16_6  = tea_0_16(x16_6)\n",
        "\n",
        "  ### 3 ###\n",
        "\n",
        "  x1_7  = Lambda(lambda x : x[:,     :256 ])(R_3)\n",
        "  x2_7  = Lambda(lambda x : x[:, 119 : 375 ])(R_3)\n",
        "  x3_7  = Lambda(lambda x : x[:, 238 :494 ])(R_3)\n",
        "  x4_7  = Lambda(lambda x : x[:, 357 : 613])(R_3)\n",
        "  x5_7  = Lambda(lambda x : x[:, 476:732])(R_3)\n",
        "  x6_7  = Lambda(lambda x : x[:, 595:851])(R_3)\n",
        "  x7_7  = Lambda(lambda x : x[:, 714:970])(R_3)\n",
        "  x8_7  = Lambda(lambda x : x[:, 833:1089])(R_3)\n",
        "  x9_7  = Lambda(lambda x : x[:, 952:1208])(R_3)\n",
        "  x10_7  = Lambda(lambda x : x[:, 1071:1327])(R_3)\n",
        "  x11_7  = Lambda(lambda x : x[:, 1190:1446])(R_3)\n",
        "  x12_7  = Lambda(lambda x : x[:, 1309:1565])(R_3)\n",
        "  x13_7  = Lambda(lambda x : x[:, 1428:1684])(R_3)\n",
        "  x14_7  = Lambda(lambda x : x[:, 1547:1803])(R_3)\n",
        "  x15_7  = Lambda(lambda x : x[:, 1666:1922])(R_3)\n",
        "  x16_7  = Lambda(lambda x : x[:, 1785:2041])(R_3)\n",
        "\n",
        "  x1_7  = tea_0_1(x1_7)\n",
        "  x2_7  = tea_0_2(x2_7)\n",
        "  x3_7  = tea_0_3(x3_7)\n",
        "  x4_7  = tea_0_4(x4_7)\n",
        "  x5_7  = tea_0_5(x5_7)\n",
        "  x6_7  = tea_0_6(x6_7)\n",
        "  x7_7  = tea_0_7(x7_7)\n",
        "  x8_7  = tea_0_8(x8_7)\n",
        "  x9_7  = tea_0_9(x9_7)\n",
        "  x10_7  = tea_0_10(x10_7)\n",
        "  x11_7  = tea_0_11(x11_7)\n",
        "  x12_7  = tea_0_12(x12_7)\n",
        "  x13_7  = tea_0_13(x13_7)\n",
        "  x14_7  = tea_0_14(x14_7)\n",
        "  x15_7  = tea_0_15(x15_7)\n",
        "  x16_7  = tea_0_16(x16_7)\n",
        "\n",
        "  x1_8  = Lambda(lambda x : x[:,     :256 ])(G_3)\n",
        "  x2_8  = Lambda(lambda x : x[:, 119 : 375 ])(G_3)\n",
        "  x3_8  = Lambda(lambda x : x[:, 238 :494 ])(G_3)\n",
        "  x4_8  = Lambda(lambda x : x[:, 357 : 613])(G_3)\n",
        "  x5_8  = Lambda(lambda x : x[:, 476:732])(G_3)\n",
        "  x6_8  = Lambda(lambda x : x[:, 595:851])(G_3)\n",
        "  x7_8  = Lambda(lambda x : x[:, 714:970])(G_3)\n",
        "  x8_8  = Lambda(lambda x : x[:, 833:1089])(G_3)\n",
        "  x9_8  = Lambda(lambda x : x[:, 952:1208])(G_3)\n",
        "  x10_8  = Lambda(lambda x : x[:, 1071:1327])(G_3)\n",
        "  x11_8  = Lambda(lambda x : x[:, 1190:1446])(G_3)\n",
        "  x12_8  = Lambda(lambda x : x[:, 1309:1565])(G_3)\n",
        "  x13_8  = Lambda(lambda x : x[:, 1428:1684])(G_3)\n",
        "  x14_8  = Lambda(lambda x : x[:, 1547:1803])(G_3)\n",
        "  x15_8  = Lambda(lambda x : x[:, 1666:1922])(G_3)\n",
        "  x16_8  = Lambda(lambda x : x[:, 1785:2041])(G_3)\n",
        "\n",
        "  x1_8  = tea_0_1(x1_8)\n",
        "  x2_8  = tea_0_2(x2_8)\n",
        "  x3_8  = tea_0_3(x3_8)\n",
        "  x4_8  = tea_0_4(x4_8)\n",
        "  x5_8  = tea_0_5(x5_8)\n",
        "  x6_8  = tea_0_6(x6_8)\n",
        "  x7_8  = tea_0_7(x7_8)\n",
        "  x8_8  = tea_0_8(x8_8)\n",
        "  x9_8  = tea_0_9(x9_8)\n",
        "  x10_8  = tea_0_10(x10_8)\n",
        "  x11_8  = tea_0_11(x11_8)\n",
        "  x12_8  = tea_0_12(x12_8)\n",
        "  x13_8  = tea_0_13(x13_8)\n",
        "  x14_8  = tea_0_14(x14_8)\n",
        "  x15_8  = tea_0_15(x15_8)\n",
        "  x16_8  = tea_0_16(x16_8)\n",
        "\n",
        "  x1_9  = Lambda(lambda x : x[:,     :256 ])(B_3)\n",
        "  x2_9  = Lambda(lambda x : x[:, 119 : 375 ])(B_3)\n",
        "  x3_9  = Lambda(lambda x : x[:, 238 :494 ])(B_3)\n",
        "  x4_9  = Lambda(lambda x : x[:, 357 : 613])(B_3)\n",
        "  x5_9  = Lambda(lambda x : x[:, 476:732])(B_3)\n",
        "  x6_9  = Lambda(lambda x : x[:, 595:851])(B_3)\n",
        "  x7_9  = Lambda(lambda x : x[:, 714:970])(B_3)\n",
        "  x8_9  = Lambda(lambda x : x[:, 833:1089])(B_3)\n",
        "  x9_9  = Lambda(lambda x : x[:, 952:1208])(B_3)\n",
        "  x10_9  = Lambda(lambda x : x[:, 1071:1327])(B_3)\n",
        "  x11_9  = Lambda(lambda x : x[:, 1190:1446])(B_3)\n",
        "  x12_9  = Lambda(lambda x : x[:, 1309:1565])(B_3)\n",
        "  x13_9  = Lambda(lambda x : x[:, 1428:1684])(B_3)\n",
        "  x14_9  = Lambda(lambda x : x[:, 1547:1803])(B_3)\n",
        "  x15_9  = Lambda(lambda x : x[:, 1666:1922])(B_3)\n",
        "  x16_9  = Lambda(lambda x : x[:, 1785:2041])(B_3)\n",
        "\n",
        "  x1_9  = tea_0_1(x1_9)\n",
        "  x2_9  = tea_0_2(x2_9)\n",
        "  x3_9  = tea_0_3(x3_9)\n",
        "  x4_9  = tea_0_4(x4_9)\n",
        "  x5_9  = tea_0_5(x5_9)\n",
        "  x6_9  = tea_0_6(x6_9)\n",
        "  x7_9  = tea_0_7(x7_9)\n",
        "  x8_9  = tea_0_8(x8_9)\n",
        "  x9_9  = tea_0_9(x9_9)\n",
        "  x10_9  = tea_0_10(x10_9)\n",
        "  x11_9  = tea_0_11(x11_9)\n",
        "  x12_9  = tea_0_12(x12_9)\n",
        "  x13_9  = tea_0_13(x13_9)\n",
        "  x14_9  = tea_0_14(x14_9)\n",
        "  x15_9  = tea_0_15(x15_9)\n",
        "  x16_9  = tea_0_16(x16_9)\n",
        "\n",
        "  #Average 1->5\n",
        "  x1_1_1 = Average()([x1_1,x1_2,x1_3,x1_4,x1_5])\n",
        "  x2_1_1 = Average()([x2_1,x2_2,x2_3,x2_4,x2_5])\n",
        "  x3_1_1 = Average()([x3_1,x3_2,x3_3,x3_4,x3_5])\n",
        "  x4_1_1 = Average()([x4_1,x4_2,x4_3,x4_4,x4_5])\n",
        "  x5_1_1 = Average()([x5_1,x5_2,x5_3,x5_4,x5_5])\n",
        "  x6_1_1 = Average()([x6_1,x6_2,x6_3,x6_4,x6_5])\n",
        "  x7_1_1 = Average()([x7_1,x7_2,x7_3,x7_4,x7_5])\n",
        "  x8_1_1 = Average()([x8_1,x8_2,x8_3,x8_4,x8_5])\n",
        "  x9_1_1 = Average()([x9_1,x9_2,x9_3,x9_4,x9_5])\n",
        "  x10_1_1 = Average()([x10_1,x10_2,x10_3,x10_4,x10_5])\n",
        "  x11_1_1 = Average()([x11_1,x11_2,x11_3,x11_4,x11_5])\n",
        "  x12_1_1 = Average()([x12_1,x12_2,x12_3,x12_4,x12_5])\n",
        "  x13_1_1 = Average()([x13_1,x13_2,x13_3,x13_4,x13_5])\n",
        "  x14_1_1 = Average()([x14_1,x14_2,x14_3,x14_4,x14_5])\n",
        "  x15_1_1 = Average()([x15_1,x15_2,x15_3,x15_4,x15_5])\n",
        "  x16_1_1 = Average()([x16_1,x16_2,x16_3,x16_4,x16_5])\n",
        "\n",
        "  #Average image 2->6\n",
        "\n",
        "  x1_1_2 = Average()([x1_2,x1_3,x1_4,x1_5,x1_6])\n",
        "  x2_1_2 = Average()([x2_2,x2_3,x2_4,x2_5,x2_6])\n",
        "  x3_1_2 = Average()([x3_2,x3_3,x3_4,x3_5,x3_6])\n",
        "  x4_1_2 = Average()([x4_2,x4_3,x4_4,x4_5,x4_6])\n",
        "  x5_1_2 = Average()([x5_2,x5_3,x5_4,x5_5,x5_6])\n",
        "  x6_1_2 = Average()([x6_2,x6_3,x6_4,x6_5,x6_6])\n",
        "  x7_1_2 = Average()([x7_2,x7_3,x7_4,x7_5,x7_6])\n",
        "  x8_1_2 = Average()([x8_2,x8_3,x8_4,x8_5,x8_6])\n",
        "  x9_1_2 = Average()([x9_2,x9_3,x9_4,x9_5,x9_6])\n",
        "  x10_1_2 = Average()([x10_2,x10_3,x10_4,x10_5,x10_6])\n",
        "  x11_1_2 = Average()([x11_2,x11_3,x11_4,x11_5,x11_6])\n",
        "  x12_1_2 = Average()([x12_2,x12_3,x12_4,x12_5,x12_6])\n",
        "  x13_1_2 = Average()([x13_2,x13_3,x13_4,x13_5,x13_6])\n",
        "  x14_1_2 = Average()([x14_2,x14_3,x14_4,x14_5,x14_6])\n",
        "  x15_1_2 = Average()([x15_2,x15_3,x15_4,x15_5,x15_6])\n",
        "  x16_1_2 = Average()([x16_2,x16_3,x16_4,x16_5,x16_6])\n",
        "\n",
        "  #Average image 3->7\n",
        "\n",
        "  x1_1_3 = Average()([x1_3,x1_4,x1_5,x1_6,x1_7])\n",
        "  x2_1_3 = Average()([x2_3,x2_4,x2_5,x2_6,x2_7])\n",
        "  x3_1_3 = Average()([x3_3,x3_4,x3_5,x3_6,x3_7])\n",
        "  x4_1_3 = Average()([x4_3,x4_4,x4_5,x4_6,x4_7])\n",
        "  x5_1_3 = Average()([x5_3,x5_4,x5_5,x5_6,x5_7])\n",
        "  x6_1_3 = Average()([x6_3,x6_4,x6_5,x6_6,x6_7])\n",
        "  x7_1_3 = Average()([x7_3,x7_4,x7_5,x7_6,x7_7])\n",
        "  x8_1_3 = Average()([x8_3,x8_4,x8_5,x8_6,x8_7])\n",
        "  x9_1_3 = Average()([x9_3,x9_4,x9_5,x9_6,x9_7])\n",
        "  x10_1_3 = Average()([x10_3,x10_4,x10_5,x10_6,x10_7])\n",
        "  x11_1_3 = Average()([x11_3,x11_4,x11_5,x11_6,x11_7])\n",
        "  x12_1_3 = Average()([x12_3,x12_4,x12_5,x12_6,x12_7])\n",
        "  x13_1_3 = Average()([x13_3,x13_4,x13_5,x13_6,x13_7])\n",
        "  x14_1_3 = Average()([x14_3,x14_4,x14_5,x14_6,x14_7])\n",
        "  x15_1_3 = Average()([x15_3,x15_4,x15_5,x15_6,x15_7])\n",
        "  x16_1_3 = Average()([x16_3,x16_4,x16_5,x16_6,x16_7])\n",
        "\n",
        "  #Average image 4->8\n",
        "\n",
        "  x1_1_4 = Average()([x1_4,x1_5,x1_6,x1_7,x1_8])\n",
        "  x2_1_4 = Average()([x2_4,x2_5,x2_6,x2_7,x2_8])\n",
        "  x3_1_4 = Average()([x3_4,x3_5,x3_6,x3_7,x3_8])\n",
        "  x4_1_4 = Average()([x4_4,x4_5,x4_6,x4_7,x4_8])\n",
        "  x5_1_4 = Average()([x5_4,x5_5,x5_6,x5_7,x5_8])\n",
        "  x6_1_4 = Average()([x6_4,x6_5,x6_6,x6_7,x6_8])\n",
        "  x7_1_4 = Average()([x7_4,x7_5,x7_6,x7_7,x7_8])\n",
        "  x8_1_4 = Average()([x8_4,x8_5,x8_6,x8_7,x8_8])\n",
        "  x9_1_4 = Average()([x9_4,x9_5,x9_6,x9_7,x9_8])\n",
        "  x10_1_4 = Average()([x10_4,x10_5,x10_6,x10_7,x10_8])\n",
        "  x11_1_4 = Average()([x11_4,x11_5,x11_6,x11_7,x11_8])\n",
        "  x12_1_4 = Average()([x12_4,x12_5,x12_6,x12_7,x12_8])\n",
        "  x13_1_4 = Average()([x13_4,x13_5,x13_6,x13_7,x13_8])\n",
        "  x14_1_4 = Average()([x14_4,x14_5,x14_6,x14_7,x14_8])\n",
        "  x15_1_4 = Average()([x15_4,x15_5,x15_6,x15_7,x15_8])\n",
        "  x16_1_4 = Average()([x16_4,x16_5,x16_6,x16_7,x16_8])\n",
        "\n",
        "  #Average image 5->9\n",
        "\n",
        "  x1_1_5 = Average()([x1_5,x1_6,x1_7,x1_8,x1_9])\n",
        "  x2_1_5 = Average()([x2_5,x2_6,x2_7,x2_8,x2_9])\n",
        "  x3_1_5 = Average()([x3_5,x3_6,x3_7,x3_8,x3_9])\n",
        "  x4_1_5 = Average()([x4_5,x4_6,x4_7,x4_8,x4_9])\n",
        "  x5_1_5 = Average()([x5_5,x5_6,x5_7,x5_8,x5_9])\n",
        "  x6_1_5 = Average()([x6_5,x6_6,x6_7,x6_8,x6_9])\n",
        "  x7_1_5 = Average()([x7_5,x7_6,x7_7,x7_8,x7_9])\n",
        "  x8_1_5 = Average()([x8_5,x8_6,x8_7,x8_8,x8_9])\n",
        "  x9_1_5 = Average()([x9_5,x9_6,x9_7,x9_6,x9_9])\n",
        "  x10_1_5 = Average()([x10_5,x10_6,x10_7,x10_8,x10_9])\n",
        "  x11_1_5 = Average()([x11_5,x11_6,x11_7,x11_8,x11_9])\n",
        "  x12_1_5 = Average()([x12_5,x12_6,x12_7,x12_8,x12_9])\n",
        "  x13_1_5 = Average()([x13_5,x13_6,x13_7,x13_8,x13_9])\n",
        "  x14_1_5 = Average()([x14_5,x14_6,x14_7,x14_8,x14_9])\n",
        "  x15_1_5 = Average()([x15_5,x15_6,x15_7,x15_8,x15_9])\n",
        "  x16_1_5 = Average()([x16_5,x16_6,x16_7,x16_8,x16_9])\n",
        "\n",
        "  x1_1 = concatenate(([x1_1_1,x2_1_1,x3_1_1,x4_1_1]),axis=1)\n",
        "  x2_1 = concatenate(([x5_1_1,x6_1_1,x7_1_1,x8_1_1]),axis=1)\n",
        "  x3_1 = concatenate(([x9_1_1,x10_1_1,x11_1_1,x12_1_1]),axis=1)\n",
        "  x4_1 = concatenate(([x13_1_1,x14_1_1,x15_1_1,x16_1_1]),axis=1)\n",
        "\n",
        "  x1_2 = concatenate(([x1_1_2,x2_1_2,x3_1_2,x4_1_2]),axis=1)\n",
        "  x2_2 = concatenate(([x5_1_2,x6_1_2,x7_1_2,x8_1_2]),axis=1)\n",
        "  x3_2 = concatenate(([x9_1_2,x10_1_2,x11_1_2,x12_1_2]),axis=1)\n",
        "  x4_2 = concatenate(([x13_1_2,x14_1_2,x15_1_2,x16_1_2]),axis=1)\n",
        "\n",
        "  x1_3 = concatenate(([x1_1_3,x2_1_3,x3_1_3,x4_1_3]),axis=1)\n",
        "  x2_3 = concatenate(([x5_1_3,x6_1_3,x7_1_3,x8_1_3]),axis=1)\n",
        "  x3_3 = concatenate(([x9_1_3,x10_1_3,x11_1_3,x12_1_3]),axis=1)\n",
        "  x4_3 = concatenate(([x13_1_3,x14_1_3,x15_1_3,x16_1_3]),axis=1)\n",
        "\n",
        "  x1_4 = concatenate(([x1_1_4,x2_1_4,x3_1_4,x4_1_4]),axis=1)\n",
        "  x2_4 = concatenate(([x5_1_4,x6_1_4,x7_1_4,x8_1_4]),axis=1)\n",
        "  x3_4 = concatenate(([x9_1_4,x10_1_4,x11_1_4,x12_1_4]),axis=1)\n",
        "  x4_4 = concatenate(([x13_1_4,x14_1_4,x15_1_4,x16_1_4]),axis=1)\n",
        "\n",
        "  x1_5 = concatenate(([x1_1_5,x2_1_5,x3_1_5,x4_1_5]),axis=1)\n",
        "  x2_5 = concatenate(([x5_1_5,x6_1_5,x7_1_5,x8_1_5]),axis=1)\n",
        "  x3_5 = concatenate(([x9_1_5,x10_1_5,x11_1_5,x12_1_5]),axis=1)\n",
        "  x4_5 = concatenate(([x13_1_5,x14_1_5,x15_1_5,x16_1_5]),axis=1)\n",
        "\n",
        "  x1_1 = tea_1_1(x1_1)\n",
        "  x2_1 = tea_1_2(x2_1)\n",
        "  x3_1 = tea_1_3(x3_1)\n",
        "  x4_1 = tea_1_4(x4_1)\n",
        "\n",
        "  x1_2 = tea_1_1(x1_2)\n",
        "  x2_2 = tea_1_2(x2_2)\n",
        "  x3_2 = tea_1_3(x3_2)\n",
        "  x4_2 = tea_1_4(x4_2)\n",
        "\n",
        "  x1_3 = tea_1_1(x1_3)\n",
        "  x2_3 = tea_1_2(x2_3)\n",
        "  x3_3 = tea_1_3(x3_3)\n",
        "  x4_3 = tea_1_4(x4_3)\n",
        "\n",
        "  x1_4 = tea_1_1(x1_4)\n",
        "  x2_4 = tea_1_2(x2_4)\n",
        "  x3_4 = tea_1_3(x3_4)\n",
        "  x4_4 = tea_1_4(x4_4)\n",
        "\n",
        "  x1_5 = tea_1_1(x1_5)\n",
        "  x2_5 = tea_1_2(x2_5)\n",
        "  x3_5 = tea_1_3(x3_5)\n",
        "  x4_5 = tea_1_4(x4_5)\n",
        "\n",
        "  #Average after layer 2\n",
        "  x_out_1 = Average()([x1_1,x1_2,x1_3,x1_4,x1_5])\n",
        "  x_out_2 = Average()([x2_1,x2_2,x2_3,x2_4,x2_5])\n",
        "  x_out_3 = Average()([x3_1,x3_2,x3_3,x3_4,x3_5])\n",
        "  x_out_4 = Average()([x4_1,x4_2,x4_3,x4_4,x4_5])\n",
        "\n",
        "  # Layer 3\n",
        "\n",
        "  x_out = concatenate(([x_out_1,x_out_2,x_out_3,x_out_4]),axis=1)\n",
        "\n",
        "  x_out = tea_2_1(x_out) # 252 divided by 9\n",
        "\n",
        "  x_out = AdditivePooling(9)(x_out)\n",
        "\n",
        "  predictions = Activation('softmax')(x_out)\n",
        "\n",
        "  #Model\n",
        "\n",
        "  model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=Adam(lr=0.0005),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  model.fit(x_train, y_train,\n",
        "            batch_size=64,\n",
        "            epochs=30,\n",
        "            verbose=1,\n",
        "            validation_split=0.2)\n",
        "\n",
        "  score = model.evaluate(x_train[test], y_train[test], verbose=0)\n",
        "  \n",
        "  acc_per_fold.append(score[1] * 100)\n",
        "  loss_per_fold.append(score[0])\n",
        "  \n",
        "  print('Test loss:', score[0])\n",
        "  print('Test accuracy:', score[1]*100,'%')\n",
        "\n",
        "  fold_no = fold_no + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ahHmxqziYw1W",
        "outputId": "3acd94c0-9c4a-4a32-b203-e9c6fcc79f15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Score per fold\n",
            "------------------------------------------------------------------------\n",
            "> Fold 1 - Loss: 0.0051498144671200076 - Accuracy: 99.78284239768982%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 2 - Loss: 0.006373972505431908 - Accuracy: 99.67426657676697%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 3 - Loss: 0.40355029074428656 - Accuracy: 88.92508149147034%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 4 - Loss: 0.3152379495830929 - Accuracy: 90.01085758209229%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 5 - Loss: 1.0184954059240483 - Accuracy: 55.591750144958496%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 6 - Loss: 0.10776251044031333 - Accuracy: 100.0%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 7 - Loss: 0.0008423418669881002 - Accuracy: 100.0%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 8 - Loss: 0.0005852213284337234 - Accuracy: 100.0%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 9 - Loss: 0.002571024069011825 - Accuracy: 99.89142417907715%\n",
            "------------------------------------------------------------------------\n",
            "> Fold 10 - Loss: 1.447936220791029 - Accuracy: 48.8043487071991%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 88.26805710792542 (+- 18.545361578747457)\n",
            "> Loss: 0.3308504751719756\n",
            "------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# == Provide average scores ==\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Score per fold')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print('------------------------------------------------------------------------')\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "RANC_supine_serial.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}