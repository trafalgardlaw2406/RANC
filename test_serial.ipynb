{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trafalgardlaw2406/RANC/blob/main/test_serial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ps4n7_5lNQbA",
        "outputId": "d7eda066-febe-4be5-b11e-3f947a778954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UolHgC8vNZMt",
        "outputId": "9952434f-c8c6-47cf-f26a-ddcaa97526b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import operator\n",
        "import functools\n",
        "import math\n",
        "import os\n",
        "\n",
        "from scipy import ndimage\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Activation, Input, Lambda, concatenate,Average\n",
        "from tensorflow.keras.datasets import mnist,fashion_mnist\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import constraints\n",
        "import sys\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.RegisterGradient(\"CustomRound\")\n",
        "def _const_round_grad(unused_op, grad):\n",
        "    return grad"
      ],
      "metadata": {
        "id": "5Hh4bRRq-sTo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bpuI-joJNq1V"
      },
      "outputs": [],
      "source": [
        "# Data Load\n",
        "import os\n",
        "import numpy as np\n",
        "import sys\n",
        "from scipy import ndimage\n",
        "\n",
        "# These functions are introduced along the Part 1 notebook.\n",
        "\n",
        "# Position vectors. We load the data with respect to the file name, which is\n",
        "# a number corresponding to a specific in-bed position. We take advantage of this\n",
        "# and use the number to get the position with help of the following vectors.\n",
        "\n",
        "positions_i = [\"justAPlaceholder\", \"supine_1\", \"right_0\",\n",
        "               \"left_0\", \"right_30\", \"right_60\",\n",
        "               \"left_30\", \"left_60\", \"supine_2\",\n",
        "               \"supine_3\", \"supine_4\", \"supine_5\",\n",
        "               \"supine_6\", \"right_fetus\", \"left_fetus\",\n",
        "               \"supine_30\", \"supine_45\", \"supine_60\"]\n",
        "\n",
        "positions_i_short = [\"justAPlaceholder\", \"supine\", \"right\",\n",
        "               \"left\", \"right\", \"right\",\n",
        "               \"left\", \"left\", \"supine\",\n",
        "               \"supine\", \"supine\", \"supine\",\n",
        "               \"supine\", \"right\", \"left\",\n",
        "               \"supine\", \"supine\", \"supine\"]\n",
        "\n",
        "positions_ii = {\n",
        "    \"B\":\"supine\", \"1\":\"supine\", \"C\":\"right\",\n",
        "    \"D\":\"left\", \"E1\":\"right\", \"E2\":\"right\",\n",
        "    \"E3\":\"left\", \"E4\":\"left\", \"E5\":\"right\",\n",
        "    \"E6\":\"left\", \"F\":\"supine\", \"G1\":\"supine\",\n",
        "    \"G2\":\"right\", \"G3\":\"left\"\n",
        "}\n",
        "\n",
        "class_positions = ['supine', 'left', 'right', 'left_fetus', 'right_fetus']\n",
        "\n",
        "# We also want the classes to be encoded as numbers so we can work easier when\n",
        "# modeling. This function achieves so. Since left_fetus and right_fetus are not\n",
        "# considered as classes in the evaluation of the original paper and since they\n",
        "# are not considered in the \"Experiment I\", we encode them also as left and right\n",
        "# positions.\n",
        "\n",
        "def token_position_short(x):\n",
        "  return {\n",
        "      'supine': 0,\n",
        "      'left': 1,\n",
        "      'right': 2,\n",
        "      'left_fetus': 1,\n",
        "      'right_fetus': 2\n",
        "  }[x]\n",
        "\n",
        "def token_position(x):\n",
        "  return {\n",
        "      \"supine_1\":0, \n",
        "      \"right_0\":1,\n",
        "      \"left_0\":2, \n",
        "      \"right_30\":3, \n",
        "      \"right_60\":4,\n",
        "      \"left_30\":5, \n",
        "      \"left_60\":6, \n",
        "      \"supine_2\":7,\n",
        "      \"supine_3\":8, \n",
        "      \"supine_4\":9, \n",
        "      \"supine_5\":10,\n",
        "      \"supine_6\":11, \n",
        "      \"right_fetus\":12, \n",
        "      \"left_fetus\":13,\n",
        "      \"supine_30\":14, \n",
        "      \"supine_45\":15, \n",
        "      \"supine_60\":16\n",
        "  }[x]\n",
        "\n",
        "def token_position_new(x):\n",
        "  return {\n",
        "      \"supine_1\":0, \n",
        "      \"supine_2\":1,\n",
        "      \"supine_3\":2, \n",
        "      \"supine_4\":3, \n",
        "      \"supine_5\":4,\n",
        "      \"supine_6\":5, \n",
        "      \"supine_30\":6, \n",
        "      \"supine_45\":7, \n",
        "      \"supine_60\":8, \n",
        "      \"left_0\":9, \n",
        "      \"left_30\":10, \n",
        "      \"left_60\":11,\n",
        "      \"left_fetus\":12, \n",
        "      \"right_0\":13,\n",
        "      \"right_30\":14, \n",
        "      \"right_60\":15,\n",
        "      \"right_fetus\":16, \n",
        "  }[x]\n",
        "list_supine = [\"1.txt\",\"8.txt\",\"9.txt\",\"10.txt\",\"11.txt\",\"12.txt\",\"15.txt\",\"16.txt\",\"17.txt\"]\n",
        "\n",
        "list_supine_norm_1 = [\"1.txt\",\"8.txt\",\"9.txt\"]\n",
        "\n",
        "list_supine_norm_2 = [\"10.txt\",\"11.txt\",\"12.txt\"]\n",
        "\n",
        "list_supine_incl = [\"15.txt\",\"16.txt\",\"17.txt\"]\n",
        "\n",
        "list_left = [\"3.txt\",\"6.txt\",\"7.txt\",\"14.txt\"]\n",
        "\n",
        "list_right = [\"2.txt\",\"4.txt\",\"5.txt\",\"13.txt\"]\n",
        "\n",
        "def token_position_supine(x):\n",
        "  return {\n",
        "      \"supine_1\":0, \n",
        "      \"supine_2\":1,\n",
        "      \"supine_3\":2, \n",
        "      \"supine_4\":3, \n",
        "      \"supine_5\":4,\n",
        "      \"supine_6\":5, \n",
        "      \"supine_30\":6, \n",
        "      \"supine_45\":7, \n",
        "      \"supine_60\":8\n",
        "    }[x]\n",
        "\n",
        "def token_position_supine_norm_1(x):\n",
        "  return {\n",
        "      \"supine_1\":0, \n",
        "      \"supine_2\":1,\n",
        "      \"supine_3\":2, \n",
        "    }[x]\n",
        "\n",
        "def token_position_supine_norm_2(x):\n",
        "  return {\n",
        "      \"supine_4\":0, \n",
        "      \"supine_5\":1,\n",
        "      \"supine_6\":2, \n",
        "    }[x]\n",
        "\n",
        "def token_position_supine_incl(x):\n",
        "  return {\n",
        "      \"supine_30\":0, \n",
        "      \"supine_45\":1, \n",
        "      \"supine_60\":2\n",
        "    }[x]\n",
        "\n",
        "def token_position_left(x):\n",
        "  return {\n",
        "      \"left_0\":0, \n",
        "      \"left_30\":1, \n",
        "      \"left_60\":2,\n",
        "      \"left_fetus\":3,\n",
        "  }[x]\n",
        "\n",
        "def token_position_right(x):\n",
        "  return {\n",
        "      \"right_0\":0,\n",
        "      \"right_30\":1, \n",
        "      \"right_60\":2,\n",
        "      \"right_fetus\":3, \n",
        "  }[x]\n",
        "\n",
        "\n",
        "def load_exp_i(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      max_val = []\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(3, len(lines) - 3):\n",
        "                              \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                \n",
        "                # Spatio-temporal median filter 3x3x3\n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                past_image = ndimage.median_filter(past_image, 3)\n",
        "                future_image = ndimage.median_filter(future_image, 3)\n",
        "                raw_data = np.concatenate((raw_data[np.newaxis, :, :], past_image[np.newaxis, :, :], future_image[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "            \n",
        "            # with open(file_path, 'r') as f:\n",
        "            #   # Start from second recording, as the first two are corrupted\n",
        "            #   lines = f.read().splitlines()[2:]\n",
        "            #   for line in f.read().splitlines()[2:]:\n",
        "            #     # print(line)\n",
        "            #     raw_data = np.fromstring(line, dtype=float, sep='\\t')\n",
        "                # Change the range from [0-1000] to [0-255].\n",
        "                  # max_val.append(np.amax(raw_data))\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              # file_data = np.round(raw_data).astype(np.uint8)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "              # print(positions_i[int(file[:-4])])\n",
        "              file_label = token_position(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "      \n",
        "      # max_over_all = max(max_val)\n",
        "      # print(max_over_all)\n",
        "\n",
        "      # data = np.round(data * 255/1000).astype(np.uint8)\n",
        "      dataset[subject] = (data, labels)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_short(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      max_val = []\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(3, len(lines) - 3):\n",
        "                              \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                \n",
        "                # Spatio-temporal median filter 3x3x3\n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                past_image = ndimage.median_filter(past_image, 3)\n",
        "                future_image = ndimage.median_filter(future_image, 3)\n",
        "                raw_data = np.concatenate((raw_data[np.newaxis, :, :], past_image[np.newaxis, :, :], future_image[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              # file_data = np.round(raw_data).astype(np.uint8)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "              # print(positions_i[int(file[:-4])])\n",
        "              file_label = token_position_short(positions_i_short[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_supine(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_supine\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(5, len(lines) - 5):\n",
        "                            \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image_1 = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_1 = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                past_image_2 = np.fromstring(lines[i-2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_2 = np.fromstring(lines[i+2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "                # Spatio-temporal median filter 5x5x5\n",
        "              \n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                \n",
        "                past_image_1 = ndimage.median_filter(past_image_1, 3)\n",
        "                future_image_1 = ndimage.median_filter(future_image_1, 3)\n",
        "                past_image_2 = ndimage.median_filter(past_image_2, 3)\n",
        "                future_image_2 = ndimage.median_filter(future_image_2, 3)\n",
        "\n",
        "                raw_data = np.concatenate((past_image_2[np.newaxis, :, :],past_image_1[np.newaxis, :, :] ,raw_data[np.newaxis, :, :], \\\n",
        "                future_image_1[np.newaxis, :, :],future_image_2[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "              \n",
        "              # a=np.amax(raw_data)\n",
        "\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              \n",
        "              # file_data = np.round(raw_data).astype(np.uint8)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "\n",
        "              file_label = token_position_supine(positions_i[int(file[:-4])])\n",
        "              \n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_supine_norm_1(path):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_supine_norm_1\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            for line in f.read().splitlines()[2:]:\n",
        "              # print(line)\n",
        "              raw_data = np.fromstring(line, dtype=float, sep='\\t')\n",
        "              # Change the range from [0-1000] to [0-255].\n",
        "              max_val = np.amax(raw_data)\n",
        "              file_data = np.round(raw_data*255/max_val).astype(np.uint8)\n",
        "              \n",
        "              # file_data = np.round(raw_data).astype(float)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "\n",
        "              file_label = token_position_supine_norm_1(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_supine_norm_2(path):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_supine_norm_2\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            for line in f.read().splitlines()[2:]:\n",
        "              # print(line)\n",
        "              raw_data = np.fromstring(line, dtype=float, sep='\\t')\n",
        "              # Change the range from [0-1000] to [0-255].\n",
        "              max_val = np.amax(raw_data)\n",
        "              file_data = np.round(raw_data*255/max_val).astype(np.uint8)\n",
        "              \n",
        "              # file_data = np.round(raw_data).astype(float)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "\n",
        "              file_label = token_position_supine_norm_2(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_supine_incl(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_supine_incl\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            # with open(file_path, 'r') as f:\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(3, len(lines) - 3):\n",
        "\n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                \n",
        "                # Spatio-temporal median filter 3x3x3\n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                past_image = ndimage.median_filter(past_image, 3)\n",
        "                future_image = ndimage.median_filter(future_image, 3)\n",
        "                raw_data = np.concatenate((raw_data[np.newaxis, :, :], past_image[np.newaxis, :, :], future_image[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "\n",
        "              # Change the range from [0-1000] to [0-255].\n",
        "              # max_vol = np.amax(raw_data)\n",
        "              file_data = np.round(raw_data ).astype(np.uint8)\n",
        "\n",
        "              # file_data = np.round(raw_data).astype(np.uint8)\n",
        "              file_data = file_data.reshape(1, 64, 32)\n",
        "\n",
        "              file_label = token_position_supine_incl(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_left(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_left\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(5, len(lines) - 5):\n",
        "                            \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image_1 = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_1 = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                past_image_2 = np.fromstring(lines[i-2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_2 = np.fromstring(lines[i+2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "                # Spatio-temporal median filter 5x5x5\n",
        "              \n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                \n",
        "                past_image_1 = ndimage.median_filter(past_image_1, 3)\n",
        "                future_image_1 = ndimage.median_filter(future_image_1, 3)\n",
        "                past_image_2 = ndimage.median_filter(past_image_2, 3)\n",
        "                future_image_2 = ndimage.median_filter(future_image_2, 3)\n",
        "\n",
        "                raw_data = np.concatenate((past_image_2[np.newaxis, :, :],past_image_1[np.newaxis, :, :] ,raw_data[np.newaxis, :, :], \\\n",
        "                future_image_1[np.newaxis, :, :],future_image_2[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "          # with open(file_path, 'r') as f:\n",
        "          #   # Start from second recording, as the first two are corrupted\n",
        "          #   for line in f.read().splitlines()[2:]:\n",
        "          #     # print(line)\n",
        "          #     raw_data = np.fromstring(line, dtype=float, sep='\\t')\n",
        "          #     # Change the range from [0-1000] to [0-255].\n",
        "\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              \n",
        "\n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "\n",
        "              file_label = token_position_left(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_right(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_right\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(5, len(lines) - 5):\n",
        "                            \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image_1 = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_1 = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                past_image_2 = np.fromstring(lines[i-2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_2 = np.fromstring(lines[i+2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "                # Spatio-temporal median filter 5x5x5\n",
        "              \n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                \n",
        "                past_image_1 = ndimage.median_filter(past_image_1, 3)\n",
        "                future_image_1 = ndimage.median_filter(future_image_1, 3)\n",
        "                past_image_2 = ndimage.median_filter(past_image_2, 3)\n",
        "                future_image_2 = ndimage.median_filter(future_image_2, 3)\n",
        "\n",
        "                raw_data = np.concatenate((past_image_2[np.newaxis, :, :],past_image_1[np.newaxis, :, :] ,raw_data[np.newaxis, :, :], \\\n",
        "                future_image_1[np.newaxis, :, :],future_image_2[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "              # Change the range from [0-1000] to [0-255].\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "\n",
        "              file_label = token_position_right(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_new(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      max_val = []\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(3, len(lines) - 3):\n",
        "                              \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                \n",
        "                # Spatio-temporal median filter 3x3x3\n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                past_image = ndimage.median_filter(past_image, 3)\n",
        "                future_image = ndimage.median_filter(future_image, 3)\n",
        "                raw_data = np.concatenate((raw_data[np.newaxis, :, :], past_image[np.newaxis, :, :], future_image[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "            \n",
        "            # with open(file_path, 'r') as f:\n",
        "            #   # Start from second recording, as the first two are corrupted\n",
        "            #   lines = f.read().splitlines()[2:]\n",
        "            #   for line in f.read().splitlines()[2:]:\n",
        "            #     # print(line)\n",
        "            #     raw_data = np.fromstring(line, dtype=float, sep='\\t')\n",
        "                # Change the range from [0-1000] to [0-255].\n",
        "                  # max_val.append(np.amax(raw_data))\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              # file_data = np.round(raw_data).astype(np.uint8)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "              # print(positions_i[int(file[:-4])])\n",
        "              file_label = token_position_new(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "      \n",
        "      # max_over_all = max(max_val)\n",
        "      # print(max_over_all)\n",
        "\n",
        "      # data = np.round(data * 255/1000).astype(np.uint8)\n",
        "      dataset[subject] = (data, labels)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "def load_exp_ii(path):\n",
        "\n",
        "  exp_ii_data_air = {}\n",
        "  exp_ii_data_spo = {}\n",
        "\n",
        "  # each directory is a subject\n",
        "  for _, subject_dirs, _ in os.walk(path):\n",
        "    for subject in subject_dirs:\n",
        "      data = None\n",
        "      labels = None\n",
        "\n",
        "      # each directory is a matresss\n",
        "      for _, mat_dirs, _ in os.walk(os.path.join(path, subject)):\n",
        "        for mat in mat_dirs:\n",
        "          for _, _, files in os.walk(os.path.join(path, subject, mat)):\n",
        "            for file in files:\n",
        "              file_path = os.path.join(path, subject, mat, file)\n",
        "              raw_data = np.loadtxt(file_path)\n",
        "              # Change the range from [0-500] to [0-255].\n",
        "              file_data = np.round(raw_data*255/500).astype(np.uint8)\n",
        "              \n",
        "              file_data = resize_and_rotate(file_data)\n",
        "              \n",
        "              file_data = file_data.view(1, 64, 32)\n",
        "\n",
        "              if file[-6] == \"E\" or file[-6] == \"G\":\n",
        "                file_label = positions_ii[file[-6:-4]]\n",
        "              else:\n",
        "                file_label = positions_ii[file[-6]]\n",
        "\n",
        "              file_label = token_position(file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "          if mat == \"Air_Mat\":\n",
        "            exp_ii_data_air[subject] = (data, labels)\n",
        "          else:\n",
        "            exp_ii_data_spo[subject] = (data, labels)\n",
        "\n",
        "          data = None\n",
        "          labels = None\n",
        "\n",
        "    return exp_ii_data_air, exp_ii_data_spo\n",
        "\n",
        "import cv2 \n",
        "\n",
        "class Mat_Dataset():\n",
        "  def __init__(self,datasets, mats, Subject_IDs):\n",
        "\n",
        "    self.samples = []\n",
        "    self.labels = []\n",
        "\n",
        "    for mat in mats:\n",
        "      data = datasets[mat]\n",
        "      self.samples.append(np.vstack([data.get(key)[0] for key in Subject_IDs]))\n",
        "      self.labels.append(np.hstack([data.get(key)[1] for key in Subject_IDs]))\n",
        "\n",
        "    self.samples = np.vstack(self.samples)\n",
        "    self.labels = np.hstack(self.labels)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.samples.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.samples[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7-wfjEPZNZ9O"
      },
      "outputs": [],
      "source": [
        "class Tea(Layer):\n",
        "    def __init__(self,\n",
        "                 units,\n",
        "                 num_SetsOfParameters = 1,\n",
        "                 **kwargs):\n",
        "        \"\"\"Initializes a new TeaLayer.\n",
        "\n",
        "        Arguments:\n",
        "            units -- The number of neurons to use for this layer.\"\"\"\n",
        "        self.units = units\n",
        "        # Needs to be set to `True` to use the `K.in_train_phase` function.\n",
        "        self.uses_learning_phase = True\n",
        "        self.nop = num_SetsOfParameters\n",
        "        self.connections = []\n",
        "        self.biases = []\n",
        "        super(Tea, self).__init__(**kwargs)\n",
        "\n",
        "def tea_weight_initializer(shape, dtype=np.float32):\n",
        "    \"\"\"Returns a tensor of alternating 1s and -1s, which is (kind of like)\n",
        "    how IBM initializes their weight matrix in their TeaLearning\n",
        "    literature.\n",
        "\n",
        "    Arguments:\n",
        "        shape -- The shape of the weights to intialize.\n",
        "\n",
        "    Keyword Arguments:\n",
        "        dtype -- The data type to use to initialize the weights.\n",
        "                 (default: {np.float32})\"\"\"\n",
        "    num_axons = shape[0]\n",
        "    num_neurons = shape[1]\n",
        "    ret_array = np.zeros((int(num_axons), int(num_neurons)), dtype=np.float32)\n",
        "    for axon_num, axon in enumerate(ret_array):\n",
        "        if axon_num % 2 == 0:\n",
        "            for i in range(len(axon)):\n",
        "                ret_array[axon_num][i] = 1\n",
        "        else:\n",
        "            for i in range(len(axon)):\n",
        "                ret_array[axon_num][i] = -1\n",
        "    return tf.convert_to_tensor(ret_array)\n",
        "\n",
        "def build(self, input_shape):\n",
        "    assert len(input_shape) >= 2\n",
        "    shape = (input_shape[-1], self.units)\n",
        "\n",
        "    self.static_weights = self.add_weight(\n",
        "        name='weights',\n",
        "        shape=shape,\n",
        "        initializer=tea_weight_initializer,\n",
        "        trainable=False)\n",
        "\n",
        "    for i in range(self.nop):\n",
        "      # Intialize connections around 0.5 because they represent probabilities.\n",
        "      connections = self.add_weight(\n",
        "          name='connections',\n",
        "          initializer=initializers.TruncatedNormal(mean=0.5),\n",
        "          shape=shape)\n",
        "      biases = self.add_weight(\n",
        "          name='biases',\n",
        "          initializer='zeros',\n",
        "          shape=(self.units,))\n",
        "      self.connections.append(connections)\n",
        "      self.biases.append(biases)\n",
        "    super(Tea, self).build(input_shape)\n",
        "\n",
        "# Bind the method to our class\n",
        "Tea.build = build\n",
        "\n",
        "def call(self, x, round = 0):\n",
        "    with tf.get_default_graph().gradient_override_map(\n",
        "        {\"Round\":\"CustomRound\"}):\n",
        "        # Constrain input\n",
        "        x = tf.round(x)\n",
        "        # Constrain connections\n",
        "        connections = self.connections[round]\n",
        "        connections = tf.round(connections)\n",
        "        connections = K.clip(connections, 0, 1)\n",
        "        # Multiply connections with weights\n",
        "        weighted_connections = connections * self.static_weights\n",
        "        # Dot input with weighted connections\n",
        "        output = K.dot(x, weighted_connections)\n",
        "        # Constrain biases\n",
        "        biases = tf.round(self.biases[round])\n",
        "        output = K.bias_add(\n",
        "            output,\n",
        "            biases,\n",
        "            data_format='channels_last'\n",
        "        )\n",
        "        # Apply activation / spike\n",
        "        output = K.in_train_phase(\n",
        "            K.sigmoid(output),\n",
        "            tf.cast(tf.greater_equal(output, 0.0), tf.float32)\n",
        "        )\n",
        "    return output\n",
        "    \n",
        "# Bind the method to our class\n",
        "Tea.call = call\n",
        "\n",
        "def compute_output_shape(self, input_shape):\n",
        "    assert input_shape and len(input_shape) >= 2\n",
        "    assert input_shape[-1]\n",
        "    output_shape = list(input_shape)\n",
        "    output_shape[-1] = self.units\n",
        "    return tuple(output_shape)\n",
        "    \n",
        "# Bind the method to our class\n",
        "Tea.compute_output_shape = compute_output_shape\n",
        "\n",
        "class AdditivePooling(Layer):\n",
        "    \"\"\"A helper layer designed to format data for output during TeaLearning.\n",
        "    If the data input to the layer has multiple spikes per classification, the\n",
        "    spikes for each tick are summed up. Then, all neurons that correspond to a\n",
        "    certain class are summed up so that the output is the number of spikes for\n",
        "    each class. Neurons are assumed to be arranged such that each\n",
        "    `num_classes` neurons represent a guess for each of the classes. For\n",
        "    example, if the guesses correspond to number from 0 to 9, the nuerons are\n",
        "    arranged as such:\n",
        "\n",
        "        neuron_num: 0  1  2  3  4  5  6  7  8  9  10 11 12  ...\n",
        "        guess:      0  1  2  3  4  5  6  7  8  9  0  1  2   ...\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_classes,\n",
        "                 **kwargs):\n",
        "        \"\"\"Initializes a new `AdditivePooling` layer.\n",
        "\n",
        "        Arguments:\n",
        "            num_classes -- The number of classes to output.\n",
        "        \"\"\"\n",
        "        self.num_classes = num_classes\n",
        "        self.num_inputs = None\n",
        "        super(AdditivePooling, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 2\n",
        "        # The number of neurons must be collapsable into the number of classes\n",
        "        assert input_shape[-1] % self.num_classes == 0\n",
        "        self.num_inputs = input_shape[-1]\n",
        "\n",
        "    def call(self, x):\n",
        "        # Sum up ticks if there are ticks\n",
        "        if len(x.shape) >= 3:\n",
        "            output = K.sum(x, axis=1)\n",
        "        else:\n",
        "            output = x\n",
        "        # Reshape output\n",
        "        output = tf.reshape(\n",
        "            output,\n",
        "            [-1, int(self.num_inputs // self.num_classes), self.num_classes]\n",
        "        )\n",
        "        # Sum up neurons\n",
        "        output = tf.reduce_sum(output, 1)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_shape = list(input_shape)\n",
        "        # Last dimension will be number of classes\n",
        "        output_shape[-1] = self.num_classes\n",
        "        # Ticks were summed, so delete tick dimension if exists\n",
        "        if len(output_shape) >= 3:\n",
        "            del output_shape[1]\n",
        "        return tuple(output_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ri2wS_-sQu6V"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VZiVpn5pNv-0"
      },
      "outputs": [],
      "source": [
        "exp_i_data = load_exp_i_supine(\"/content/drive/MyDrive/RANC/dataset/experiment-i\")\n",
        "\n",
        "datasets = {\"Base\":exp_i_data}\n",
        "subjects = [\"S1\",\"S2\",\"S3\",\"S4\",\"S5\",\"S6\",\"S7\",\"S8\",\"S9\",\"S10\",\"S11\",\"S12\",\"S13\"]\n",
        "sub = \"S2\"\n",
        "\n",
        "subjects.remove(sub)\n",
        "random.seed(1)\n",
        "random.shuffle(subjects)\n",
        "\n",
        "train_data = Mat_Dataset(datasets,[\"Base\"],subjects)\n",
        "test_data = Mat_Dataset(datasets,[\"Base\"],[sub])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AvvLWsclRDo9"
      },
      "outputs": [],
      "source": [
        "y_train = to_categorical(train_data.labels, 9)\n",
        "y_test = to_categorical(test_data.labels, 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "o3_R3fOZRJYu"
      },
      "outputs": [],
      "source": [
        "x_train = []\n",
        "\n",
        "for i in range(len(train_data.samples)):\n",
        "    \n",
        "    train_data.samples[i] = cv2.equalizeHist(train_data.samples[i])\n",
        "\n",
        "    heat = cv2.applyColorMap(train_data.samples[i], cv2.COLORMAP_JET)\n",
        "    mask = np.ones_like(heat)\n",
        "    bin1 = np.array(heat>=mask*63).astype(np.uint8)\n",
        "    bin2 = np.array(heat>=mask*127).astype(np.uint8)\n",
        "    bin3 = np.array(heat>=mask*190).astype(np.uint8)\n",
        "    bin_out = np.concatenate((bin1,bin2,bin3),axis=2)\n",
        "\n",
        "    x_train.append(bin_out)\n",
        "\n",
        "x_test = []\n",
        "\n",
        "for i in range(len(test_data.samples)):\n",
        "\n",
        "    test_data.samples[i] = cv2.equalizeHist(test_data.samples[i])\n",
        "    \n",
        "    heat = cv2.applyColorMap(test_data.samples[i], cv2.COLORMAP_JET)\n",
        "    mask = np.ones_like(heat)\n",
        "    bin1 = np.array(heat>=mask*63).astype(np.uint8)\n",
        "    bin2 = np.array(heat>=mask*127).astype(np.uint8)\n",
        "    bin3 = np.array(heat>=mask*190).astype(np.uint8)\n",
        "    bin_out = np.concatenate((bin1,bin2,bin3),axis=2)\n",
        "\n",
        "    x_test.append(bin_out)\n",
        "x_train = np.array(x_train).astype(np.uint8)\n",
        "x_test = np.array(x_test).astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYlNOuVLRKzf",
        "outputId": "1b939b01-a714-4341-bdd0-1089ed72d5b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 8656 samples\n",
            "Epoch 1/50\n",
            "8656/8656 [==============================] - 8s 883us/sample - loss: 1.4910 - acc: 0.4972\n",
            "Epoch 2/50\n",
            "8656/8656 [==============================] - 4s 417us/sample - loss: 0.2017 - acc: 0.9536\n",
            "Epoch 3/50\n",
            "8656/8656 [==============================] - 4s 410us/sample - loss: 0.0440 - acc: 0.9960\n",
            "Epoch 4/50\n",
            "8656/8656 [==============================] - 4s 413us/sample - loss: 0.0230 - acc: 0.9977\n",
            "Epoch 5/50\n",
            "8656/8656 [==============================] - 4s 415us/sample - loss: 0.0123 - acc: 0.9993\n",
            "Epoch 6/50\n",
            "8656/8656 [==============================] - 4s 418us/sample - loss: 0.0092 - acc: 0.9999\n",
            "Epoch 7/50\n",
            "8656/8656 [==============================] - 4s 421us/sample - loss: 0.0077 - acc: 0.9995\n",
            "Epoch 8/50\n",
            "8656/8656 [==============================] - 4s 419us/sample - loss: 0.0063 - acc: 0.9997\n",
            "Epoch 9/50\n",
            "8656/8656 [==============================] - 4s 414us/sample - loss: 0.0043 - acc: 0.9999\n",
            "Epoch 10/50\n",
            "8656/8656 [==============================] - 4s 409us/sample - loss: 0.0040 - acc: 0.9998\n",
            "Epoch 11/50\n",
            "8656/8656 [==============================] - 4s 416us/sample - loss: 0.0023 - acc: 1.0000\n",
            "Epoch 12/50\n",
            "8656/8656 [==============================] - 4s 416us/sample - loss: 0.0020 - acc: 1.0000\n",
            "Epoch 13/50\n",
            "8656/8656 [==============================] - 4s 412us/sample - loss: 0.0024 - acc: 0.9999\n",
            "Epoch 14/50\n",
            "8656/8656 [==============================] - 4s 410us/sample - loss: 0.0020 - acc: 1.0000\n",
            "Epoch 15/50\n",
            "8656/8656 [==============================] - 4s 414us/sample - loss: 0.0017 - acc: 1.0000\n",
            "Epoch 16/50\n",
            "8656/8656 [==============================] - 4s 405us/sample - loss: 0.0015 - acc: 1.0000\n",
            "Epoch 17/50\n",
            "8656/8656 [==============================] - 4s 423us/sample - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 18/50\n",
            "8656/8656 [==============================] - 4s 411us/sample - loss: 9.2461e-04 - acc: 1.0000\n",
            "Epoch 19/50\n",
            "8656/8656 [==============================] - 4s 416us/sample - loss: 0.0010 - acc: 1.0000\n",
            "Epoch 20/50\n",
            "8656/8656 [==============================] - 4s 408us/sample - loss: 7.1396e-04 - acc: 1.0000\n",
            "Epoch 21/50\n",
            "8656/8656 [==============================] - 4s 420us/sample - loss: 9.1643e-04 - acc: 0.9999\n",
            "Epoch 22/50\n",
            "8656/8656 [==============================] - 4s 416us/sample - loss: 7.0049e-04 - acc: 1.0000\n",
            "Epoch 23/50\n",
            "8656/8656 [==============================] - 4s 414us/sample - loss: 5.5095e-04 - acc: 1.0000\n",
            "Epoch 24/50\n",
            "8656/8656 [==============================] - 4s 414us/sample - loss: 6.0641e-04 - acc: 1.0000\n",
            "Epoch 25/50\n",
            "8656/8656 [==============================] - 4s 409us/sample - loss: 9.1444e-04 - acc: 1.0000\n",
            "Epoch 26/50\n",
            "8656/8656 [==============================] - 4s 427us/sample - loss: 7.5543e-04 - acc: 0.9999\n",
            "Epoch 27/50\n",
            "8656/8656 [==============================] - 4s 408us/sample - loss: 4.8715e-04 - acc: 1.0000\n",
            "Epoch 28/50\n",
            "8656/8656 [==============================] - 4s 411us/sample - loss: 4.4003e-04 - acc: 1.0000\n",
            "Epoch 29/50\n",
            "8656/8656 [==============================] - 4s 409us/sample - loss: 3.6739e-04 - acc: 1.0000\n",
            "Epoch 30/50\n",
            "8656/8656 [==============================] - 4s 412us/sample - loss: 5.0118e-04 - acc: 1.0000\n",
            "Epoch 31/50\n",
            "8656/8656 [==============================] - 4s 418us/sample - loss: 5.2563e-04 - acc: 1.0000\n",
            "Epoch 32/50\n",
            "8656/8656 [==============================] - 4s 416us/sample - loss: 4.2070e-04 - acc: 1.0000\n",
            "Epoch 33/50\n",
            "8656/8656 [==============================] - 4s 409us/sample - loss: 3.9043e-04 - acc: 1.0000\n",
            "Epoch 34/50\n",
            "8656/8656 [==============================] - 4s 414us/sample - loss: 3.2459e-04 - acc: 1.0000\n",
            "Epoch 35/50\n",
            "8656/8656 [==============================] - 4s 414us/sample - loss: 3.4619e-04 - acc: 1.0000\n",
            "Epoch 36/50\n",
            "8656/8656 [==============================] - 4s 415us/sample - loss: 2.7954e-04 - acc: 1.0000\n",
            "Epoch 37/50\n",
            "8656/8656 [==============================] - 4s 414us/sample - loss: 3.9924e-04 - acc: 1.0000\n",
            "Epoch 38/50\n",
            "8656/8656 [==============================] - 4s 411us/sample - loss: 2.2021e-04 - acc: 1.0000\n",
            "Epoch 39/50\n",
            "8656/8656 [==============================] - 4s 408us/sample - loss: 2.6151e-04 - acc: 1.0000\n",
            "Epoch 40/50\n",
            "8656/8656 [==============================] - 4s 407us/sample - loss: 1.4971e-04 - acc: 1.0000\n",
            "Epoch 41/50\n",
            "8656/8656 [==============================] - 4s 408us/sample - loss: 1.7395e-04 - acc: 1.0000\n",
            "Epoch 42/50\n",
            "8656/8656 [==============================] - 4s 408us/sample - loss: 1.9161e-04 - acc: 1.0000\n",
            "Epoch 43/50\n",
            "8656/8656 [==============================] - 4s 407us/sample - loss: 2.0455e-04 - acc: 1.0000\n",
            "Epoch 44/50\n",
            "8656/8656 [==============================] - 4s 412us/sample - loss: 1.4486e-04 - acc: 1.0000\n",
            "Epoch 45/50\n",
            "8656/8656 [==============================] - 4s 416us/sample - loss: 1.5687e-04 - acc: 1.0000\n",
            "Epoch 46/50\n",
            "8656/8656 [==============================] - 4s 405us/sample - loss: 1.3533e-04 - acc: 1.0000\n",
            "Epoch 47/50\n",
            "8656/8656 [==============================] - 3s 396us/sample - loss: 2.2281e-04 - acc: 1.0000\n",
            "Epoch 48/50\n",
            "8656/8656 [==============================] - 4s 409us/sample - loss: 4.2487e-04 - acc: 1.0000\n",
            "Epoch 49/50\n",
            "8656/8656 [==============================] - 4s 414us/sample - loss: 1.4089e-04 - acc: 1.0000\n",
            "Epoch 50/50\n",
            "8656/8656 [==============================] - 3s 404us/sample - loss: 1.3364e-04 - acc: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 2.2852630724365843\n",
            "Test accuracy: 0.6708861\n"
          ]
        }
      ],
      "source": [
        "inputs = Input(shape=(64, 32, 9))\n",
        "\n",
        "flattened = Flatten()(inputs)\n",
        "\n",
        "# Init 21 cores\n",
        "\n",
        "tea_0_1 = Tea(units = 64,  num_SetsOfParameters = 9)\n",
        "tea_0_2 = Tea(units = 64,  num_SetsOfParameters = 9)\n",
        "tea_0_3 = Tea(units = 64,  num_SetsOfParameters = 9)\n",
        "tea_0_4 = Tea(units = 64,  num_SetsOfParameters = 9)\n",
        "tea_0_5 = Tea(units = 64,  num_SetsOfParameters = 9)\n",
        "tea_0_6 = Tea(units = 64,  num_SetsOfParameters = 9)\n",
        "tea_0_7 = Tea(units = 64,  num_SetsOfParameters = 9)\n",
        "tea_0_8 = Tea(units = 64,  num_SetsOfParameters = 9)\n",
        "tea_0_9 = Tea(units = 64,  num_SetsOfParameters = 9)\n",
        "tea_0_10 = Tea(units = 64,  num_SetsOfParameters = 9)\n",
        "tea_0_11 = Tea(units = 64,  num_SetsOfParameters = 9)\n",
        "tea_0_12 = Tea(units = 64,  num_SetsOfParameters = 9)\n",
        "tea_0_13 = Tea(units = 64,  num_SetsOfParameters = 9)\n",
        "tea_0_14 = Tea(units = 64,  num_SetsOfParameters = 9)\n",
        "tea_0_15 = Tea(units = 64,  num_SetsOfParameters = 9)\n",
        "tea_0_16 = Tea(units = 64,  num_SetsOfParameters = 9)\n",
        "\n",
        "tea_1_1 = Tea(units = 64,  num_SetsOfParameters = 5)\n",
        "tea_1_2 = Tea(units = 64,  num_SetsOfParameters = 5)\n",
        "tea_1_3 = Tea(units = 64,  num_SetsOfParameters = 5)\n",
        "tea_1_4 = Tea(units = 64,  num_SetsOfParameters = 5)\n",
        "\n",
        "tea_2_1 = Tea(252)\n",
        "\n",
        "R_1 = Lambda(lambda x : x[:,     :1*2048 ])(flattened)\n",
        "G_1 = Lambda(lambda x : x[:, 1*2048:2*2048])(flattened)\n",
        "B_1 = Lambda(lambda x : x[:, 2*2048:3*2048])(flattened)\n",
        "\n",
        "R_2 = Lambda(lambda x : x[:, 3*2048:4*2048 ])(flattened)\n",
        "G_2 = Lambda(lambda x : x[:, 4*2048:5*2048 ])(flattened)\n",
        "B_2 = Lambda(lambda x : x[:, 5*2048:6*2048 ])(flattened)\n",
        "\n",
        "R_3 = Lambda(lambda x : x[:, 6*2048:7*2048])(flattened)\n",
        "G_3 = Lambda(lambda x : x[:, 7*2048:8*2048])(flattened)\n",
        "B_3 = Lambda(lambda x : x[:, 8*2048:9*2048])(flattened)\n",
        "\n",
        "x1_1  = Lambda(lambda x : x[:,     :256 ])(R_1)\n",
        "x2_1  = Lambda(lambda x : x[:, 119 : 375 ])(R_1)\n",
        "x3_1  = Lambda(lambda x : x[:, 238 :494 ])(R_1)\n",
        "x4_1  = Lambda(lambda x : x[:, 357 : 613])(R_1)\n",
        "x5_1  = Lambda(lambda x : x[:, 476:732])(R_1)\n",
        "x6_1  = Lambda(lambda x : x[:, 595:851])(R_1)\n",
        "x7_1  = Lambda(lambda x : x[:, 714:970])(R_1)\n",
        "x8_1  = Lambda(lambda x : x[:, 833:1089])(R_1)\n",
        "x9_1  = Lambda(lambda x : x[:, 952:1208])(R_1)\n",
        "x10_1  = Lambda(lambda x : x[:, 1071:1327])(R_1)\n",
        "x11_1  = Lambda(lambda x : x[:, 1190:1446])(R_1)\n",
        "x12_1  = Lambda(lambda x : x[:, 1309:1565])(R_1)\n",
        "x13_1  = Lambda(lambda x : x[:, 1428:1684])(R_1)\n",
        "x14_1  = Lambda(lambda x : x[:, 1547:1803])(R_1)\n",
        "x15_1  = Lambda(lambda x : x[:, 1666:1922])(R_1)\n",
        "x16_1  = Lambda(lambda x : x[:, 1785:2041])(R_1)\n",
        "\n",
        "x1_1  = tea_0_1(x1_1,0)\n",
        "x2_1  = tea_0_2(x2_1,0)\n",
        "x3_1  = tea_0_3(x3_1,0)\n",
        "x4_1  = tea_0_4(x4_1,0)\n",
        "x5_1  = tea_0_5(x5_1,0)\n",
        "x6_1  = tea_0_6(x6_1,0)\n",
        "x7_1  = tea_0_7(x7_1,0)\n",
        "x8_1  = tea_0_8(x8_1,0)\n",
        "x9_1  = tea_0_9(x9_1,0)\n",
        "x10_1  = tea_0_10(x10_1,0)\n",
        "x11_1  = tea_0_11(x11_1,0)\n",
        "x12_1  = tea_0_12(x12_1,0)\n",
        "x13_1  = tea_0_13(x13_1,0)\n",
        "x14_1  = tea_0_14(x14_1,0)\n",
        "x15_1  = tea_0_15(x15_1,0)\n",
        "x16_1  = tea_0_16(x16_1,0)\n",
        "\n",
        "x1_2  = Lambda(lambda x : x[:,     :256 ])(G_1)\n",
        "x2_2  = Lambda(lambda x : x[:, 119 : 375 ])(G_1)\n",
        "x3_2  = Lambda(lambda x : x[:, 238 :494 ])(G_1)\n",
        "x4_2  = Lambda(lambda x : x[:, 357 : 613])(G_1)\n",
        "x5_2  = Lambda(lambda x : x[:, 476:732])(G_1)\n",
        "x6_2  = Lambda(lambda x : x[:, 595:851])(G_1)\n",
        "x7_2  = Lambda(lambda x : x[:, 714:970])(G_1)\n",
        "x8_2  = Lambda(lambda x : x[:, 833:1089])(G_1)\n",
        "x9_2  = Lambda(lambda x : x[:, 952:1208])(G_1)\n",
        "x10_2  = Lambda(lambda x : x[:, 1071:1327])(G_1)\n",
        "x11_2  = Lambda(lambda x : x[:, 1190:1446])(G_1)\n",
        "x12_2  = Lambda(lambda x : x[:, 1309:1565])(G_1)\n",
        "x13_2  = Lambda(lambda x : x[:, 1428:1684])(G_1)\n",
        "x14_2  = Lambda(lambda x : x[:, 1547:1803])(G_1)\n",
        "x15_2  = Lambda(lambda x : x[:, 1666:1922])(G_1)\n",
        "x16_2  = Lambda(lambda x : x[:, 1785:2041])(G_1)\n",
        "\n",
        "x1_2  = tea_0_1(x1_2,1)\n",
        "x2_2  = tea_0_2(x2_2,1)\n",
        "x3_2  = tea_0_3(x3_2,1)\n",
        "x4_2  = tea_0_4(x4_2,1)\n",
        "x5_2  = tea_0_5(x5_2,1)\n",
        "x6_2  = tea_0_6(x6_2,1)\n",
        "x7_2  = tea_0_7(x7_2,1)\n",
        "x8_2  = tea_0_8(x8_2,1)\n",
        "x9_2  = tea_0_9(x9_2,1)\n",
        "x10_2  = tea_0_10(x10_2,1)\n",
        "x11_2  = tea_0_11(x11_2,1)\n",
        "x12_2  = tea_0_12(x12_2,1)\n",
        "x13_2  = tea_0_13(x13_2,1)\n",
        "x14_2  = tea_0_14(x14_2,1)\n",
        "x15_2  = tea_0_15(x15_2,1)\n",
        "x16_2  = tea_0_16(x16_2,1)\n",
        "\n",
        "x1_3  = Lambda(lambda x : x[:,     :256 ])(B_1)\n",
        "x2_3  = Lambda(lambda x : x[:, 119 : 375 ])(B_1)\n",
        "x3_3  = Lambda(lambda x : x[:, 238 :494 ])(B_1)\n",
        "x4_3  = Lambda(lambda x : x[:, 357 : 613])(B_1)\n",
        "x5_3  = Lambda(lambda x : x[:, 476:732])(B_1)\n",
        "x6_3  = Lambda(lambda x : x[:, 595:851])(B_1)\n",
        "x7_3  = Lambda(lambda x : x[:, 714:970])(B_1)\n",
        "x8_3  = Lambda(lambda x : x[:, 833:1089])(B_1)\n",
        "x9_3  = Lambda(lambda x : x[:, 952:1208])(B_1)\n",
        "x10_3  = Lambda(lambda x : x[:, 1071:1327])(B_1)\n",
        "x11_3  = Lambda(lambda x : x[:, 1190:1446])(B_1)\n",
        "x12_3  = Lambda(lambda x : x[:, 1309:1565])(B_1)\n",
        "x13_3  = Lambda(lambda x : x[:, 1428:1684])(B_1)\n",
        "x14_3  = Lambda(lambda x : x[:, 1547:1803])(B_1)\n",
        "x15_3  = Lambda(lambda x : x[:, 1666:1922])(B_1)\n",
        "x16_3  = Lambda(lambda x : x[:, 1785:2041])(B_1)\n",
        "\n",
        "x1_3  = tea_0_1(x1_3,2)\n",
        "x2_3  = tea_0_2(x2_3,2)\n",
        "x3_3  = tea_0_3(x3_3,2)\n",
        "x4_3  = tea_0_4(x4_3,2)\n",
        "x5_3  = tea_0_5(x5_3,2)\n",
        "x6_3  = tea_0_6(x6_3,2)\n",
        "x7_3  = tea_0_7(x7_3,2)\n",
        "x8_3  = tea_0_8(x8_3,2)\n",
        "x9_3  = tea_0_9(x9_3,2)\n",
        "x10_3  = tea_0_10(x10_3,2)\n",
        "x11_3  = tea_0_11(x11_3,2)\n",
        "x12_3  = tea_0_12(x12_3,2)\n",
        "x13_3  = tea_0_13(x13_3,2)\n",
        "x14_3  = tea_0_14(x14_3,2)\n",
        "x15_3  = tea_0_15(x15_3,2)\n",
        "x16_3  = tea_0_16(x16_3,2)\n",
        "\n",
        "### 2 ###\n",
        "\n",
        "x1_4  = Lambda(lambda x : x[:,     :256 ])(R_2)\n",
        "x2_4  = Lambda(lambda x : x[:, 119 : 375 ])(R_2)\n",
        "x3_4  = Lambda(lambda x : x[:, 238 :494 ])(R_2)\n",
        "x4_4  = Lambda(lambda x : x[:, 357 : 613])(R_2)\n",
        "x5_4  = Lambda(lambda x : x[:, 476:732])(R_2)\n",
        "x6_4  = Lambda(lambda x : x[:, 595:851])(R_2)\n",
        "x7_4  = Lambda(lambda x : x[:, 714:970])(R_2)\n",
        "x8_4  = Lambda(lambda x : x[:, 833:1089])(R_2)\n",
        "x9_4  = Lambda(lambda x : x[:, 952:1208])(R_2)\n",
        "x10_4  = Lambda(lambda x : x[:, 1071:1327])(R_2)\n",
        "x11_4  = Lambda(lambda x : x[:, 1190:1446])(R_2)\n",
        "x12_4  = Lambda(lambda x : x[:, 1309:1565])(R_2)\n",
        "x13_4  = Lambda(lambda x : x[:, 1428:1684])(R_2)\n",
        "x14_4  = Lambda(lambda x : x[:, 1547:1803])(R_2)\n",
        "x15_4  = Lambda(lambda x : x[:, 1666:1922])(R_2)\n",
        "x16_4  = Lambda(lambda x : x[:, 1785:2041])(R_2)\n",
        "\n",
        "x1_4  = tea_0_1(x1_4,3)\n",
        "x2_4  = tea_0_2(x2_4,3)\n",
        "x3_4  = tea_0_3(x3_4,3)\n",
        "x4_4  = tea_0_4(x4_4,3)\n",
        "x5_4  = tea_0_5(x5_4,3)\n",
        "x6_4  = tea_0_6(x6_4,3)\n",
        "x7_4  = tea_0_7(x7_4,3)\n",
        "x8_4  = tea_0_8(x8_4,3)\n",
        "x9_4  = tea_0_9(x9_4,3)\n",
        "x10_4  = tea_0_10(x10_4,3)\n",
        "x11_4  = tea_0_11(x11_4,3)\n",
        "x12_4  = tea_0_12(x12_4,3)\n",
        "x13_4  = tea_0_13(x13_4,3)\n",
        "x14_4  = tea_0_14(x14_4,3)\n",
        "x15_4  = tea_0_15(x15_4,3)\n",
        "x16_4  = tea_0_16(x16_4,3)\n",
        "\n",
        "x1_5  = Lambda(lambda x : x[:,     :256 ])(G_2)\n",
        "x2_5  = Lambda(lambda x : x[:, 119 : 375 ])(G_2)\n",
        "x3_5  = Lambda(lambda x : x[:, 238 :494 ])(G_2)\n",
        "x4_5  = Lambda(lambda x : x[:, 357 : 613])(G_2)\n",
        "x5_5  = Lambda(lambda x : x[:, 476:732])(G_2)\n",
        "x6_5  = Lambda(lambda x : x[:, 595:851])(G_2)\n",
        "x7_5  = Lambda(lambda x : x[:, 714:970])(G_2)\n",
        "x8_5  = Lambda(lambda x : x[:, 833:1089])(G_2)\n",
        "x9_5  = Lambda(lambda x : x[:, 952:1208])(G_2)\n",
        "x10_5  = Lambda(lambda x : x[:, 1071:1327])(G_2)\n",
        "x11_5  = Lambda(lambda x : x[:, 1190:1446])(G_2)\n",
        "x12_5  = Lambda(lambda x : x[:, 1309:1565])(G_2)\n",
        "x13_5  = Lambda(lambda x : x[:, 1428:1684])(G_2)\n",
        "x14_5  = Lambda(lambda x : x[:, 1547:1803])(G_2)\n",
        "x15_5  = Lambda(lambda x : x[:, 1666:1922])(G_2)\n",
        "x16_5  = Lambda(lambda x : x[:, 1785:2041])(G_2)\n",
        "\n",
        "x1_5  = tea_0_1(x1_5,4)\n",
        "x2_5  = tea_0_2(x2_5,4)\n",
        "x3_5  = tea_0_3(x3_5,4)\n",
        "x4_5  = tea_0_4(x4_5,4)\n",
        "x5_5  = tea_0_5(x5_5,4)\n",
        "x6_5  = tea_0_6(x6_5,4)\n",
        "x7_5  = tea_0_7(x7_5,4)\n",
        "x8_5  = tea_0_8(x8_5,4)\n",
        "x9_5  = tea_0_9(x9_5,4)\n",
        "x10_5  = tea_0_10(x10_5,4)\n",
        "x11_5  = tea_0_11(x11_5,4)\n",
        "x12_5  = tea_0_12(x12_5,4)\n",
        "x13_5  = tea_0_13(x13_5,4)\n",
        "x14_5  = tea_0_14(x14_5,4)\n",
        "x15_5  = tea_0_15(x15_5,4)\n",
        "x16_5  = tea_0_16(x16_5,4)\n",
        "\n",
        "x1_6  = Lambda(lambda x : x[:,     :256 ])(B_2)\n",
        "x2_6  = Lambda(lambda x : x[:, 119 : 375 ])(B_2)\n",
        "x3_6  = Lambda(lambda x : x[:, 238 :494 ])(B_2)\n",
        "x4_6  = Lambda(lambda x : x[:, 357 : 613])(B_2)\n",
        "x5_6  = Lambda(lambda x : x[:, 476:732])(B_2)\n",
        "x6_6  = Lambda(lambda x : x[:, 595:851])(B_2)\n",
        "x7_6  = Lambda(lambda x : x[:, 714:970])(B_2)\n",
        "x8_6  = Lambda(lambda x : x[:, 833:1089])(B_2)\n",
        "x9_6  = Lambda(lambda x : x[:, 952:1208])(B_2)\n",
        "x10_6  = Lambda(lambda x : x[:, 1071:1327])(B_2)\n",
        "x11_6  = Lambda(lambda x : x[:, 1190:1446])(B_2)\n",
        "x12_6  = Lambda(lambda x : x[:, 1309:1565])(B_2)\n",
        "x13_6  = Lambda(lambda x : x[:, 1428:1684])(B_2)\n",
        "x14_6  = Lambda(lambda x : x[:, 1547:1803])(B_2)\n",
        "x15_6  = Lambda(lambda x : x[:, 1666:1922])(B_2)\n",
        "x16_6  = Lambda(lambda x : x[:, 1785:2041])(B_2)\n",
        "\n",
        "x1_6  = tea_0_1(x1_6,5)\n",
        "x2_6  = tea_0_2(x2_6,5)\n",
        "x3_6  = tea_0_3(x3_6,5)\n",
        "x4_6  = tea_0_4(x4_6,5)\n",
        "x5_6  = tea_0_5(x5_6,5)\n",
        "x6_6  = tea_0_6(x6_6,5)\n",
        "x7_6  = tea_0_7(x7_6,5)\n",
        "x8_6  = tea_0_8(x8_6,5)\n",
        "x9_6  = tea_0_9(x9_6,5)\n",
        "x10_6  = tea_0_10(x10_6,5)\n",
        "x11_6  = tea_0_11(x11_6,5)\n",
        "x12_6  = tea_0_12(x12_6,5)\n",
        "x13_6  = tea_0_13(x13_6,5)\n",
        "x14_6  = tea_0_14(x14_6,5)\n",
        "x15_6  = tea_0_15(x15_6,5)\n",
        "x16_6  = tea_0_16(x16_6,5)\n",
        "\n",
        "### 3 ###\n",
        "\n",
        "x1_7  = Lambda(lambda x : x[:,     :256 ])(R_3)\n",
        "x2_7  = Lambda(lambda x : x[:, 119 : 375 ])(R_3)\n",
        "x3_7  = Lambda(lambda x : x[:, 238 :494 ])(R_3)\n",
        "x4_7  = Lambda(lambda x : x[:, 357 : 613])(R_3)\n",
        "x5_7  = Lambda(lambda x : x[:, 476:732])(R_3)\n",
        "x6_7  = Lambda(lambda x : x[:, 595:851])(R_3)\n",
        "x7_7  = Lambda(lambda x : x[:, 714:970])(R_3)\n",
        "x8_7  = Lambda(lambda x : x[:, 833:1089])(R_3)\n",
        "x9_7  = Lambda(lambda x : x[:, 952:1208])(R_3)\n",
        "x10_7  = Lambda(lambda x : x[:, 1071:1327])(R_3)\n",
        "x11_7  = Lambda(lambda x : x[:, 1190:1446])(R_3)\n",
        "x12_7  = Lambda(lambda x : x[:, 1309:1565])(R_3)\n",
        "x13_7  = Lambda(lambda x : x[:, 1428:1684])(R_3)\n",
        "x14_7  = Lambda(lambda x : x[:, 1547:1803])(R_3)\n",
        "x15_7  = Lambda(lambda x : x[:, 1666:1922])(R_3)\n",
        "x16_7  = Lambda(lambda x : x[:, 1785:2041])(R_3)\n",
        "\n",
        "x1_7  = tea_0_1(x1_7,6)\n",
        "x2_7  = tea_0_2(x2_7,6)\n",
        "x3_7  = tea_0_3(x3_7,6)\n",
        "x4_7  = tea_0_4(x4_7,6)\n",
        "x5_7  = tea_0_5(x5_7,6)\n",
        "x6_7  = tea_0_6(x6_7,6)\n",
        "x7_7  = tea_0_7(x7_7,6)\n",
        "x8_7  = tea_0_8(x8_7,6)\n",
        "x9_7  = tea_0_9(x9_7,6)\n",
        "x10_7  = tea_0_10(x10_7,6)\n",
        "x11_7  = tea_0_11(x11_7,6)\n",
        "x12_7  = tea_0_12(x12_7,6)\n",
        "x13_7  = tea_0_13(x13_7,6)\n",
        "x14_7  = tea_0_14(x14_7,6)\n",
        "x15_7  = tea_0_15(x15_7,6)\n",
        "x16_7  = tea_0_16(x16_7,6)\n",
        "\n",
        "x1_8  = Lambda(lambda x : x[:,     :256 ])(G_3)\n",
        "x2_8  = Lambda(lambda x : x[:, 119 : 375 ])(G_3)\n",
        "x3_8  = Lambda(lambda x : x[:, 238 :494 ])(G_3)\n",
        "x4_8  = Lambda(lambda x : x[:, 357 : 613])(G_3)\n",
        "x5_8  = Lambda(lambda x : x[:, 476:732])(G_3)\n",
        "x6_8  = Lambda(lambda x : x[:, 595:851])(G_3)\n",
        "x7_8  = Lambda(lambda x : x[:, 714:970])(G_3)\n",
        "x8_8  = Lambda(lambda x : x[:, 833:1089])(G_3)\n",
        "x9_8  = Lambda(lambda x : x[:, 952:1208])(G_3)\n",
        "x10_8  = Lambda(lambda x : x[:, 1071:1327])(G_3)\n",
        "x11_8  = Lambda(lambda x : x[:, 1190:1446])(G_3)\n",
        "x12_8  = Lambda(lambda x : x[:, 1309:1565])(G_3)\n",
        "x13_8  = Lambda(lambda x : x[:, 1428:1684])(G_3)\n",
        "x14_8  = Lambda(lambda x : x[:, 1547:1803])(G_3)\n",
        "x15_8  = Lambda(lambda x : x[:, 1666:1922])(G_3)\n",
        "x16_8  = Lambda(lambda x : x[:, 1785:2041])(G_3)\n",
        "\n",
        "x1_8  = tea_0_1(x1_8,7)\n",
        "x2_8  = tea_0_2(x2_8,7)\n",
        "x3_8  = tea_0_3(x3_8,7)\n",
        "x4_8  = tea_0_4(x4_8,7)\n",
        "x5_8  = tea_0_5(x5_8,7)\n",
        "x6_8  = tea_0_6(x6_8,7)\n",
        "x7_8  = tea_0_7(x7_8,7)\n",
        "x8_8  = tea_0_8(x8_8,7)\n",
        "x9_8  = tea_0_9(x9_8,7)\n",
        "x10_8  = tea_0_10(x10_8,7)\n",
        "x11_8  = tea_0_11(x11_8,7)\n",
        "x12_8  = tea_0_12(x12_8,7)\n",
        "x13_8  = tea_0_13(x13_8,7)\n",
        "x14_8  = tea_0_14(x14_8,7)\n",
        "x15_8  = tea_0_15(x15_8,7)\n",
        "x16_8  = tea_0_16(x16_8,7)\n",
        "\n",
        "x1_9  = Lambda(lambda x : x[:,     :256 ])(B_3)\n",
        "x2_9  = Lambda(lambda x : x[:, 119 : 375 ])(B_3)\n",
        "x3_9  = Lambda(lambda x : x[:, 238 :494 ])(B_3)\n",
        "x4_9  = Lambda(lambda x : x[:, 357 : 613])(B_3)\n",
        "x5_9  = Lambda(lambda x : x[:, 476:732])(B_3)\n",
        "x6_9  = Lambda(lambda x : x[:, 595:851])(B_3)\n",
        "x7_9  = Lambda(lambda x : x[:, 714:970])(B_3)\n",
        "x8_9  = Lambda(lambda x : x[:, 833:1089])(B_3)\n",
        "x9_9  = Lambda(lambda x : x[:, 952:1208])(B_3)\n",
        "x10_9  = Lambda(lambda x : x[:, 1071:1327])(B_3)\n",
        "x11_9  = Lambda(lambda x : x[:, 1190:1446])(B_3)\n",
        "x12_9  = Lambda(lambda x : x[:, 1309:1565])(B_3)\n",
        "x13_9  = Lambda(lambda x : x[:, 1428:1684])(B_3)\n",
        "x14_9  = Lambda(lambda x : x[:, 1547:1803])(B_3)\n",
        "x15_9  = Lambda(lambda x : x[:, 1666:1922])(B_3)\n",
        "x16_9  = Lambda(lambda x : x[:, 1785:2041])(B_3)\n",
        "\n",
        "x1_9  = tea_0_1(x1_9,8)\n",
        "x2_9  = tea_0_2(x2_9,8)\n",
        "x3_9  = tea_0_3(x3_9,8)\n",
        "x4_9  = tea_0_4(x4_9,8)\n",
        "x5_9  = tea_0_5(x5_9,8)\n",
        "x6_9  = tea_0_6(x6_9,8)\n",
        "x7_9  = tea_0_7(x7_9,8)\n",
        "x8_9  = tea_0_8(x8_9,8)\n",
        "x9_9  = tea_0_9(x9_9,8)\n",
        "x10_9  = tea_0_10(x10_9,8)\n",
        "x11_9  = tea_0_11(x11_9,8)\n",
        "x12_9  = tea_0_12(x12_9,8)\n",
        "x13_9  = tea_0_13(x13_9,8)\n",
        "x14_9  = tea_0_14(x14_9,8)\n",
        "x15_9  = tea_0_15(x15_9,8)\n",
        "x16_9  = tea_0_16(x16_9,8)\n",
        "#Average 1->5\n",
        "x1_1_1 = Average()([x1_1,x1_2,x1_3,x1_4,x1_5])\n",
        "x2_1_1 = Average()([x2_1,x2_2,x2_3,x2_4,x2_5])\n",
        "x3_1_1 = Average()([x3_1,x3_2,x3_3,x3_4,x3_5])\n",
        "x4_1_1 = Average()([x4_1,x4_2,x4_3,x4_4,x4_5])\n",
        "x5_1_1 = Average()([x5_1,x5_2,x5_3,x5_4,x5_5])\n",
        "x6_1_1 = Average()([x6_1,x6_2,x6_3,x6_4,x6_5])\n",
        "x7_1_1 = Average()([x7_1,x7_2,x7_3,x7_4,x7_5])\n",
        "x8_1_1 = Average()([x8_1,x8_2,x8_3,x8_4,x8_5])\n",
        "x9_1_1 = Average()([x9_1,x9_2,x9_3,x9_4,x9_5])\n",
        "x10_1_1 = Average()([x10_1,x10_2,x10_3,x10_4,x10_5])\n",
        "x11_1_1 = Average()([x11_1,x11_2,x11_3,x11_4,x11_5])\n",
        "x12_1_1 = Average()([x12_1,x12_2,x12_3,x12_4,x12_5])\n",
        "x13_1_1 = Average()([x13_1,x13_2,x13_3,x13_4,x13_5])\n",
        "x14_1_1 = Average()([x14_1,x14_2,x14_3,x14_4,x14_5])\n",
        "x15_1_1 = Average()([x15_1,x15_2,x15_3,x15_4,x15_5])\n",
        "x16_1_1 = Average()([x16_1,x16_2,x16_3,x16_4,x16_5])\n",
        "\n",
        "#Average image 2->6\n",
        "\n",
        "x1_1_2 = Average()([x1_2,x1_3,x1_4,x1_5,x1_6])\n",
        "x2_1_2 = Average()([x2_2,x2_3,x2_4,x2_5,x2_6])\n",
        "x3_1_2 = Average()([x3_2,x3_3,x3_4,x3_5,x3_6])\n",
        "x4_1_2 = Average()([x4_2,x4_3,x4_4,x4_5,x4_6])\n",
        "x5_1_2 = Average()([x5_2,x5_3,x5_4,x5_5,x5_6])\n",
        "x6_1_2 = Average()([x6_2,x6_3,x6_4,x6_5,x6_6])\n",
        "x7_1_2 = Average()([x7_2,x7_3,x7_4,x7_5,x7_6])\n",
        "x8_1_2 = Average()([x8_2,x8_3,x8_4,x8_5,x8_6])\n",
        "x9_1_2 = Average()([x9_2,x9_3,x9_4,x9_5,x9_6])\n",
        "x10_1_2 = Average()([x10_2,x10_3,x10_4,x10_5,x10_6])\n",
        "x11_1_2 = Average()([x11_2,x11_3,x11_4,x11_5,x11_6])\n",
        "x12_1_2 = Average()([x12_2,x12_3,x12_4,x12_5,x12_6])\n",
        "x13_1_2 = Average()([x13_2,x13_3,x13_4,x13_5,x13_6])\n",
        "x14_1_2 = Average()([x14_2,x14_3,x14_4,x14_5,x14_6])\n",
        "x15_1_2 = Average()([x15_2,x15_3,x15_4,x15_5,x15_6])\n",
        "x16_1_2 = Average()([x16_2,x16_3,x16_4,x16_5,x16_6])\n",
        "\n",
        "#Average image 3->7\n",
        "\n",
        "x1_1_3 = Average()([x1_3,x1_4,x1_5,x1_6,x1_7])\n",
        "x2_1_3 = Average()([x2_3,x2_4,x2_5,x2_6,x2_7])\n",
        "x3_1_3 = Average()([x3_3,x3_4,x3_5,x3_6,x3_7])\n",
        "x4_1_3 = Average()([x4_3,x4_4,x4_5,x4_6,x4_7])\n",
        "x5_1_3 = Average()([x5_3,x5_4,x5_5,x5_6,x5_7])\n",
        "x6_1_3 = Average()([x6_3,x6_4,x6_5,x6_6,x6_7])\n",
        "x7_1_3 = Average()([x7_3,x7_4,x7_5,x7_6,x7_7])\n",
        "x8_1_3 = Average()([x8_3,x8_4,x8_5,x8_6,x8_7])\n",
        "x9_1_3 = Average()([x9_3,x9_4,x9_5,x9_6,x9_7])\n",
        "x10_1_3 = Average()([x10_3,x10_4,x10_5,x10_6,x10_7])\n",
        "x11_1_3 = Average()([x11_3,x11_4,x11_5,x11_6,x11_7])\n",
        "x12_1_3 = Average()([x12_3,x12_4,x12_5,x12_6,x12_7])\n",
        "x13_1_3 = Average()([x13_3,x13_4,x13_5,x13_6,x13_7])\n",
        "x14_1_3 = Average()([x14_3,x14_4,x14_5,x14_6,x14_7])\n",
        "x15_1_3 = Average()([x15_3,x15_4,x15_5,x15_6,x15_7])\n",
        "x16_1_3 = Average()([x16_3,x16_4,x16_5,x16_6,x16_7])\n",
        "\n",
        "#Average image 4->8\n",
        "\n",
        "x1_1_4 = Average()([x1_4,x1_5,x1_6,x1_7,x1_8])\n",
        "x2_1_4 = Average()([x2_4,x2_5,x2_6,x2_7,x2_8])\n",
        "x3_1_4 = Average()([x3_4,x3_5,x3_6,x3_7,x3_8])\n",
        "x4_1_4 = Average()([x4_4,x4_5,x4_6,x4_7,x4_8])\n",
        "x5_1_4 = Average()([x5_4,x5_5,x5_6,x5_7,x5_8])\n",
        "x6_1_4 = Average()([x6_4,x6_5,x6_6,x6_7,x6_8])\n",
        "x7_1_4 = Average()([x7_4,x7_5,x7_6,x7_7,x7_8])\n",
        "x8_1_4 = Average()([x8_4,x8_5,x8_6,x8_7,x8_8])\n",
        "x9_1_4 = Average()([x9_4,x9_5,x9_6,x9_7,x9_8])\n",
        "x10_1_4 = Average()([x10_4,x10_5,x10_6,x10_7,x10_8])\n",
        "x11_1_4 = Average()([x11_4,x11_5,x11_6,x11_7,x11_8])\n",
        "x12_1_4 = Average()([x12_4,x12_5,x12_6,x12_7,x12_8])\n",
        "x13_1_4 = Average()([x13_4,x13_5,x13_6,x13_7,x13_8])\n",
        "x14_1_4 = Average()([x14_4,x14_5,x14_6,x14_7,x14_8])\n",
        "x15_1_4 = Average()([x15_4,x15_5,x15_6,x15_7,x15_8])\n",
        "x16_1_4 = Average()([x16_4,x16_5,x16_6,x16_7,x16_8])\n",
        "\n",
        "#Average image 5->9\n",
        "\n",
        "x1_1_5 = Average()([x1_5,x1_6,x1_7,x1_8,x1_9])\n",
        "x2_1_5 = Average()([x2_5,x2_6,x2_7,x2_8,x2_9])\n",
        "x3_1_5 = Average()([x3_5,x3_6,x3_7,x3_8,x3_9])\n",
        "x4_1_5 = Average()([x4_5,x4_6,x4_7,x4_8,x4_9])\n",
        "x5_1_5 = Average()([x5_5,x5_6,x5_7,x5_8,x5_9])\n",
        "x6_1_5 = Average()([x6_5,x6_6,x6_7,x6_8,x6_9])\n",
        "x7_1_5 = Average()([x7_5,x7_6,x7_7,x7_8,x7_9])\n",
        "x8_1_5 = Average()([x8_5,x8_6,x8_7,x8_8,x8_9])\n",
        "x9_1_5 = Average()([x9_5,x9_6,x9_7,x9_6,x9_9])\n",
        "x10_1_5 = Average()([x10_5,x10_6,x10_7,x10_8,x10_9])\n",
        "x11_1_5 = Average()([x11_5,x11_6,x11_7,x11_8,x11_9])\n",
        "x12_1_5 = Average()([x12_5,x12_6,x12_7,x12_8,x12_9])\n",
        "x13_1_5 = Average()([x13_5,x13_6,x13_7,x13_8,x13_9])\n",
        "x14_1_5 = Average()([x14_5,x14_6,x14_7,x14_8,x14_9])\n",
        "x15_1_5 = Average()([x15_5,x15_6,x15_7,x15_8,x15_9])\n",
        "x16_1_5 = Average()([x16_5,x16_6,x16_7,x16_8,x16_9])\n",
        "\n",
        "\n",
        "x1_1 = concatenate(([x1_1_1,x2_1_1,x3_1_1,x4_1_1]),axis=1)\n",
        "x2_1 = concatenate(([x5_1_1,x6_1_1,x7_1_1,x8_1_1]),axis=1)\n",
        "x3_1 = concatenate(([x9_1_1,x10_1_1,x11_1_1,x12_1_1]),axis=1)\n",
        "x4_1 = concatenate(([x13_1_1,x14_1_1,x15_1_1,x16_1_1]),axis=1)\n",
        "\n",
        "x1_2 = concatenate(([x1_1_2,x2_1_2,x3_1_2,x4_1_2]),axis=1)\n",
        "x2_2 = concatenate(([x5_1_2,x6_1_2,x7_1_2,x8_1_2]),axis=1)\n",
        "x3_2 = concatenate(([x9_1_2,x10_1_2,x11_1_2,x12_1_2]),axis=1)\n",
        "x4_2 = concatenate(([x13_1_2,x14_1_2,x15_1_2,x16_1_2]),axis=1)\n",
        "\n",
        "x1_3 = concatenate(([x1_1_3,x2_1_3,x3_1_3,x4_1_3]),axis=1)\n",
        "x2_3 = concatenate(([x5_1_3,x6_1_3,x7_1_3,x8_1_3]),axis=1)\n",
        "x3_3 = concatenate(([x9_1_3,x10_1_3,x11_1_3,x12_1_3]),axis=1)\n",
        "x4_3 = concatenate(([x13_1_3,x14_1_3,x15_1_3,x16_1_3]),axis=1)\n",
        "\n",
        "x1_4 = concatenate(([x1_1_4,x2_1_4,x3_1_4,x4_1_4]),axis=1)\n",
        "x2_4 = concatenate(([x5_1_4,x6_1_4,x7_1_4,x8_1_4]),axis=1)\n",
        "x3_4 = concatenate(([x9_1_4,x10_1_4,x11_1_4,x12_1_4]),axis=1)\n",
        "x4_4 = concatenate(([x13_1_4,x14_1_4,x15_1_4,x16_1_4]),axis=1)\n",
        "\n",
        "x1_5 = concatenate(([x1_1_5,x2_1_5,x3_1_5,x4_1_5]),axis=1)\n",
        "x2_5 = concatenate(([x5_1_5,x6_1_5,x7_1_5,x8_1_5]),axis=1)\n",
        "x3_5 = concatenate(([x9_1_5,x10_1_5,x11_1_5,x12_1_5]),axis=1)\n",
        "x4_5 = concatenate(([x13_1_5,x14_1_5,x15_1_5,x16_1_5]),axis=1)\n",
        "\n",
        "x1_1 = tea_1_1(x1_1,0)\n",
        "x2_1 = tea_1_2(x2_1,0)\n",
        "x3_1 = tea_1_3(x3_1,0)\n",
        "x4_1 = tea_1_4(x4_1,0)\n",
        "\n",
        "x1_2 = tea_1_1(x1_2,1)\n",
        "x2_2 = tea_1_2(x2_2,1)\n",
        "x3_2 = tea_1_3(x3_2,1)\n",
        "x4_2 = tea_1_4(x4_2,1)\n",
        "\n",
        "x1_3 = tea_1_1(x1_3,2)\n",
        "x2_3 = tea_1_2(x2_3,2)\n",
        "x3_3 = tea_1_3(x3_3,2)\n",
        "x4_3 = tea_1_4(x4_3,2)\n",
        "\n",
        "x1_4 = tea_1_1(x1_4,3)\n",
        "x2_4 = tea_1_2(x2_4,3)\n",
        "x3_4 = tea_1_3(x3_4,3)\n",
        "x4_4 = tea_1_4(x4_4,3)\n",
        "\n",
        "x1_5 = tea_1_1(x1_5,4)\n",
        "x2_5 = tea_1_2(x2_5,4)\n",
        "x3_5 = tea_1_3(x3_5,4)\n",
        "x4_5 = tea_1_4(x4_5,4)\n",
        "#Average after layer 2\n",
        "x_out_1 = Average()([x1_1,x1_2,x1_3,x1_4,x1_5])\n",
        "x_out_2 = Average()([x2_1,x2_2,x2_3,x2_4,x2_5])\n",
        "x_out_3 = Average()([x3_1,x3_2,x3_3,x3_4,x3_5])\n",
        "x_out_4 = Average()([x4_1,x4_2,x4_3,x4_4,x4_5])\n",
        "\n",
        "# Layer 3\n",
        "\n",
        "x_out = concatenate(([x_out_1,x_out_2,x_out_3,x_out_4]),axis=1)\n",
        "\n",
        "x_out = tea_2_1(x_out) # 252 divided by 9\n",
        "\n",
        "x_out = AdditivePooling(9)(x_out)\n",
        "\n",
        "predictions = Activation('softmax')(x_out)\n",
        "#Model\n",
        "\n",
        "model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=0.0001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=128,\n",
        "          epochs=50,\n",
        "          verbose=1,\n",
        "          validation_split=0.0)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1nbZd2c7XssQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "test_serial.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPe8fftjXJ2Gv5XX1Iabmnc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}