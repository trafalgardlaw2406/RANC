{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trafalgardlaw2406/RANC/blob/main/RANC_21cores_3classes_serial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvKwA8axKYVU",
        "outputId": "5176f351-3136-46ef-846a-1d486bf83d7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WS_6zX6haERo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74cd8365-4432-44b9-fe61-cefe2f145776"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import operator\n",
        "import functools\n",
        "import math\n",
        "import os\n",
        "\n",
        "from scipy import ndimage\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Layer, InputSpec\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Activation, Input, Lambda, concatenate,Average\n",
        "from tensorflow.keras.datasets import mnist,fashion_mnist\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import constraints\n",
        "import sys\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PsaOHIMcOd_4"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/RANC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rC6EvC0XOpSY"
      },
      "outputs": [],
      "source": [
        "# Data Load\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# PyTorch (modeling)\n",
        "# import torch\n",
        "# from torch import nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data.sampler import SubsetRandomSampler\n",
        "# from torchvision import transforms\n",
        "# import torchvision.transforms.functional as TF\n",
        "# from torch.utils.data import Dataset\n",
        "# from torch.utils.data import random_split\n",
        "# from torch.utils.data import DataLoader\n",
        "\n",
        "# # Visualization\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "import sys\n",
        "from scipy import ndimage\n",
        "\n",
        "# These functions are introduced along the Part 1 notebook.\n",
        "\n",
        "# Position vectors. We load the data with respect to the file name, which is\n",
        "# a number corresponding to a specific in-bed position. We take advantage of this\n",
        "# and use the number to get the position with help of the following vectors.\n",
        "\n",
        "positions_i = [\"justAPlaceholder\", \"supine_1\", \"right_0\",\n",
        "               \"left_0\", \"right_30\", \"right_60\",\n",
        "               \"left_30\", \"left_60\", \"supine_2\",\n",
        "               \"supine_3\", \"supine_4\", \"supine_5\",\n",
        "               \"supine_6\", \"right_fetus\", \"left_fetus\",\n",
        "               \"supine_30\", \"supine_45\", \"supine_60\"]\n",
        "\n",
        "positions_i_short = [\"justAPlaceholder\", \"supine\", \"right\",\n",
        "               \"left\", \"right\", \"right\",\n",
        "               \"left\", \"left\", \"supine\",\n",
        "               \"supine\", \"supine\", \"supine\",\n",
        "               \"supine\", \"right\", \"left\",\n",
        "               \"supine\", \"supine\", \"supine\"]\n",
        "\n",
        "positions_ii = {\n",
        "    \"B\":\"supine\", \"1\":\"supine\", \"C\":\"right\",\n",
        "    \"D\":\"left\", \"E1\":\"right\", \"E2\":\"right\",\n",
        "    \"E3\":\"left\", \"E4\":\"left\", \"E5\":\"right\",\n",
        "    \"E6\":\"left\", \"F\":\"supine\", \"G1\":\"supine\",\n",
        "    \"G2\":\"right\", \"G3\":\"left\"\n",
        "}\n",
        "\n",
        "class_positions = ['supine', 'left', 'right', 'left_fetus', 'right_fetus']\n",
        "\n",
        "# We also want the classes to be encoded as numbers so we can work easier when\n",
        "# modeling. This function achieves so. Since left_fetus and right_fetus are not\n",
        "# considered as classes in the evaluation of the original paper and since they\n",
        "# are not considered in the \"Experiment I\", we encode them also as left and right\n",
        "# positions.\n",
        "\n",
        "def token_position_short(x):\n",
        "  return {\n",
        "      'supine': 0,\n",
        "      'left': 1,\n",
        "      'right': 2,\n",
        "      'left_fetus': 1,\n",
        "      'right_fetus': 2\n",
        "  }[x]\n",
        "\n",
        "def token_position(x):\n",
        "  return {\n",
        "      \"supine_1\":0, \n",
        "      \"right_0\":1,\n",
        "      \"left_0\":2, \n",
        "      \"right_30\":3, \n",
        "      \"right_60\":4,\n",
        "      \"left_30\":5, \n",
        "      \"left_60\":6, \n",
        "      \"supine_2\":7,\n",
        "      \"supine_3\":8, \n",
        "      \"supine_4\":9, \n",
        "      \"supine_5\":10,\n",
        "      \"supine_6\":11, \n",
        "      \"right_fetus\":12, \n",
        "      \"left_fetus\":13,\n",
        "      \"supine_30\":14, \n",
        "      \"supine_45\":15, \n",
        "      \"supine_60\":16\n",
        "  }[x]\n",
        "\n",
        "def token_position_new(x):\n",
        "  return {\n",
        "      \"supine_1\":0, \n",
        "      \"supine_2\":1,\n",
        "      \"supine_3\":2, \n",
        "      \"supine_4\":3, \n",
        "      \"supine_5\":4,\n",
        "      \"supine_6\":5, \n",
        "      \"supine_30\":6, \n",
        "      \"supine_45\":7, \n",
        "      \"supine_60\":8, \n",
        "      \"left_0\":9, \n",
        "      \"left_30\":10, \n",
        "      \"left_60\":11,\n",
        "      \"left_fetus\":12, \n",
        "      \"right_0\":13,\n",
        "      \"right_30\":14, \n",
        "      \"right_60\":15,\n",
        "      \"right_fetus\":16, \n",
        "  }[x]\n",
        "list_supine = [\"1.txt\",\"8.txt\",\"9.txt\",\"10.txt\",\"11.txt\",\"12.txt\",\"15.txt\",\"16.txt\",\"17.txt\"]\n",
        "\n",
        "list_supine_norm_1 = [\"1.txt\",\"8.txt\",\"9.txt\"]\n",
        "\n",
        "list_supine_norm_2 = [\"10.txt\",\"11.txt\",\"12.txt\"]\n",
        "\n",
        "list_supine_incl = [\"15.txt\",\"16.txt\",\"17.txt\"]\n",
        "\n",
        "list_left = [\"3.txt\",\"6.txt\",\"7.txt\",\"14.txt\"]\n",
        "\n",
        "list_right = [\"2.txt\",\"4.txt\",\"5.txt\",\"13.txt\"]\n",
        "\n",
        "def token_position_supine(x):\n",
        "  return {\n",
        "      \"supine_1\":0, \n",
        "      \"supine_2\":1,\n",
        "      \"supine_3\":2, \n",
        "      \"supine_4\":3, \n",
        "      \"supine_5\":4,\n",
        "      \"supine_6\":5, \n",
        "      \"supine_30\":6, \n",
        "      \"supine_45\":7, \n",
        "      \"supine_60\":8\n",
        "    }[x]\n",
        "\n",
        "def token_position_supine_norm_1(x):\n",
        "  return {\n",
        "      \"supine_1\":0, \n",
        "      \"supine_2\":1,\n",
        "      \"supine_3\":2, \n",
        "    }[x]\n",
        "\n",
        "def token_position_supine_norm_2(x):\n",
        "  return {\n",
        "      \"supine_4\":0, \n",
        "      \"supine_5\":1,\n",
        "      \"supine_6\":2, \n",
        "    }[x]\n",
        "\n",
        "def token_position_supine_incl(x):\n",
        "  return {\n",
        "      \"supine_30\":0, \n",
        "      \"supine_45\":1, \n",
        "      \"supine_60\":2\n",
        "    }[x]\n",
        "\n",
        "def token_position_left(x):\n",
        "  return {\n",
        "      \"left_0\":0, \n",
        "      \"left_30\":1, \n",
        "      \"left_60\":2,\n",
        "      \"left_fetus\":3,\n",
        "  }[x]\n",
        "\n",
        "def token_position_right(x):\n",
        "  return {\n",
        "      \"right_0\":0,\n",
        "      \"right_30\":1, \n",
        "      \"right_60\":2,\n",
        "      \"right_fetus\":3, \n",
        "  }[x]\n",
        "\n",
        "\n",
        "def load_exp_i(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      max_val = []\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(3, len(lines) - 3):\n",
        "                              \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                \n",
        "                # Spatio-temporal median filter 3x3x3\n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                past_image = ndimage.median_filter(past_image, 3)\n",
        "                future_image = ndimage.median_filter(future_image, 3)\n",
        "                raw_data = np.concatenate((raw_data[np.newaxis, :, :], past_image[np.newaxis, :, :], future_image[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "            \n",
        "            # with open(file_path, 'r') as f:\n",
        "            #   # Start from second recording, as the first two are corrupted\n",
        "            #   lines = f.read().splitlines()[2:]\n",
        "            #   for line in f.read().splitlines()[2:]:\n",
        "            #     # print(line)\n",
        "            #     raw_data = np.fromstring(line, dtype=float, sep='\\t')\n",
        "                # Change the range from [0-1000] to [0-255].\n",
        "                  # max_val.append(np.amax(raw_data))\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              # file_data = np.round(raw_data).astype(np.uint8)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "              # print(positions_i[int(file[:-4])])\n",
        "              file_label = token_position(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "      \n",
        "      # max_over_all = max(max_val)\n",
        "      # print(max_over_all)\n",
        "\n",
        "      # data = np.round(data * 255/1000).astype(np.uint8)\n",
        "      dataset[subject] = (data, labels)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_short(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      max_val = []\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(3, len(lines) - 3):\n",
        "                              \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                \n",
        "                # Spatio-temporal median filter 3x3x3\n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                past_image = ndimage.median_filter(past_image, 3)\n",
        "                future_image = ndimage.median_filter(future_image, 3)\n",
        "                raw_data = np.concatenate((raw_data[np.newaxis, :, :], past_image[np.newaxis, :, :], future_image[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              # file_data = np.round(raw_data).astype(np.uint8)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "              # print(positions_i[int(file[:-4])])\n",
        "              file_label = token_position_short(positions_i_short[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_supine(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_supine\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(5, len(lines) - 5):\n",
        "                            \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image_1 = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_1 = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                past_image_2 = np.fromstring(lines[i-2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_2 = np.fromstring(lines[i+2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "                # Spatio-temporal median filter 5x5x5\n",
        "              \n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                \n",
        "                past_image_1 = ndimage.median_filter(past_image_1, 3)\n",
        "                future_image_1 = ndimage.median_filter(future_image_1, 3)\n",
        "                past_image_2 = ndimage.median_filter(past_image_2, 3)\n",
        "                future_image_2 = ndimage.median_filter(future_image_2, 3)\n",
        "\n",
        "                raw_data = np.concatenate((past_image_2[np.newaxis, :, :],past_image_1[np.newaxis, :, :] ,raw_data[np.newaxis, :, :], \\\n",
        "                future_image_1[np.newaxis, :, :],future_image_2[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "              \n",
        "              # a=np.amax(raw_data)\n",
        "\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              \n",
        "              # file_data = np.round(raw_data).astype(np.uint8)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "\n",
        "              file_label = token_position_supine(positions_i[int(file[:-4])])\n",
        "              \n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_supine_norm_1(path):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_supine_norm_1\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            for line in f.read().splitlines()[2:]:\n",
        "              # print(line)\n",
        "              raw_data = np.fromstring(line, dtype=float, sep='\\t')\n",
        "              # Change the range from [0-1000] to [0-255].\n",
        "              max_val = np.amax(raw_data)\n",
        "              file_data = np.round(raw_data*255/max_val).astype(np.uint8)\n",
        "              \n",
        "              # file_data = np.round(raw_data).astype(float)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "\n",
        "              file_label = token_position_supine_norm_1(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_supine_norm_2(path):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_supine_norm_2\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            for line in f.read().splitlines()[2:]:\n",
        "              # print(line)\n",
        "              raw_data = np.fromstring(line, dtype=float, sep='\\t')\n",
        "              # Change the range from [0-1000] to [0-255].\n",
        "              max_val = np.amax(raw_data)\n",
        "              file_data = np.round(raw_data*255/max_val).astype(np.uint8)\n",
        "              \n",
        "              # file_data = np.round(raw_data).astype(float)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "\n",
        "              file_label = token_position_supine_norm_2(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_supine_incl(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_supine_incl\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            # with open(file_path, 'r') as f:\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(3, len(lines) - 3):\n",
        "\n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                \n",
        "                # Spatio-temporal median filter 3x3x3\n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                past_image = ndimage.median_filter(past_image, 3)\n",
        "                future_image = ndimage.median_filter(future_image, 3)\n",
        "                raw_data = np.concatenate((raw_data[np.newaxis, :, :], past_image[np.newaxis, :, :], future_image[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "\n",
        "              # Change the range from [0-1000] to [0-255].\n",
        "              # max_vol = np.amax(raw_data)\n",
        "              file_data = np.round(raw_data ).astype(np.uint8)\n",
        "\n",
        "              # file_data = np.round(raw_data).astype(np.uint8)\n",
        "              file_data = file_data.reshape(1, 64, 32)\n",
        "\n",
        "              file_label = token_position_supine_incl(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_left(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_left\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(5, len(lines) - 5):\n",
        "                            \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image_1 = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_1 = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                past_image_2 = np.fromstring(lines[i-2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_2 = np.fromstring(lines[i+2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "                # Spatio-temporal median filter 5x5x5\n",
        "              \n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                \n",
        "                past_image_1 = ndimage.median_filter(past_image_1, 3)\n",
        "                future_image_1 = ndimage.median_filter(future_image_1, 3)\n",
        "                past_image_2 = ndimage.median_filter(past_image_2, 3)\n",
        "                future_image_2 = ndimage.median_filter(future_image_2, 3)\n",
        "\n",
        "                raw_data = np.concatenate((past_image_2[np.newaxis, :, :],past_image_1[np.newaxis, :, :] ,raw_data[np.newaxis, :, :], \\\n",
        "                future_image_1[np.newaxis, :, :],future_image_2[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "          # with open(file_path, 'r') as f:\n",
        "          #   # Start from second recording, as the first two are corrupted\n",
        "          #   for line in f.read().splitlines()[2:]:\n",
        "          #     # print(line)\n",
        "          #     raw_data = np.fromstring(line, dtype=float, sep='\\t')\n",
        "          #     # Change the range from [0-1000] to [0-255].\n",
        "\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              \n",
        "\n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "\n",
        "              file_label = token_position_left(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_right(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        files = list_right\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            # Start from second recording, as the first two are corrupted\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(5, len(lines) - 5):\n",
        "                            \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image_1 = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_1 = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                past_image_2 = np.fromstring(lines[i-2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image_2 = np.fromstring(lines[i+2], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "                # Spatio-temporal median filter 5x5x5\n",
        "              \n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                \n",
        "                past_image_1 = ndimage.median_filter(past_image_1, 3)\n",
        "                future_image_1 = ndimage.median_filter(future_image_1, 3)\n",
        "                past_image_2 = ndimage.median_filter(past_image_2, 3)\n",
        "                future_image_2 = ndimage.median_filter(future_image_2, 3)\n",
        "\n",
        "                raw_data = np.concatenate((past_image_2[np.newaxis, :, :],past_image_1[np.newaxis, :, :] ,raw_data[np.newaxis, :, :], \\\n",
        "                future_image_1[np.newaxis, :, :],future_image_2[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "              # Change the range from [0-1000] to [0-255].\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "\n",
        "              file_label = token_position_right(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "      dataset[subject] = (data, labels)\n",
        "  return dataset\n",
        "\n",
        "def load_exp_i_new(path,preprocess=True):\n",
        "  \"\"\"\n",
        "  Creates a numpy array for the data and labels.\n",
        "  params:\n",
        "  ------\n",
        "  path    -- Data path.\n",
        "  returns:\n",
        "  -------\n",
        "  A numpy array (data, labels).\n",
        "  \"\"\"\n",
        "\n",
        "  dataset = {}\n",
        "\n",
        "  for _, dirs, _ in os.walk(path):\n",
        "    for directory in dirs:\n",
        "      # each directory is a subject\n",
        "      subject = directory\n",
        "      data = None\n",
        "      labels = None\n",
        "      max_val = []\n",
        "      for _, _, files in os.walk(os.path.join(path, directory)):\n",
        "        # print(files)\n",
        "        for file in files:\n",
        "          # print(file)\n",
        "          file_path = os.path.join(path, directory, file)\n",
        "          with open(file_path, 'r') as f:\n",
        "            lines = f.read().splitlines()[2:]\n",
        "            for i in range(3, len(lines) - 3):\n",
        "                              \n",
        "              raw_data = np.fromstring(lines[i], dtype=float, sep='\\t').reshape(64, 32)\n",
        "              \n",
        "              if preprocess is True:\n",
        "                past_image = np.fromstring(lines[i-1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                future_image = np.fromstring(lines[i+1], dtype=float, sep='\\t').reshape(64, 32)\n",
        "                \n",
        "                # Spatio-temporal median filter 3x3x3\n",
        "                raw_data = ndimage.median_filter(raw_data, 3)\n",
        "                past_image = ndimage.median_filter(past_image, 3)\n",
        "                future_image = ndimage.median_filter(future_image, 3)\n",
        "                raw_data = np.concatenate((raw_data[np.newaxis, :, :], past_image[np.newaxis, :, :], future_image[np.newaxis, :, :]), axis=0)\n",
        "                raw_data = np.median(raw_data, axis=0)\n",
        "            \n",
        "            # with open(file_path, 'r') as f:\n",
        "            #   # Start from second recording, as the first two are corrupted\n",
        "            #   lines = f.read().splitlines()[2:]\n",
        "            #   for line in f.read().splitlines()[2:]:\n",
        "            #     # print(line)\n",
        "            #     raw_data = np.fromstring(line, dtype=float, sep='\\t')\n",
        "                # Change the range from [0-1000] to [0-255].\n",
        "                  # max_val.append(np.amax(raw_data))\n",
        "              file_data = np.round(raw_data*255/1000).astype(np.uint8)\n",
        "              # file_data = np.round(raw_data).astype(np.uint8)\n",
        "              \n",
        "              file_data = file_data.reshape((1,64,32))\n",
        "              # print(positions_i[int(file[:-4])])\n",
        "              file_label = token_position_new(positions_i[int(file[:-4])])\n",
        "              # print(\"directory: \",directory,\"file_name: \" ,file,\"file_label: \",file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "      \n",
        "      # max_over_all = max(max_val)\n",
        "      # print(max_over_all)\n",
        "\n",
        "      # data = np.round(data * 255/1000).astype(np.uint8)\n",
        "      dataset[subject] = (data, labels)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "def load_exp_ii(path):\n",
        "\n",
        "  exp_ii_data_air = {}\n",
        "  exp_ii_data_spo = {}\n",
        "\n",
        "  # each directory is a subject\n",
        "  for _, subject_dirs, _ in os.walk(path):\n",
        "    for subject in subject_dirs:\n",
        "      data = None\n",
        "      labels = None\n",
        "\n",
        "      # each directory is a matresss\n",
        "      for _, mat_dirs, _ in os.walk(os.path.join(path, subject)):\n",
        "        for mat in mat_dirs:\n",
        "          for _, _, files in os.walk(os.path.join(path, subject, mat)):\n",
        "            for file in files:\n",
        "              file_path = os.path.join(path, subject, mat, file)\n",
        "              raw_data = np.loadtxt(file_path)\n",
        "              # Change the range from [0-500] to [0-255].\n",
        "              file_data = np.round(raw_data*255/500).astype(np.uint8)\n",
        "              \n",
        "              file_data = resize_and_rotate(file_data)\n",
        "              \n",
        "              file_data = file_data.view(1, 64, 32)\n",
        "\n",
        "              if file[-6] == \"E\" or file[-6] == \"G\":\n",
        "                file_label = positions_ii[file[-6:-4]]\n",
        "              else:\n",
        "                file_label = positions_ii[file[-6]]\n",
        "\n",
        "              file_label = token_position(file_label)\n",
        "              file_label = np.array([file_label])\n",
        "\n",
        "              if data is None:\n",
        "                data = file_data\n",
        "              else:\n",
        "                data = np.concatenate((data, file_data), axis=0)\n",
        "\n",
        "              if labels is None:\n",
        "                labels = file_label\n",
        "              else:\n",
        "                labels = np.concatenate((labels, file_label), axis=0)\n",
        "\n",
        "          if mat == \"Air_Mat\":\n",
        "            exp_ii_data_air[subject] = (data, labels)\n",
        "          else:\n",
        "            exp_ii_data_spo[subject] = (data, labels)\n",
        "\n",
        "          data = None\n",
        "          labels = None\n",
        "\n",
        "    return exp_ii_data_air, exp_ii_data_spo\n",
        "\n",
        "import cv2 \n",
        "\n",
        "class Mat_Dataset():\n",
        "  def __init__(self,datasets, mats, Subject_IDs):\n",
        "\n",
        "    self.samples = []\n",
        "    self.labels = []\n",
        "\n",
        "    for mat in mats:\n",
        "      data = datasets[mat]\n",
        "      self.samples.append(np.vstack([data.get(key)[0] for key in Subject_IDs]))\n",
        "      self.labels.append(np.hstack([data.get(key)[1] for key in Subject_IDs]))\n",
        "\n",
        "    self.samples = np.vstack(self.samples)\n",
        "    self.labels = np.hstack(self.labels)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.samples.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.samples[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ORhMxY2jy2vW"
      },
      "outputs": [],
      "source": [
        "@tf.RegisterGradient(\"CustomRound\")\n",
        "def _const_round_grad(unused_op, grad):\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sT_E1OQEy6l2"
      },
      "outputs": [],
      "source": [
        "class Tea(Layer):\n",
        "    def __init__(self,\n",
        "                 units,\n",
        "                 **kwargs):\n",
        "        \"\"\"Initializes a new TeaLayer.\n",
        "\n",
        "        Arguments:\n",
        "            units -- The number of neurons to use for this layer.\"\"\"\n",
        "        self.units = units\n",
        "        # Needs to be set to `True` to use the `K.in_train_phase` function.\n",
        "        self.uses_learning_phase = True\n",
        "        super(Tea, self).__init__(**kwargs)\n",
        "\n",
        "def tea_weight_initializer(shape, dtype=np.float32):\n",
        "    \"\"\"Returns a tensor of alternating 1s and -1s, which is (kind of like)\n",
        "    how IBM initializes their weight matrix in their TeaLearning\n",
        "    literature.\n",
        "\n",
        "    Arguments:\n",
        "        shape -- The shape of the weights to intialize.\n",
        "\n",
        "    Keyword Arguments:\n",
        "        dtype -- The data type to use to initialize the weights.\n",
        "                 (default: {np.float32})\"\"\"\n",
        "    num_axons = shape[0]\n",
        "    num_neurons = shape[1]\n",
        "    ret_array = np.zeros((int(num_axons), int(num_neurons)), dtype=np.float32)\n",
        "    for axon_num, axon in enumerate(ret_array):\n",
        "        if axon_num % 2 == 0:\n",
        "            for i in range(len(axon)):\n",
        "                ret_array[axon_num][i] = 1\n",
        "        else:\n",
        "            for i in range(len(axon)):\n",
        "                ret_array[axon_num][i] = -1\n",
        "    return tf.convert_to_tensor(ret_array)\n",
        "\n",
        "def build(self, input_shape):\n",
        "    assert len(input_shape) >= 2\n",
        "    shape = (input_shape[-1], self.units)\n",
        "    self.static_weights = self.add_weight(\n",
        "        name='weights',\n",
        "        shape=shape,\n",
        "        initializer=tea_weight_initializer,\n",
        "        trainable=False)\n",
        "    # Intialize connections around 0.5 because they represent probabilities.\n",
        "    self.connections = self.add_weight(\n",
        "        name='connections',\n",
        "        initializer=initializers.TruncatedNormal(mean=0.5),\n",
        "        shape=shape)\n",
        "    self.biases = self.add_weight(\n",
        "        name='biases',\n",
        "        initializer='zeros',\n",
        "        shape=(self.units,))\n",
        "    super(Tea, self).build(input_shape)\n",
        "\n",
        "# Bind the method to our class\n",
        "Tea.build = build\n",
        "\n",
        "def call(self, x):\n",
        "    with tf.get_default_graph().gradient_override_map(\n",
        "        {\"Round\":\"CustomRound\"}):\n",
        "        # Constrain input\n",
        "        x = tf.round(x)\n",
        "        # Constrain connections\n",
        "        connections = self.connections\n",
        "        connections = tf.round(connections)\n",
        "        connections = K.clip(connections, 0, 1)\n",
        "        # Multiply connections with weights\n",
        "        weighted_connections = connections * self.static_weights\n",
        "        # Dot input with weighted connections\n",
        "        output = K.dot(x, weighted_connections)\n",
        "        # Constrain biases\n",
        "        biases = tf.round(self.biases)\n",
        "        output = K.bias_add(\n",
        "            output,\n",
        "            biases,\n",
        "            data_format='channels_last'\n",
        "        )\n",
        "        # Apply activation / spike\n",
        "        output = K.in_train_phase(\n",
        "            K.sigmoid(output),\n",
        "            tf.cast(tf.greater_equal(output, 0.0), tf.float32)\n",
        "        )\n",
        "    return output\n",
        "    \n",
        "# Bind the method to our class\n",
        "Tea.call = call\n",
        "\n",
        "def compute_output_shape(self, input_shape):\n",
        "    assert input_shape and len(input_shape) >= 2\n",
        "    assert input_shape[-1]\n",
        "    output_shape = list(input_shape)\n",
        "    output_shape[-1] = self.units\n",
        "    return tuple(output_shape)\n",
        "    \n",
        "# Bind the method to our class\n",
        "Tea.compute_output_shape = compute_output_shape\n",
        "\n",
        "\n",
        "class AdditivePooling(Layer):\n",
        "    \"\"\"A helper layer designed to format data for output during TeaLearning.\n",
        "    If the data input to the layer has multiple spikes per classification, the\n",
        "    spikes for each tick are summed up. Then, all neurons that correspond to a\n",
        "    certain class are summed up so that the output is the number of spikes for\n",
        "    each class. Neurons are assumed to be arranged such that each\n",
        "    `num_classes` neurons represent a guess for each of the classes. For\n",
        "    example, if the guesses correspond to number from 0 to 9, the nuerons are\n",
        "    arranged as such:\n",
        "\n",
        "        neuron_num: 0  1  2  3  4  5  6  7  8  9  10 11 12  ...\n",
        "        guess:      0  1  2  3  4  5  6  7  8  9  0  1  2   ...\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_classes,\n",
        "                 **kwargs):\n",
        "        \"\"\"Initializes a new `AdditivePooling` layer.\n",
        "\n",
        "        Arguments:\n",
        "            num_classes -- The number of classes to output.\n",
        "        \"\"\"\n",
        "        self.num_classes = num_classes\n",
        "        self.num_inputs = None\n",
        "        super(AdditivePooling, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) >= 2\n",
        "        # The number of neurons must be collapsable into the number of classes\n",
        "        assert input_shape[-1] % self.num_classes == 0\n",
        "        self.num_inputs = input_shape[-1]\n",
        "\n",
        "    def call(self, x):\n",
        "        # Sum up ticks if there are ticks\n",
        "        if len(x.shape) >= 3:\n",
        "            output = K.sum(x, axis=1)\n",
        "        else:\n",
        "            output = x\n",
        "        # Reshape output\n",
        "        output = tf.reshape(\n",
        "            output,\n",
        "            [-1, int(self.num_inputs // self.num_classes), self.num_classes]\n",
        "        )\n",
        "        # Sum up neurons\n",
        "        output = tf.reduce_sum(output, 1)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_shape = list(input_shape)\n",
        "        # Last dimension will be number of classes\n",
        "        output_shape[-1] = self.num_classes\n",
        "        # Ticks were summed, so delete tick dimension if exists\n",
        "        if len(output_shape) >= 3:\n",
        "            del output_shape[1]\n",
        "        return tuple(output_shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZkGllFs8SiX"
      },
      "source": [
        "Test 3 class - Serial "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4SmVVBT14gwH"
      },
      "outputs": [],
      "source": [
        "exp_i_data_3_class = load_exp_i_short(\"/content/gdrive/MyDrive/RANC/dataset/experiment-i\",preprocess=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "J8lMVWlo8UgV"
      },
      "outputs": [],
      "source": [
        "datasets_3_class = {\"Base\":exp_i_data_3_class}\n",
        "subjects = [\"S1\",\"S2\",\"S3\",\"S4\",\"S5\",\"S6\",\"S7\",\"S8\",\"S9\",\"S10\",\"S11\",\"S12\",\"S13\"]\n",
        "sub=\"S6\"\n",
        "subjects.remove(sub)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZyiLx9Wc8bXm"
      },
      "outputs": [],
      "source": [
        "train_data_3_class = Mat_Dataset(datasets_3_class,[\"Base\"],subjects)\n",
        "test_data_3_class = Mat_Dataset(datasets_3_class,[\"Base\"],[sub])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(train_data_3_class.samples)):\n",
        "  train_data_3_class.samples[i] = cv2.equalizeHist(train_data_3_class.samples[i])\n",
        "for i in range(len(test_data_3_class.samples)):\n",
        "  test_data_3_class.samples[i] = cv2.equalizeHist(test_data_3_class.samples[i])"
      ],
      "metadata": {
        "id": "mH3J616-o7jr"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(set(train_data_3_class.labels))\n",
        "\n",
        "y_train_3_class = to_categorical(train_data_3_class.labels, 3)\n",
        "y_test_3_class = to_categorical(test_data_3_class.labels, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mXl7Yt-MvtI",
        "outputId": "e65322d5-bef4-489d-97a2-1ed9e6deb2f8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0, 1, 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_copy = np.empty_like(train_data_3_class.samples)\n",
        "x_train_copy[:,:,:]=train_data_3_class.samples\n",
        "\n",
        "x_test_copy = np.empty_like(test_data_3_class.samples)\n",
        "x_test_copy[:,:,:]=test_data_3_class.samples\n",
        "\n",
        "train_data = x_train_copy\n",
        "test_data = x_test_copy\n",
        "\n",
        "x_train = []\n",
        "for i in range(len(train_data_3_class.samples)):\n",
        "    mask = np.ones_like(train_data[i])\n",
        "\n",
        "    e_1 = np.array(train_data[i]>=mask*25).astype(float)\n",
        "    e_2 = np.array(train_data[i]>=mask*75).astype(float)\n",
        "    e_3 = np.array(train_data[i]>=mask*125).astype(float)\n",
        "    e_4 = np.array(train_data[i]>=mask*175).astype(float)\n",
        "    e_5 = np.array(train_data[i]>=mask*225).astype(float)\n",
        "\n",
        "    x_train.append(np.concatenate((e_1[:,:,np.newaxis],e_2[:,:,np.newaxis],e_3[:,:,np.newaxis],\\\n",
        "                                    e_4[:,:,np.newaxis],e_5[:,:,np.newaxis]),axis=2))\n",
        "\n",
        "\n",
        "x_test = []\n",
        "for i in range(len(test_data_3_class.samples)):\n",
        "    mask = np.ones_like(test_data[i])\n",
        "    e_1 = np.array(test_data[i]>=mask*25).astype(float)\n",
        "    e_2 = np.array(test_data[i]>=mask*75).astype(float)\n",
        "    e_3 = np.array(test_data[i]>=mask*125).astype(float)\n",
        "    e_4 = np.array(test_data[i]>=mask*175).astype(float)\n",
        "    e_5 = np.array(test_data[i]>=mask*225).astype(float)\n",
        "    \n",
        "    x_test.append(np.concatenate((e_1[:,:,np.newaxis],e_2[:,:,np.newaxis],e_3[:,:,np.newaxis],\\\n",
        "                                    e_4[:,:,np.newaxis],e_5[:,:,np.newaxis],),axis=2))"
      ],
      "metadata": {
        "id": "b_0oOp22BhWk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array(x_train)\n",
        "x_test = np.array(x_test)\n",
        "\n",
        "y_train = np.array(y_train_3_class)\n",
        "y_test = np.array(y_test_3_class)"
      ],
      "metadata": {
        "id": "ZvMsk3tiwuHn"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(1)\n",
        "(x_train,y_train) = shuffle(x_train,y_train)\n",
        "random.seed(1)\n",
        "(x_test,y_test) = shuffle(x_test,y_test)"
      ],
      "metadata": {
        "id": "HAvYQc5qw4jg"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train)\n",
        "print(x_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXwFRA7jy83n",
        "outputId": "c58b2f09-af76-49a2-a5dd-73a3490bdda2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " ...\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]]\n",
            "(17111, 64, 32, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "BRaYLBdaVln-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_3_classes = Input(shape=(64, 32, 5))\n",
        "\n",
        "flattened_inputs = Flatten()(inputs_3_classes)\n",
        "\n",
        "flattened_inputs_1 = Lambda(lambda x : x[:,     :1*2048])(flattened_inputs)\n",
        "flattened_inputs_2 = Lambda(lambda x : x[:,1*2048:2*2048])(flattened_inputs)\n",
        "flattened_inputs_3 = Lambda(lambda x : x[:,2*2048:3*2048])(flattened_inputs)\n",
        "flattened_inputs_4 = Lambda(lambda x : x[:,3*2048:4*2048])(flattened_inputs)\n",
        "flattened_inputs_5 = Lambda(lambda x : x[:,4*2048:5*2048])(flattened_inputs)\n",
        "\n",
        "\n",
        "x1_1  = Lambda(lambda x : x[:,     :256 ])(flattened_inputs_1)\n",
        "x2_1  = Lambda(lambda x : x[:, 119 : 375 ])(flattened_inputs_1)\n",
        "x3_1  = Lambda(lambda x : x[:, 238 :494 ])(flattened_inputs_1)\n",
        "x4_1  = Lambda(lambda x : x[:, 357 : 613])(flattened_inputs_1)\n",
        "x5_1  = Lambda(lambda x : x[:, 476:732])(flattened_inputs_1)\n",
        "x6_1  = Lambda(lambda x : x[:, 595:851])(flattened_inputs_1)\n",
        "x7_1  = Lambda(lambda x : x[:, 714:970])(flattened_inputs_1)\n",
        "x8_1  = Lambda(lambda x : x[:, 833:1089])(flattened_inputs_1)\n",
        "x9_1  = Lambda(lambda x : x[:, 952:1208])(flattened_inputs_1)\n",
        "x10_1  = Lambda(lambda x : x[:, 1071:1327])(flattened_inputs_1)\n",
        "x11_1  = Lambda(lambda x : x[:, 1190:1446])(flattened_inputs_1)\n",
        "x12_1  = Lambda(lambda x : x[:, 1309:1565])(flattened_inputs_1)\n",
        "x13_1  = Lambda(lambda x : x[:, 1428:1684])(flattened_inputs_1)\n",
        "x14_1  = Lambda(lambda x : x[:, 1547:1803])(flattened_inputs_1)\n",
        "x15_1  = Lambda(lambda x : x[:, 1666:1922])(flattened_inputs_1)\n",
        "x16_1  = Lambda(lambda x : x[:, 1785:2041])(flattened_inputs_1)\n",
        "\n",
        "x1_1  = Tea(64)(x1_1)\n",
        "x2_1  = Tea(64)(x2_1)\n",
        "x3_1  = Tea(64)(x3_1)\n",
        "x4_1  = Tea(64)(x4_1)\n",
        "x5_1  = Tea(64)(x5_1)\n",
        "x6_1  = Tea(64)(x6_1)\n",
        "x7_1  = Tea(64)(x7_1)\n",
        "x8_1  = Tea(64)(x8_1)\n",
        "x9_1  = Tea(64)(x9_1)\n",
        "x10_1  = Tea(64)(x10_1)\n",
        "x11_1  = Tea(64)(x11_1)\n",
        "x12_1  = Tea(64)(x12_1)\n",
        "x13_1  = Tea(64)(x13_1)\n",
        "x14_1  = Tea(64)(x14_1)\n",
        "x15_1  = Tea(64)(x15_1)\n",
        "x16_1  = Tea(64)(x16_1)\n",
        "\n",
        "x1_2  = Lambda(lambda x : x[:,     :256 ])(flattened_inputs_2)\n",
        "x2_2  = Lambda(lambda x : x[:, 119 : 375 ])(flattened_inputs_2)\n",
        "x3_2  = Lambda(lambda x : x[:, 238 :494 ])(flattened_inputs_2)\n",
        "x4_2  = Lambda(lambda x : x[:, 357 : 613])(flattened_inputs_2)\n",
        "x5_2  = Lambda(lambda x : x[:, 476:732])(flattened_inputs_2)\n",
        "x6_2  = Lambda(lambda x : x[:, 595:851])(flattened_inputs_2)\n",
        "x7_2  = Lambda(lambda x : x[:, 714:970])(flattened_inputs_2)\n",
        "x8_2  = Lambda(lambda x : x[:, 833:1089])(flattened_inputs_2)\n",
        "x9_2  = Lambda(lambda x : x[:, 952:1208])(flattened_inputs_2)\n",
        "x10_2  = Lambda(lambda x : x[:, 1071:1327])(flattened_inputs_2)\n",
        "x11_2  = Lambda(lambda x : x[:, 1190:1446])(flattened_inputs_2)\n",
        "x12_2  = Lambda(lambda x : x[:, 1309:1565])(flattened_inputs_2)\n",
        "x13_2  = Lambda(lambda x : x[:, 1428:1684])(flattened_inputs_2)\n",
        "x14_2  = Lambda(lambda x : x[:, 1547:1803])(flattened_inputs_2)\n",
        "x15_2  = Lambda(lambda x : x[:, 1666:1922])(flattened_inputs_2)\n",
        "x16_2  = Lambda(lambda x : x[:, 1785:2041])(flattened_inputs_2)\n",
        "\n",
        "\n",
        "x1_2  = Tea(64)(x1_2)\n",
        "x2_2  = Tea(64)(x2_2)\n",
        "x3_2  = Tea(64)(x3_2)\n",
        "x4_2  = Tea(64)(x4_2)\n",
        "x5_2  = Tea(64)(x5_2)\n",
        "x6_2  = Tea(64)(x6_2)\n",
        "x7_2  = Tea(64)(x7_2)\n",
        "x8_2  = Tea(64)(x8_2)\n",
        "x9_2  = Tea(64)(x9_2)\n",
        "x10_2  = Tea(64)(x10_2)\n",
        "x11_2  = Tea(64)(x11_2)\n",
        "x12_2  = Tea(64)(x12_2)\n",
        "x13_2  = Tea(64)(x13_2)\n",
        "x14_2  = Tea(64)(x14_2)\n",
        "x15_2  = Tea(64)(x15_2)\n",
        "x16_2  = Tea(64)(x16_2)\n",
        "\n",
        "x1_3  = Lambda(lambda x : x[:,     :256 ])(flattened_inputs_3)\n",
        "x2_3  = Lambda(lambda x : x[:, 119 : 375 ])(flattened_inputs_3)\n",
        "x3_3  = Lambda(lambda x : x[:, 238 :494 ])(flattened_inputs_3)\n",
        "x4_3  = Lambda(lambda x : x[:, 357 : 613])(flattened_inputs_3)\n",
        "x5_3  = Lambda(lambda x : x[:, 476:732])(flattened_inputs_3)\n",
        "x6_3  = Lambda(lambda x : x[:, 595:851])(flattened_inputs_3)\n",
        "x7_3  = Lambda(lambda x : x[:, 714:970])(flattened_inputs_3)\n",
        "x8_3  = Lambda(lambda x : x[:, 833:1089])(flattened_inputs_3)\n",
        "x9_3  = Lambda(lambda x : x[:, 952:1208])(flattened_inputs_3)\n",
        "x10_3  = Lambda(lambda x : x[:, 1071:1327])(flattened_inputs_3)\n",
        "x11_3  = Lambda(lambda x : x[:, 1190:1446])(flattened_inputs_3)\n",
        "x12_3  = Lambda(lambda x : x[:, 1309:1565])(flattened_inputs_3)\n",
        "x13_3  = Lambda(lambda x : x[:, 1428:1684])(flattened_inputs_3)\n",
        "x14_3  = Lambda(lambda x : x[:, 1547:1803])(flattened_inputs_3)\n",
        "x15_3  = Lambda(lambda x : x[:, 1666:1922])(flattened_inputs_3)\n",
        "x16_3  = Lambda(lambda x : x[:, 1785:2041])(flattened_inputs_3)\n",
        "\n",
        "x1_3  = Tea(64)(x1_3)\n",
        "x2_3  = Tea(64)(x2_3)\n",
        "x3_3  = Tea(64)(x3_3)\n",
        "x4_3  = Tea(64)(x4_3)\n",
        "x5_3  = Tea(64)(x5_3)\n",
        "x6_3  = Tea(64)(x6_3)\n",
        "x7_3  = Tea(64)(x7_3)\n",
        "x8_3  = Tea(64)(x8_3)\n",
        "x9_3  = Tea(64)(x9_3)\n",
        "x10_3  = Tea(64)(x10_3)\n",
        "x11_3  = Tea(64)(x11_3)\n",
        "x12_3  = Tea(64)(x12_3)\n",
        "x13_3  = Tea(64)(x13_3)\n",
        "x14_3  = Tea(64)(x14_3)\n",
        "x15_3  = Tea(64)(x15_3)\n",
        "x16_3  = Tea(64)(x16_3)\n",
        "\n",
        "x1_4  = Lambda(lambda x : x[:,     :256 ])(flattened_inputs_4)\n",
        "x2_4  = Lambda(lambda x : x[:, 119 : 375 ])(flattened_inputs_4)\n",
        "x3_4  = Lambda(lambda x : x[:, 238 :494 ])(flattened_inputs_4)\n",
        "x4_4  = Lambda(lambda x : x[:, 357 : 613])(flattened_inputs_4)\n",
        "x5_4  = Lambda(lambda x : x[:, 476:732])(flattened_inputs_4)\n",
        "x6_4  = Lambda(lambda x : x[:, 595:851])(flattened_inputs_4)\n",
        "x7_4  = Lambda(lambda x : x[:, 714:970])(flattened_inputs_4)\n",
        "x8_4  = Lambda(lambda x : x[:, 833:1089])(flattened_inputs_4)\n",
        "x9_4  = Lambda(lambda x : x[:, 952:1208])(flattened_inputs_4)\n",
        "x10_4  = Lambda(lambda x : x[:, 1071:1327])(flattened_inputs_4)\n",
        "x11_4  = Lambda(lambda x : x[:, 1190:1446])(flattened_inputs_4)\n",
        "x12_4  = Lambda(lambda x : x[:, 1309:1565])(flattened_inputs_4)\n",
        "x13_4  = Lambda(lambda x : x[:, 1428:1684])(flattened_inputs_4)\n",
        "x14_4  = Lambda(lambda x : x[:, 1547:1803])(flattened_inputs_4)\n",
        "x15_4  = Lambda(lambda x : x[:, 1666:1922])(flattened_inputs_4)\n",
        "x16_4  = Lambda(lambda x : x[:, 1785:2041])(flattened_inputs_4)\n",
        "\n",
        "x1_4  = Tea(64)(x1_4)\n",
        "x2_4  = Tea(64)(x2_4)\n",
        "x3_4  = Tea(64)(x3_4)\n",
        "x4_4  = Tea(64)(x4_4)\n",
        "x5_4  = Tea(64)(x5_4)\n",
        "x6_4  = Tea(64)(x6_4)\n",
        "x7_4  = Tea(64)(x7_4)\n",
        "x8_4  = Tea(64)(x8_4)\n",
        "x9_4  = Tea(64)(x9_4)\n",
        "x10_4  = Tea(64)(x10_4)\n",
        "x11_4  = Tea(64)(x11_4)\n",
        "x12_4  = Tea(64)(x12_4)\n",
        "x13_4  = Tea(64)(x13_4)\n",
        "x14_4  = Tea(64)(x14_4)\n",
        "x15_4  = Tea(64)(x15_4)\n",
        "x16_4  = Tea(64)(x16_4)\n",
        "\n",
        "x1_5  = Lambda(lambda x : x[:,     :256 ])(flattened_inputs_5)\n",
        "x2_5  = Lambda(lambda x : x[:, 119 : 375 ])(flattened_inputs_5)\n",
        "x3_5  = Lambda(lambda x : x[:, 238 :494 ])(flattened_inputs_5)\n",
        "x4_5  = Lambda(lambda x : x[:, 357 : 613])(flattened_inputs_5)\n",
        "x5_5  = Lambda(lambda x : x[:, 476:732])(flattened_inputs_5)\n",
        "x6_5  = Lambda(lambda x : x[:, 595:851])(flattened_inputs_5)\n",
        "x7_5  = Lambda(lambda x : x[:, 714:970])(flattened_inputs_5)\n",
        "x8_5  = Lambda(lambda x : x[:, 833:1089])(flattened_inputs_5)\n",
        "x9_5  = Lambda(lambda x : x[:, 952:1208])(flattened_inputs_5)\n",
        "x10_5  = Lambda(lambda x : x[:, 1071:1327])(flattened_inputs_5)\n",
        "x11_5  = Lambda(lambda x : x[:, 1190:1446])(flattened_inputs_5)\n",
        "x12_5  = Lambda(lambda x : x[:, 1309:1565])(flattened_inputs_5)\n",
        "x13_5  = Lambda(lambda x : x[:, 1428:1684])(flattened_inputs_5)\n",
        "x14_5  = Lambda(lambda x : x[:, 1547:1803])(flattened_inputs_5)\n",
        "x15_5  = Lambda(lambda x : x[:, 1666:1922])(flattened_inputs_5)\n",
        "x16_5  = Lambda(lambda x : x[:, 1785:2041])(flattened_inputs_5)\n",
        "\n",
        "x1_5  = Tea(64)(x1_5)\n",
        "x2_5  = Tea(64)(x2_5)\n",
        "x3_5  = Tea(64)(x3_5)\n",
        "x4_5  = Tea(64)(x4_5)\n",
        "x5_5  = Tea(64)(x5_5)\n",
        "x6_5  = Tea(64)(x6_5)\n",
        "x7_5  = Tea(64)(x7_5)\n",
        "x8_5  = Tea(64)(x8_5)\n",
        "x9_5  = Tea(64)(x9_5)\n",
        "x10_5  = Tea(64)(x10_5)\n",
        "x11_5  = Tea(64)(x11_5)\n",
        "x12_5  = Tea(64)(x12_5)\n",
        "x13_5  = Tea(64)(x13_5)\n",
        "x14_5  = Tea(64)(x14_5)\n",
        "x15_5  = Tea(64)(x15_5)\n",
        "x16_5  = Tea(64)(x16_5)\n",
        "\n",
        "#Average image 1,2,3 \n",
        "\n",
        "x1_1_1 = Average()([x1_1,x1_2,x1_3])\n",
        "x2_1_1 = Average()([x2_1,x2_2,x2_3])\n",
        "x3_1_1 = Average()([x3_1,x3_2,x3_3])\n",
        "x4_1_1 = Average()([x4_1,x4_2,x4_3])\n",
        "x5_1_1 = Average()([x5_1,x5_2,x5_3])\n",
        "x6_1_1 = Average()([x6_1,x6_2,x6_3])\n",
        "x7_1_1 = Average()([x7_1,x7_2,x7_3])\n",
        "x8_1_1 = Average()([x8_1,x8_2,x8_3])\n",
        "x9_1_1 = Average()([x9_1,x9_2,x9_3])\n",
        "x10_1_1 = Average()([x10_1,x10_2,x10_3])\n",
        "x11_1_1 = Average()([x11_1,x11_2,x11_3])\n",
        "x12_1_1 = Average()([x12_1,x12_2,x12_3])\n",
        "x13_1_1 = Average()([x13_1,x13_2,x13_3])\n",
        "x14_1_1 = Average()([x14_1,x14_2,x14_3])\n",
        "x15_1_1 = Average()([x15_1,x15_2,x15_3])\n",
        "x16_1_1 = Average()([x16_1,x16_2,x16_3])\n",
        "\n",
        "#Average image 2,3,4\n",
        "\n",
        "x1_1_2 = Average()([x1_2,x1_3,x1_4])\n",
        "x2_1_2 = Average()([x2_2,x2_3,x2_4])\n",
        "x3_1_2 = Average()([x3_2,x3_3,x3_4])\n",
        "x4_1_2 = Average()([x4_2,x4_3,x4_4])\n",
        "x5_1_2 = Average()([x5_2,x5_3,x5_4])\n",
        "x6_1_2 = Average()([x6_2,x6_3,x6_4])\n",
        "x7_1_2 = Average()([x7_2,x7_3,x7_4])\n",
        "x8_1_2 = Average()([x8_2,x8_3,x8_4])\n",
        "x9_1_2 = Average()([x9_2,x9_3,x9_4])\n",
        "x10_1_2 = Average()([x10_2,x10_3,x10_4])\n",
        "x11_1_2 = Average()([x11_2,x11_3,x11_4])\n",
        "x12_1_2 = Average()([x12_2,x12_3,x12_4])\n",
        "x13_1_2 = Average()([x13_2,x13_3,x13_4])\n",
        "x14_1_2 = Average()([x14_2,x14_3,x14_4])\n",
        "x15_1_2 = Average()([x15_2,x15_3,x15_4])\n",
        "x16_1_2 = Average()([x16_2,x16_3,x16_4])\n",
        "\n",
        "#Average image 3,4,5\n",
        "\n",
        "x1_1_3 = Average()([x1_3,x1_4,x1_5])\n",
        "x2_1_3 = Average()([x2_3,x2_4,x2_5])\n",
        "x3_1_3 = Average()([x3_3,x3_4,x3_5])\n",
        "x4_1_3 = Average()([x4_3,x4_4,x4_5])\n",
        "x5_1_3 = Average()([x5_3,x5_4,x5_5])\n",
        "x6_1_3 = Average()([x6_3,x6_4,x6_5])\n",
        "x7_1_3 = Average()([x7_3,x7_4,x7_5])\n",
        "x8_1_3 = Average()([x8_3,x8_4,x8_5])\n",
        "x9_1_3 = Average()([x9_3,x9_4,x9_5])\n",
        "x10_1_3 = Average()([x10_3,x10_4,x10_5])\n",
        "x11_1_3 = Average()([x11_3,x11_4,x11_5])\n",
        "x12_1_3 = Average()([x12_3,x12_4,x12_5])\n",
        "x13_1_3 = Average()([x13_3,x13_4,x13_5])\n",
        "x14_1_3 = Average()([x14_3,x14_4,x14_5])\n",
        "x15_1_3 = Average()([x15_3,x15_4,x15_5])\n",
        "x16_1_3 = Average()([x16_3,x16_4,x16_5])\n",
        "\n",
        "#Layer 2:\n",
        "\n",
        "x1_1 = concatenate(([x1_1_1,x2_1_1,x3_1_1,x4_1_1]),axis=1)\n",
        "x2_1 = concatenate(([x5_1_1,x6_1_1,x7_1_1,x8_1_1]),axis=1)\n",
        "x3_1 = concatenate(([x9_1_1,x10_1_1,x11_1_1,x12_1_1]),axis=1)\n",
        "x4_1 = concatenate(([x13_1_1,x14_1_1,x15_1_1,x16_1_1]),axis=1)\n",
        "\n",
        "x1_2 = concatenate(([x1_1_2,x2_1_2,x3_1_2,x4_1_2]),axis=1)\n",
        "x2_2 = concatenate(([x5_1_2,x6_1_2,x7_1_2,x8_1_2]),axis=1)\n",
        "x3_2 = concatenate(([x9_1_2,x10_1_2,x11_1_2,x12_1_2]),axis=1)\n",
        "x4_2 = concatenate(([x13_1_2,x14_1_2,x15_1_2,x16_1_2]),axis=1)\n",
        "\n",
        "x1_3 = concatenate(([x1_1_3,x2_1_3,x3_1_3,x4_1_3]),axis=1)\n",
        "x2_3 = concatenate(([x5_1_3,x6_1_3,x7_1_3,x8_1_3]),axis=1)\n",
        "x3_3 = concatenate(([x9_1_3,x10_1_3,x11_1_3,x12_1_3]),axis=1)\n",
        "x4_3 = concatenate(([x13_1_3,x14_1_3,x15_1_3,x16_1_3]),axis=1)\n",
        "\n",
        "\n",
        "x1_1 = Tea(64)(x1_1)\n",
        "x2_1 = Tea(64)(x2_1)\n",
        "x3_1 = Tea(64)(x3_1)\n",
        "x4_1 = Tea(64)(x4_1)\n",
        "\n",
        "x1_2 = Tea(64)(x1_2)\n",
        "x2_2 = Tea(64)(x2_2)\n",
        "x3_2 = Tea(64)(x3_2)\n",
        "x4_2 = Tea(64)(x4_2)\n",
        "\n",
        "x1_3 = Tea(64)(x1_3)\n",
        "x2_3 = Tea(64)(x2_3)\n",
        "x3_3 = Tea(64)(x3_3)\n",
        "x4_3 = Tea(64)(x4_3)\n",
        "\n",
        "#Average after layer 2\n",
        "x_out_1 = Average()([x1_1,x1_2,x1_3])\n",
        "x_out_2 = Average()([x2_1,x2_2,x2_3])\n",
        "x_out_3 = Average()([x3_1,x3_2,x3_3])\n",
        "x_out_4 = Average()([x4_1,x4_2,x4_3])\n",
        "\n",
        "# Layer 3\n",
        "\n",
        "x_out = concatenate(([x_out_1,x_out_2,x_out_3,x_out_4]),axis=1)\n",
        "\n",
        "x_out = Tea(255)(x_out) # 255 divided by 3\n",
        "\n",
        "x_out = AdditivePooling(3)(x_out)\n",
        "\n",
        "predictions_3_classes = Activation('softmax')(x_out)\n",
        "\n",
        "#Model\n",
        "\n",
        "model = Model(inputs=inputs_3_classes, outputs=predictions_3_classes)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=0.0001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=256,\n",
        "          epochs=20,\n",
        "          verbose=1,\n",
        "          validation_split=0.2)\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mThYwmTsCF08",
        "outputId": "7a342f61-c7bc-4898-f5ce-b6d6187c96ef"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 13688 samples, validate on 3423 samples\n",
            "Epoch 1/20\n",
            "13568/13688 [============================>.] - ETA: 0s - loss: 0.6696 - acc: 0.7590"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2057: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates = self.state_updates\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13688/13688 [==============================] - 7s 513us/sample - loss: 0.6648 - acc: 0.7607 - val_loss: 0.3867 - val_acc: 0.8639\n",
            "Epoch 2/20\n",
            "13688/13688 [==============================] - 3s 196us/sample - loss: 0.0643 - acc: 0.9813 - val_loss: 0.1099 - val_acc: 0.9585\n",
            "Epoch 3/20\n",
            "13688/13688 [==============================] - 3s 197us/sample - loss: 0.0140 - acc: 0.9965 - val_loss: 0.0558 - val_acc: 0.9819\n",
            "Epoch 4/20\n",
            "13688/13688 [==============================] - 3s 191us/sample - loss: 0.0068 - acc: 0.9988 - val_loss: 0.0360 - val_acc: 0.9874\n",
            "Epoch 5/20\n",
            "13688/13688 [==============================] - 3s 194us/sample - loss: 0.0030 - acc: 0.9996 - val_loss: 0.0329 - val_acc: 0.9869\n",
            "Epoch 6/20\n",
            "13688/13688 [==============================] - 3s 195us/sample - loss: 0.0018 - acc: 0.9999 - val_loss: 0.0396 - val_acc: 0.9874\n",
            "Epoch 7/20\n",
            "13688/13688 [==============================] - 3s 199us/sample - loss: 0.0016 - acc: 0.9998 - val_loss: 0.0326 - val_acc: 0.9886\n",
            "Epoch 8/20\n",
            "13688/13688 [==============================] - 3s 198us/sample - loss: 0.0016 - acc: 0.9998 - val_loss: 0.0283 - val_acc: 0.9895\n",
            "Epoch 9/20\n",
            "13688/13688 [==============================] - 3s 193us/sample - loss: 9.9915e-04 - acc: 0.9999 - val_loss: 0.0238 - val_acc: 0.9918\n",
            "Epoch 10/20\n",
            "13688/13688 [==============================] - 3s 197us/sample - loss: 7.4624e-04 - acc: 1.0000 - val_loss: 0.0267 - val_acc: 0.9924\n",
            "Epoch 11/20\n",
            "13688/13688 [==============================] - 3s 194us/sample - loss: 6.7615e-04 - acc: 0.9999 - val_loss: 0.0216 - val_acc: 0.9924\n",
            "Epoch 12/20\n",
            "13688/13688 [==============================] - 3s 194us/sample - loss: 3.8053e-04 - acc: 1.0000 - val_loss: 0.0243 - val_acc: 0.9927\n",
            "Epoch 13/20\n",
            "13688/13688 [==============================] - 3s 195us/sample - loss: 3.5871e-04 - acc: 1.0000 - val_loss: 0.0184 - val_acc: 0.9962\n",
            "Epoch 14/20\n",
            "13688/13688 [==============================] - 3s 191us/sample - loss: 3.0075e-04 - acc: 1.0000 - val_loss: 0.0265 - val_acc: 0.9927\n",
            "Epoch 15/20\n",
            "13688/13688 [==============================] - 3s 194us/sample - loss: 5.1817e-04 - acc: 0.9999 - val_loss: 0.0164 - val_acc: 0.9950\n",
            "Epoch 16/20\n",
            "13688/13688 [==============================] - 3s 193us/sample - loss: 2.3195e-04 - acc: 1.0000 - val_loss: 0.0155 - val_acc: 0.9950\n",
            "Epoch 17/20\n",
            "13688/13688 [==============================] - 3s 191us/sample - loss: 1.3492e-04 - acc: 1.0000 - val_loss: 0.0190 - val_acc: 0.9942\n",
            "Epoch 18/20\n",
            "13688/13688 [==============================] - 3s 193us/sample - loss: 1.4174e-04 - acc: 1.0000 - val_loss: 0.0163 - val_acc: 0.9947\n",
            "Epoch 19/20\n",
            "13688/13688 [==============================] - 3s 193us/sample - loss: 1.1647e-04 - acc: 1.0000 - val_loss: 0.0170 - val_acc: 0.9947\n",
            "Epoch 20/20\n",
            "13688/13688 [==============================] - 3s 196us/sample - loss: 1.2656e-04 - acc: 1.0000 - val_loss: 0.0234 - val_acc: 0.9933\n",
            "Test loss: 0.023499299284867846\n",
            "Test accuracy: 0.99126637\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "RANC_21cores_3classes_serial.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}